<!DOCTYPE html>
<html lang="en"><head>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-d4d76bf8491c20bad77d141916dc28e1.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.43">

  <title>Computer Vision – Intro to learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; font-weight: bold; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto-2f366650f320edcfcf53d73c80250a32.css">
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Intro to learning</h1>

<div class="quarto-title-authors">
</div>

</section>
<section id="notation" class="slide level2">
<h2>Notation</h2>
<ul>
<li><span class="math inline">\(\mathbf{x}\)</span>: model inputs (instead f <span class="math inline">\({\boldsymbol\ell}\)</span>)</li>
<li><span class="math inline">\(\mathbf{y}\)</span>: a model’s final output.</li>
<li><span class="math inline">\(\mathbf{x}_0 \rightarrow \mathbf{x}_1 \rightarrow \ldots \rightarrow \mathbf{y}\)</span>: neural network’s sequence of transformations</li>
<li>for single layer: <span class="math inline">\({\mathbf{x}_{\texttt{in}}}\)</span> as input and <span class="math inline">\({\mathbf{x}_{\texttt{out}}}\)</span> as output</li>
<li><span class="math inline">\(\mathbf{h}\)</span> and <span class="math inline">\(\mathbf{z}\)</span>: intermediate representations in neural nets</li>
</ul>
</section>
<section id="introduction" class="slide level2">
<h2>Introduction</h2>
<div class="callout callout-tip callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Goal</strong></p>
</div>
<div class="callout-content">
<p>The goal of learning is to extract lessons from past experience in order to solve future problems.</p>
<p>Typically, this involves searching for an algorithm that solves past instances of the problem.</p>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Past and future do not necessarily refer to the calendar date; instead they refer to what the has previously seen and what the learner will see next.</p>
</div>
</div>
</div>
</section>
<section id="introduction-1" class="slide level2">
<h2>Introduction</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Algorithm</strong></p>
</div>
<div class="callout-content">
<p>Because learning is itself an algorithm, it can be understood as a meta-algorithm: an algorithm that outputs algorithms:</p>
</div>
</div>
</div>

<img data-src="./img/intro_to_learning/learning_as_meta_algorithm.png" style="width:75.0%" class="r-stretch quarto-figure-center" id="fig-learning_as_meta_algorithm"><p class="caption">
Figure&nbsp;1: Learning is an algorithm that outputs algorithms.
</p></section>
<section id="introduction-2" class="slide level2">
<h2>Introduction</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Phases</strong></p>
</div>
<div class="callout-content">
<p>Learning usually consists of two phases:</p>
<ul>
<li>the <strong>training</strong> phase, where we search for an algorithm that performs well on past instances of the problem (training data)</li>
<li>the <strong>testing</strong> phase, where we deploy our learned algorithm to solve new instances of the problem.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="learning-from-examples" class="slide level2">
<h2>Learning from Examples</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Example</strong></p>
</div>
<div class="callout-content">
<p>Imagine you find an ancient mathematics text: <span class="math display">\[\begin{aligned}
    2 \star 3 &amp;= 36\nonumber \\
    7 \star 1 &amp;= 49\nonumber \\
    5 \star 2 &amp;= 100\nonumber \\
    2 \star 2 &amp;= 16\nonumber
\end{aligned}
\]</span></p>
</div>
</div>
</div>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Question</strong></p>
</div>
<div class="callout-content">
<p>What do you think <span class="math inline">\(\star\)</span> represents?</p>
</div>
</div>
</div>
</section>
<section id="learning-from-examples-1" class="slide level2">
<h2>Learning from Examples</h2>

<img data-src="./img/intro_to_learning/star_symbol_learning.png" class="r-stretch quarto-figure-center" id="fig-star_symbol_learning"><p class="caption">
Figure&nbsp;2: How your brain may have solved the star problem.
</p></section>
<section id="learning-from-examples-2" class="slide level2">
<h2>Learning from Examples</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Tip</strong></p>
</div>
<div class="callout-content">
<p>This kind of learning, where you observe example input-output behavior and infer a functional mapping that explains this behavior, is called <strong>supervised learning</strong>.</p>
<p>Another name for this kind of learning is <strong>fitting a model</strong> to data.</p>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Non-computability</strong></p>
</div>
<div class="callout-content">
<p>Some things are not learnable from examples, such as noncomputable functions. An example of a noncomputable function is a function that takes as input a program and outputs a 1 if the program will eventually finish running, and a 0 if it will run forever.</p>
</div>
</div>
</div>
</section>
<section id="learning-from-examples-3" class="slide level2">
<h2>Learning from Examples</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Example - formal definition</strong></p>
</div>
<div class="callout-content">
<p>A formal definition of <em>example</em>, is an {<code>input</code>, <code>output</code>} pair.</p>
<p>The examples you were given for <span class="math inline">\(\star\)</span> consisted of four such pairs:</p>
<p><span class="math display">\[\begin{aligned}
    &amp;\{\texttt{input:} [2,3], \texttt{output:} 36\}\nonumber \\
    &amp;\{\texttt{input:} [7,1], \texttt{output:} 49\}\nonumber \\
    &amp;\{\texttt{input:} [5,2], \texttt{output:} 100\}\nonumber \\
    &amp;\{\texttt{input:} [2,2], \texttt{output:}16\}\nonumber
\end{aligned}\]</span></p>
</div>
</div>
</div>
</section>
<section id="learning-from-examples-4" class="slide level2">
<h2>Learning from Examples</h2>

<img data-src="./img/intro_to_learning/inpainting_example.png" class="r-stretch quarto-figure-center" id="fig-intro_to_learning-inpainting_example"><p class="caption">
Figure&nbsp;3: A complicated function that could be learned from examples.
</p></section>
<section id="learning-without-examples" class="slide level2 smaller">
<h2>Learning without Examples</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Unsupervised learning</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong>input data</strong> <span class="math inline">\(\{x^{(i)}\}^N_{i=1}\)</span> - given</li>
<li><strong>target outputs</strong> <span class="math inline">\(\{y^{(i)}\}^N_{i=1}\)</span> - unknown.</li>
</ul>
<p>Learner has to come up with a model or representation of the input data that has useful properties, as measured by some <strong>objective function</strong>.</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Reinforcement learning</strong></p>
</div>
<div class="callout-content">
<p>We suppose that we are given an <strong>reward function</strong>: <span class="math display">\[
r: \mathcal{Y} \rightarrow \mathbb{R}.
\]</span></p>
<p>The learner tries to come up with a function that <strong>maximizes rewards</strong>.</p>
</div>
</div>
</div>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Difference</strong></p>
</div>
<div class="callout-content">
<p>Unsupervised learning has access to training data whereas reinforcement learning usually does not; instead the reinforcement learner has to collect its own training data.</p>
</div>
</div>
</div>
</section>
<section id="key-ingredients" class="slide level2">
<h2>Key Ingredients</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Ingredients</strong></p>
</div>
<div class="callout-content">
<ol type="1">
<li><p>What does it mean for the learner to succeed, or, at least, to perform well?</p></li>
<li><p>What is the set of possible mappings from inputs to outputs that we will search over?</p></li>
<li><p><em>How</em>, exactly, do we search the hypothesis space for a specific mapping that maximizes the objective?</p></li>
</ol>
</div>
</div>
</div>
<div class="centered">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="./img/intro_to_learning/key_ingredients.png" class="quarto-figure quarto-figure-center" height="320"></p>
</figure>
</div>
</div>
</section>
<section id="key-ingredients-1" class="slide level2">
<h2>Key Ingredients</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Learner’s algorithm</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
f: \mathcal{X} \rightarrow \mathcal{Y},
\]</span> Commonly, <span class="math inline">\(f\)</span> is referred to as the <strong>learned function</strong>.</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Learner’s objective</strong></p>
</div>
<div class="callout-content">
<p>Function that scores model outputs: <span class="math display">\[
\mathcal{L}: \mathcal{Y} \rightarrow \mathbb{R},
\]</span> or function that compares model outputs to target answers: <span class="math display">\[
\mathcal{L}: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}.
\]</span></p>
<p>We will interchangeably call this <span class="math inline">\(\mathcal{L}\)</span> either the <strong>objective function</strong>, the <strong>loss function</strong>, or the <strong>loss</strong>.</p>
</div>
</div>
</div>
<!-- ::: aside -->
<!-- A loss almost always refers to an objective we seek to *minimize*, whereas an objective function can be used to describe objectives we seek to minimize as well as those we seek to maximize. -->
<!-- ::: -->
</section>
<section id="hypothesis-space" class="slide level2">
<h2>Hypothesis space</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Description</strong></p>
</div>
<div class="callout-content">
<p>The hypothesis space is a set <span class="math inline">\(\mathcal{F}\)</span> of all the possible functions considered by the learner.</p>
<p>Examples:</p>
<ol type="1">
<li>All mappings from <span class="math inline">\(\mathbb{R}^2 \rightarrow \mathbb{R}\)</span></li>
<li>All functions <span class="math inline">\(\mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}_{\geq 0}\)</span> that satisfy the conditions of being a distance metric.</li>
</ol>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Parameterization</strong></p>
</div>
<div class="callout-content">
<p>We may say that our <em>parameterized</em> hypothesis space is <span class="math display">\[
y = \theta_1 x + \theta_0m
\]</span> where <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span> are the parameters.</p>
<p>This example corresponds to the space of affine functions from <span class="math inline">\(\mathbb{R} \rightarrow \mathbb{R}\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="hypothesis-space-1" class="slide level2">
<h2>Hypothesis space</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Parameterization: another way</strong></p>
</div>
<div class="callout-content">
<p>Another choice could be <span class="math display">\[
y = \theta_2\theta_1 x + \theta_0,
\]</span> with parameters <span class="math inline">\(\theta_0\)</span>, <span class="math inline">\(\theta_1\)</span>, and <span class="math inline">\(\theta_2\)</span>: <strong>same space</strong>, but <strong>different parameterizations</strong>!</p>
</div>
</div>
</div>

<aside><div>
<p><strong>Overparameterized</strong> models, where you use more parameters than the minimum necessary to fit the data, are especially important in modern computer vision; most neural networks are overparameterized.</p>
</div></aside></section>
<section id="empirical-risk-minimization" class="slide level2">
<h2>Empirical Risk Minimization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>ERM model</strong></p>
</div>
<div class="callout-content">
<p>Learn a function predicting <span class="math inline">\(\mathbf{y}\)</span> from <span class="math inline">\(\mathbf{x}\)</span> given many training examples <span class="math inline">\(\{\mathbf{x}^{(i)},\mathbf{y}^{(i)}\}^N_{i=1}\)</span>.</p>
<p>The idea is to minimize the average error (i.e., risk) we incur over all the training data (i.e., empirical distribution). The ERM problem is stated as follows:</p>
<p><span class="math display">\[
\begin{aligned}
    \mathop{\mathrm{arg\,min}}_{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^N \mathcal{L}(f(\mathbf{x}^{(i)}),\mathbf{y}^{(i)}) \quad\triangleleft\quad \text{ERM}
\end{aligned}
\]</span></p>
<p>Here, <span class="math inline">\(\mathcal{F}\)</span> is the hypothesis space, <span class="math inline">\(\mathcal{L}\)</span> is the loss function, and <span class="math inline">\(\{\mathbf{x}^{(i)}, \mathbf{y}^{(i)}\}_{i=1}^N\)</span> is the training data (example {<code>input</code>, <code>output</code>} pairs), and <span class="math inline">\(f\)</span> is the learned function.</p>
</div>
</div>
</div>
</section>
<section id="learning-as-probabilistic-inference" class="slide level2">
<h2>Learning as Probabilistic Inference</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Probabilistic inference</strong></p>
</div>
<div class="callout-content">
<p>We can interpret ERM as doing maximum likelihood probabilistic inference.</p>
<p>In this interpretation, we are trying to infer the hypothesis <span class="math inline">\(f\)</span> that assigns the highest probability to the data.</p>
<p>For a model that predicts <span class="math inline">\(\mathbf{y}\)</span> given <span class="math inline">\(\mathbf{x}\)</span>, the max likelihood <span class="math inline">\(f\)</span> is:</p>
<p><span class="math display">\[\begin{aligned}
    \mathop{\mathrm{arg\,max}}_f p\big(\{\mathbf{y}^{(i)}\}_{i=1}^N \bigm | \{\mathbf{x}^{(i)}\}_{i=1}^N, f\big) \quad\quad \triangleleft \quad\text{Max likelihood learning}
\end{aligned}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="learning-as-probabilistic-inference-1" class="slide level2">
<h2>Learning as Probabilistic Inference</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Probabilistic inference</strong></p>
</div>
<div class="callout-content">
<p>The term <span class="math inline">\(p\big(\{\mathbf{y}^{(i)}\}_{i=1}^N \bigm | \{\mathbf{x}^{(i)}\}_{i=1}^N, f\big)\)</span> is called the <strong>likelihood</strong> of the <span class="math inline">\(\mathbf{y}\)</span> values given the model <span class="math inline">\(f\)</span> and the observed <span class="math inline">\(\mathbf{x}\)</span> values, and maximizing this quantity is called <strong>maximum likelihood learning</strong>.</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>MAP learning</strong></p>
</div>
<div class="callout-content">
<p>When a prior <span class="math inline">\(p(f)\)</span> is used in conjunction with a likelihood function, we arrive at <strong>maximum a posteriori learning</strong> (<strong>MAP learning</strong>), which infers the most probable hypothesis given the training data:</p>
<p><span class="math display">\[\begin{aligned}
    &amp;\mathop{\mathrm{arg\,max}}_f p\big(f \bigm | \{\mathbf{x}^{(i)}, \mathbf{y}^{(i)}\}_{i=1}^N\big) \quad\quad \triangleleft \quad \text{MAP learning}\\
    &amp; = \mathop{\mathrm{arg\,max}}_f p\big(\{\mathbf{y}^{(i)}\}_{i=1}^N \bigm | \{\mathbf{x}^{(i)}\}_{i=1}^N, f\big)p\big(f\big) \quad\quad \triangleleft \quad \text{by Bayes' rule}
\end{aligned}\]</span></p>
</div>
</div>
</div>
</section>
<section>
<section id="examples" class="title-slide slide level1 center">
<h1>Examples</h1>

</section>
<section id="linear-least-squares-regression" class="slide level2">
<h2>Linear Least-Squares Regression</h2>

<img data-src="./img/intro_to_learning/ols_train_test.png" style="width:70.0%" class="r-stretch quarto-figure-center" id="fig-intro_to_learning-ols_train_test"><p class="caption">
Figure&nbsp;4: The goal of learning is to use the training data to predict the <span class="math inline">\(y\)</span> value of the test query. In our example we find that for every 1 degree increase in temperature, we can expect <span class="math inline">\(\sim 10\)</span> more people to go to the beach.
</p></section>
<section id="linear-least-squares-regression-1" class="slide level2">
<h2>Linear Least-Squares Regression</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Hypothesis space</strong></p>
</div>
<div class="callout-content">
<p>The relationship between <span class="math inline">\(x\)</span> and our predictions <span class="math inline">\(\hat{y}\)</span> of <span class="math inline">\(y\)</span> has the form <span class="math inline">\(\hat{y} = f_{\theta}(x) = \theta_1 x + \theta_0\)</span>.</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Parameterization</strong></p>
</div>
<div class="callout-content">
<p>This hypothesis space is parameterized by a two scalars, <span class="math inline">\(\theta_0, \theta_1 \in \mathbb{R}\)</span>, the intercept and slope of the line.</p>
<p>We denote <span class="math inline">\(\theta = [\theta_0, \theta_1]\)</span>.</p>
</div>
</div>
</div>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Important</strong></p>
</div>
<div class="callout-content">
<p>Learning consists of finding the value of these parameters that maximizes the objective.</p>
</div>
</div>
</div>
</section>
<section id="linear-least-squares-regression-2" class="slide level2">
<h2>Linear Least-Squares Regression</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Objective</strong></p>
</div>
<div class="callout-content">
<p>Our <em>objective</em> is that <span class="math inline">\((\hat{y}^{(i)} - y^{(i)})^2\)</span> should be small for all training examples <span class="math inline">\(\{x^{(i)}, y^{(i)}\}_{i=1}^N\)</span>. We call this objective the <span class="math inline">\(L_2\)</span> loss:</p>
<p><span class="math display">\[\begin{aligned}
    J(\theta) &amp;= \sum_i \mathcal{L}(\hat{y}^{(i)}, y^{(i)})\\
    &amp;\quad \mathcal{L}(\hat{y}, y) = (\hat{y} - y)^2 \quad\quad \triangleleft \quad L_2 \text{ loss}
\end{aligned}\]</span></p>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p><span class="math inline">\(J(\theta)\)</span> will denote the total objective; <span class="math inline">\(\mathcal{L}\)</span> will denote the loss per datapoint. That is <span class="math display">\[
J(\theta) = \sum_{i=1}^N \mathcal{L}(f_{\theta}(x^{(i)}), y^{(i)}).
\]</span></p>
</div>
</div>
</div>
</section>
<section id="linear-least-squares-regression-3" class="slide level2">
<h2>Linear Least-Squares Regression</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Problem statement</strong></p>
</div>
<div class="callout-content">
<p>The full learning problem is as follows: <span class="math display">\[\begin{aligned}
    \theta^* = \mathop{\mathrm{arg\,min}}_{\theta} \sum_{i=1}^N (\theta_1 x^{(i)} + \theta_0 - y^{(i)})^2.
\end{aligned}
\]</span></p>
</div>
</div>
</div>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Random solution?</strong></p>
</div>
<div class="callout-content">
<p>A first idea might be “try a bunch of random values for <span class="math inline">\(\theta\)</span> and return the one that maximizes the objective.”</p>
<p><strong>Will be slow!</strong></p>
</div>
</div>
</div>
</section>
<section id="linear-least-squares-regression-4" class="slide level2">
<h2>Linear Least-Squares Regression</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Calculus way</strong></p>
</div>
<div class="callout-content">
<p>We are trying to find the minimum of the objective <span class="math inline">\(J(\theta)\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
    J(\theta) = \sum_{i=1}^N (\theta_1 x^{(i)} + \theta_0 - y^{(i)})^2.
\end{aligned}\]</span></p>
<p>This function can be rewritten as <span class="math display">\[\begin{aligned}
    J(\theta) = (\mathbf{y} - \mathbf{X}\theta)^\mathsf{T}(\mathbf{y} - \mathbf{X}\theta),
\end{aligned}
\]</span> <span class="math display">\[\begin{aligned}
\mathbf{X} =
\begin{bmatrix}
    1 &amp; x^{(1)}  \\
    1 &amp; x^{(2)} \\
    \vdots &amp; \vdots \\
    1 &amp; x^{(N)}
\end{bmatrix}
\quad
\mathbf{y} =
\begin{bmatrix}
    y^{(1)}  \\
    y^{(2)} \\
    \vdots \\
    y^{(N)}
\end{bmatrix}
\quad
\theta =
\begin{bmatrix}
    \theta_0 \\
    \theta_1
\end{bmatrix}.
\end{aligned}\]</span></p>
</div>
</div>
</div>
</section>
<section id="linear-least-squares-regression-5" class="slide level2">
<h2>Linear Least-Squares Regression</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Calculus way</strong></p>
</div>
<div class="callout-content">
<p>The <span class="math inline">\(J\)</span> is a quadratic form, which has a single global minimum where the derivative is zero, and no other points where the derivative is zero. The derivative is: <span class="math display">\[\begin{aligned}
    \frac{\partial J(\theta)}{\partial \theta} =  2(\mathbf{X}^\mathsf{T}\mathbf{X} \theta - \mathbf{X}^\mathsf{T}\mathbf{y}).
\end{aligned}
\]</span></p>
<p>We set this derivative to zero and solve for <span class="math inline">\(\theta^*\)</span>: <span class="math display">\[
\begin{aligned}
    &amp;2(\mathbf{X}^\mathsf{T}\mathbf{X} \theta^* - \mathbf{X}^\mathsf{T}\mathbf{y}) = 0\\
&amp;\mathbf{X}^\mathsf{T}\mathbf{X} \theta^* = \mathbf{X}^\mathsf{T}\mathbf{y}\\
&amp;\theta^* = (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\mathbf{y}.
\end{aligned}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="linear-least-squares-regression-6" class="slide level2">
<h2>Linear Least-Squares Regression</h2>

<img data-src="./img/intro_to_learning/ols_fit.png" class="r-stretch quarto-figure-center" id="fig-intro_to_learning-ols_fit"><p class="caption">
Figure&nbsp;5: The <span class="math inline">\(\theta^*\)</span> defines the best fitting line to our data. A best fit line is a visualization of a function <span class="math inline">\(f_{\theta}\)</span>, that predicts the <span class="math inline">\(y\)</span>-value for each input <span class="math inline">\(x\)</span>-value.
</p></section>
<section id="linear-least-squares-regression-7" class="slide level2">
<h2>Linear Least-Squares Regression</h2>

<img data-src="./img/intro_to_learning/ols_summary.png" class="r-stretch quarto-figure-center" id="fig-intro_to_learning-ols_system_diagram"><p class="caption">
Figure&nbsp;6: Linear regression finds a line that predicts the training data’s <span class="math inline">\(y\)</span>-values from its <span class="math inline">\(x\)</span>-values.
</p></section>
<section id="program-induction" class="slide level2">
<h2>Program Induction</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Program induction</strong></p>
</div>
<div class="callout-content">
<p><strong>Program induction</strong>: one of the broadest classes of learning algorithm. In this setting, our hypothesis space may be all Python programs.</p>
</div>
</div>
</div>

<img data-src="./img/intro_to_learning/ols_system_diagram.png" class="r-stretch quarto-figure-center" id="fig-intro_to_learning-ols_system_diagram"><p class="caption">
Figure&nbsp;7: Linear regression finds a line that predicts the training data’s <span class="math inline">\(y\)</span>-values from its <span class="math inline">\(x\)</span>-values.
</p></section>
<section id="program-induction-2" class="slide level2">
<h2>Program Induction</h2>

<img data-src="./img/intro_to_learning/program_induction_system_diagram.png" class="r-stretch quarto-figure-center" id="fig-intro_to_learning-program_induction_system_diagram"><p class="caption">
Figure&nbsp;8: Python program induction finds a Python program that predicts the training data’s <span class="math inline">\(y\)</span>-values from its <span class="math inline">\(x\)</span>-values.
</p></section>
<section id="classification-and-softmax-regression" class="slide level2">
<h2>Classification and Softmax Regression</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Definition</strong></p>
</div>
<div class="callout-content">
<p>A common problem in computer vision is to recognize objects. Our input is an image <span class="math inline">\(\mathbf{x}\)</span>, and our target output is a class label <span class="math inline">\(\mathbf{y}\)</span></p>
</div>
</div>
</div>

<img data-src="./img/intro_to_learning/image_classification.png" class="r-stretch quarto-figure-center" id="fig-intro_to_learning-image_classification"><p class="caption">
Figure&nbsp;9: Image classification.
</p></section>
<section id="classification-and-softmax-regression-1" class="slide level2">
<h2>Classification and Softmax Regression</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Input</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[\mathbf{x} \in \mathbb{R}^{H \times W \times 3},
\]</span> where <span class="math inline">\(H\)</span> is image height and <span class="math inline">\(W\)</span> is image width.</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Output</strong></p>
</div>
<div class="callout-content">
<p>Let <span class="math inline">\(\mathbf{y}\)</span> be a <span class="math inline">\(K\)</span>-dimensional vector, for <span class="math inline">\(K\)</span> possible classes, such that: <span class="math display">\[
y_k = \begin{cases}1, \; \text{if} \; \mathbf{y} \; \text{represents class}\; k, \\
0, \; \text{otherwise}\end{cases}
\]</span> This representation is called a <strong>one-hot code</strong>.</p>
</div>
</div>
</div>
</section>
<section id="classification-and-softmax-regression-2" class="slide level2">
<h2>Classification and Softmax Regression</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Goal</strong></p>
</div>
<div class="callout-content">
<p>Learn a function <span class="math inline">\(f_{\theta}\)</span> that output vectors <span class="math inline">\(\hat{\mathbf{y}}\)</span> that match the one-hot codes, thereby correctly classifying the input images.</p>
</div>
</div>
</div>

<img data-src="img/intro_to_learning/one_hot_codes.png" class="r-stretch quarto-figure-center"><p class="caption">An example of one-hot codes for representing <span class="math inline">\(K\)</span>=5 different classes.</p></section>
<section id="classification-and-softmax-regression-3" class="slide level2">
<h2>Classification and Softmax Regression</h2>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Loss function - version 1</strong></p>
</div>
<div class="callout-content">
<p>Perhaps we should minimize misclassifications? That would correspond to the so called <strong>0-1 loss</strong>:</p>
<p><span class="math display">\[
\begin{aligned}
    \mathcal{L}(\hat{\mathbf{y}},\mathbf{y}) = \mathbb{1}(\hat{\mathbf{y}}\neq\mathbf{y}),
\end{aligned}
\]</span> where <span class="math inline">\(\mathbb{1}\)</span> is the indicator function that evaluates to 1 if and only if its argument is true, and 0 otherwise. Unfortunately, minimizing this loss is a discrete optimization problem, and it is <strong>NP-hard</strong>.</p>
</div>
</div>
</div>
</section>
<section id="classification-and-softmax-regression-4" class="slide level2">
<h2>Classification and Softmax Regression</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Loss function - version 2</strong></p>
</div>
<div class="callout-content">
<p>Instead, people commonly use the cross-entropy loss, which is continuous and differentiable (making it easier to optimize): <span class="math display">\[\begin{aligned}
    \mathcal{L}(\hat{\mathbf{y}},\mathbf{y}) = H(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{k=1}^K y_k \log \hat{y}_k \quad\quad \triangleleft \quad \text{cross-entropy loss}
\end{aligned}\]</span></p>
</div>
</div>
</div>
</section>
<section id="classification-and-softmax-regression-5" class="slide level2">
<h2>Classification and Softmax Regression</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Interpretation</strong></p>
</div>
<div class="callout-content">
<p><span class="math inline">\(\hat{y}_k\)</span> should <em>represent the probability</em> we think the image is an image of class <span class="math inline">\(k\)</span>. Under that interpretation, minimizing cross-entropy maximizes the log likelihood of the ground truth observation <span class="math inline">\(\mathbf{y}\)</span> under our model’s prediction <span class="math inline">\(\hat{\mathbf{y}}\)</span>. <span class="math inline">\(\hat{y}\)</span> should represent a <strong>pmf</strong>.</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Probability mass function (pmf)</strong></p>
</div>
<div class="callout-content">
<p>A pmf <span class="math inline">\(\mathbf{p}\)</span>, over <span class="math inline">\(K\)</span> classes, is defined as a <span class="math inline">\(K\)</span>-dimensional vector with elements in the range <span class="math inline">\([0,1]\)</span> that sums to 1. In other words, <span class="math inline">\(\mathbf{p}\)</span> is a point on the <span class="math inline">\((K-1)\)</span>-<strong>simplex</strong>, which we denote as <span class="math inline">\(\mathbf{p} \in \vartriangle^{K-1}\)</span>.</p>
</div>
</div>
</div>

<aside><div>
<p>The <span class="math inline">\((K-1)\)</span>-simplex, <span class="math inline">\(\vartriangle^{K-1}\)</span>, is the set of all <span class="math inline">\(K\)</span>-dimensional vectors whose elements sum to 1. <span class="math inline">\(K\)</span>-dimensional one-hot codes live on the vertices of <span class="math inline">\(\vartriangle^{K-1}\)</span>.</p>
</div></aside></section>
<section id="classification-and-softmax-regression-6" class="slide level2">
<h2>Classification and Softmax Regression</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Procedure</strong></p>
</div>
<div class="callout-content">
<p>To ensure that the output of our learned function <span class="math inline">\(f_{\theta}\)</span> has this property, i.e., <span class="math inline">\(f_{\theta} \in \vartriangle^{K-1}\)</span>, we can compose two steps:</p>
<ul>
<li>first apply a function <span class="math inline">\(z_{\theta}: \mathcal{X} \rightarrow \mathbb{R}^K\)</span></li>
<li>then squash the output into the range <span class="math inline">\([0,1]\)</span> and normalize it to sum to 1.</li>
</ul>
</div>
</div>
</div>
</section>
<section id="classification-and-softmax-regression-7" class="slide level2">
<h2>Classification and Softmax Regression</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Squashing</strong></p>
</div>
<div class="callout-content">
<p>A popular way to squash is via the <strong>softmax</strong> function:</p>
<p><span class="math display">\[\begin{aligned}
    &amp;\mathbf{z} = z_{\theta}(\mathbf{x})\\
    &amp;\hat{\mathbf{y}} = \texttt{softmax}(\mathbf{z})\\
    &amp;\quad \quad \hat{y}_j = \frac{e^{-z_j}}{\sum_{i=1}^K e^{-z_k}}.
\end{aligned}\]</span></p>
<p>The values in <span class="math inline">\(\mathbf{z}\)</span> are called the <strong>logits</strong>.</p>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p>Now we have:</p>
<p><span class="math display">\[\begin{aligned}
    \hat{\mathbf{y}} = f_{\theta}(\mathbf{x}) = \texttt{softmax}(z_{\theta}(\mathbf{x}))
\end{aligned}\]</span></p>
</div>
</div>
</div>
</section>
<section id="classification-and-softmax-regression-8" class="slide level2">
<h2>Classification and Softmax Regression</h2>

<img data-src="./img/intro_to_learning/softmax_regression_diagram.png" class="r-stretch quarto-figure-center" id="fig-softmax_regression_diagram"><p class="caption">
Figure&nbsp;10: Softmax regression for image classification.
</p></section>
<section id="classification-and-softmax-regression-9" class="slide level2">
<h2>Classification and Softmax Regression</h2>

<img data-src="./img/intro_to_learning/softmax_regression_learning_problem.png" class="r-stretch quarto-figure-center" id="fig-intro_to_learning-meta_learning_diagram"><p class="caption">
Figure&nbsp;11: Learning is a meta-algorithm, an algorithm that outputs algorithms; metalearning is just learning applied to learning, and therefore it is a meta-meta-algorithm.
</p></section>
<section id="learning-to-learn" class="slide level2">
<h2>Learning to Learn</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Metalearning</strong></p>
</div>
<div class="callout-content">
<p>It’s a special case of learning where the hypothesis space is learning algorithms.</p>
<p>The goal of metalearning is to handle the case where the future problem we will encounter is itself a learning problem.</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Example</strong></p>
</div>
<div class="callout-content">
<p>Suppose that we are given the following {<code>input</code>, <code>output</code>} examples:</p>
<p><span class="math display">\[\begin{aligned}
    &amp;\{\texttt{input:} \big(x:[1,2], y:[1,2]\big), &amp;&amp;\texttt{output:} y = x\}\nonumber \\
    &amp;\{\texttt{input:} \big(x:[1,2], y:[2,4]\big), &amp;&amp;\texttt{output:} y = 2x\}\nonumber \\
    &amp;\{\texttt{input:} \big(x:[1,2], y:[0.5,1]\big), &amp;&amp;\texttt{output:} y = \frac{x}{2}\}\nonumber
\end{aligned}\]</span></p>
<p>The learner can fit these examples by learning to perform least-squares regression.</p>
</div>
</div>
</div>
</section>
<section id="learning-to-learn-1" class="slide level2">
<h2>Learning to Learn</h2>

<img data-src="./img/intro_to_learning/meta_learning_diagram.png" class="r-stretch quarto-figure-center" id="fig-intro_to_learning-meta_learning_diagram"><p class="caption">
Figure&nbsp;12: Learning is a meta-algorithm, an algorithm that outputs algorithms; metalearning is learning applied to learning: a <strong>meta-meta-algorithm</strong>.
</p></section>
<section id="learning-to-learn-2" class="slide level2">
<h2>Learning to Learn</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Recursion</strong></p>
</div>
<div class="callout-content">
<p>Notice that you can apply this idea recursively, constructing meta-meta-...-metalearners.</p>
<p>Humans perform at least three levels of this process, if not more: we have <em>evolved</em> to be <em>taught</em> in school how to <em>learn</em> quickly on our own.</p>
</div>
</div>
</div>

<aside><div>
<p><strong>Evolution</strong> is a learning algorithm according to our present definition.</p>
</div></aside></section></section>
<section>
<section id="gradient-based-learning-algorithms" class="title-slide slide level1 center">
<h1>Gradient-Based Learning Algorithms</h1>

</section>
<section id="gradient-based-learning-algorithms-1" class="slide level2">
<h2>Gradient-Based Learning Algorithms</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Setting</strong></p>
</div>
<div class="callout-content">
<p>We consider the task of minimizing a cost function <span class="math inline">\(J: \cdot \rightarrow \mathbb{R}\)</span>, which is a function that maps some arbitrary input to a scalar cost.</p>
<p>In learning problems, the domain of <span class="math inline">\(J\)</span> is the training data and the parameters <span class="math inline">\(\theta\)</span>. We will often consider the training data to be fixed and only denote the objective as a function of the parameters, <span class="math inline">\(J(\theta)\)</span>. Our goal is to solve: <span class="math display">\[
\theta^* = \arg\min_{\theta} J(\theta)
\]</span></p>
</div>
</div>
</div>
</section>
<section id="gradient-based-learning-algorithms-2" class="slide level2">
<h2>Gradient-Based Learning Algorithms</h2>

<img data-src="./img/gradient_descent/optimization_schematic.png" style="width:50.0%" class="r-stretch quarto-figure-center" id="fig-gradient_descent-optimization_schematic"><p class="caption">
Figure&nbsp;13: General optimization loop.
</p></section>
<section id="gradient-based-learning-algorithms-3" class="slide level2">
<h2>Gradient-Based Learning Algorithms</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Zeroth-order optimization</strong></p>
</div>
<div class="callout-content">
<p>The update function only gets to observe the value <span class="math inline">\(J(\theta)\)</span>. The only way, then, to find <span class="math inline">\(\theta\)</span>’s that minimize the loss is to <strong>sample</strong> different values for <span class="math inline">\(\theta\)</span> and move toward the values that are lower.</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>First-order optimization</strong></p>
</div>
<div class="callout-content">
<p>Also called <strong>gradient-based optimization</strong>: the update function takes as input the gradient of the cost with respect to the parameters at the current operating point, <span class="math inline">\(\nabla_{\theta}J(\theta)\)</span>. This reveals the <strong>direction</strong>.</p>
</div>
</div>
</div>

<aside><div>
<p>Higher-order optimization methods observe higher-order derivatives of the loss, such as the Hessian <span class="math inline">\(H\)</span>, which tells you how the landscape is locally curving.</p>
</div></aside></section>
<section id="gradient-based-learning-algorithms-4" class="slide level2">
<h2>Gradient-Based Learning Algorithms</h2>

<img data-src="./img/gradient_descent/alg1.png" class="r-stretch quarto-figure-center" id="alg-gradient_descent_basic_gradient_descent"><p class="caption">Gradient descent <code>GD</code>. Optimizing a cost function <span class="math inline">\(J: \theta \rightarrow \mathbb{R}\)</span> by descending the gradient <span class="math inline">\(\nabla_{\theta} J\)</span>.</p></section>
<section id="gradient-based-learning-algorithms-5" class="slide level2">
<h2>Gradient-Based Learning Algorithms</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Hyperparameters</strong></p>
</div>
<div class="callout-content">
<ul>
<li><strong>learning rate</strong> <span class="math inline">\(\eta\)</span>, which controls the step size (learning rate times gradient magnitude)</li>
<li>the number of steps <span class="math inline">\(K\)</span>.</li>
</ul>
<p>If the learning rate is sufficiently small and the initial parameter vector <span class="math inline">\(\theta^0\)</span> is random, then this algorithm will almost surely converge to a local minimum of <span class="math inline">\(J\)</span> as <span class="math inline">\(K \rightarrow \infty\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="gradient-based-learning-algorithms-6" class="slide level2">
<h2>Gradient-Based Learning Algorithms</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Learning Rate Schedules</strong></p>
</div>
<div class="callout-content">
<p>We are calling some function <span class="math inline">\(\texttt{lr}(\eta^0,k)\)</span> to get the learning rate on each iteration of descent: <span class="math display">\[
\eta^{k} = \texttt{lr}(\eta^0,k)
\]</span> Generally, we want an update rule where <span class="math inline">\(\eta^{k+1} &lt; \eta^k\)</span> (making smaller steps).</p>
</div>
</div>
</div>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Examples</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\begin{aligned}
    \texttt{lr}(\eta^0,k) &amp;= \beta^{-k} \eta^0 &amp;\quad\quad \triangleleft\quad \text{exponential decay}\\
    \texttt{lr}(\eta^0,k) &amp;= \beta^{-\lfloor k/M \rfloor} \eta^0 &amp;\quad\quad \triangleleft\quad \text{stepwise exponential decay}\\
    \texttt{lr}(\eta^0,k) &amp;= \frac{(K - k)}{K} \eta^0 &amp;\quad\quad \triangleleft\quad \text{linear decay}
\end{aligned}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="gradient-based-learning-algorithms-7" class="slide level2">
<h2>Gradient-Based Learning Algorithms</h2>

<img data-src="./img/gradient_descent/alg2.png" class="r-stretch quarto-figure-center" id="alg-gradient_descent_gradient_descent_with_lr_decay"><p class="caption">Gradient descent with learning rate decay algorithm.</p></section>
<section id="gradient-based-learning-algorithms-8" class="slide level2">
<h2>Gradient-Based Learning Algorithms</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Momentum</strong></p>
</div>
<div class="callout-content">
<p>Momentum means that we set the parameter update to be a direction <span class="math inline">\(\mathbf{v}^{k+1}\)</span>, given by a weighted combination of the previous update direction, <span class="math inline">\(\mathbf{v}^{k}\)</span>, plus the current negative gradient: <span class="math display">\[
\mathbf{v}^{k+1} = \mu \mathbf{v}^{k} - \eta\nabla_{\theta} J(\theta^k)
\]</span> The weight <span class="math inline">\(\mu\)</span> in this combination is a new hyperparameter, sometimes simply called the <strong>momentum</strong>.</p>
</div>
</div>
</div>
</section>
<section id="gradient-based-learning-algorithms-9" class="slide level2">
<h2>Gradient-Based Learning Algorithms</h2>

<img data-src="./img/gradient_descent/alg3.png" class="r-stretch quarto-figure-center" id="alg-gradient_descent_gradient_descent_with_momentum"><p class="caption">Gradient descent with momentum algorithm.</p></section>
<section id="gradient-based-learning-algorithms-10" class="slide level2">
<h2>Gradient-Based Learning Algorithms</h2>

<img data-src="./img/gradient_descent/momentum_out1.png" class="r-stretch quarto-figure-center" id="fig-gradient_descent-momentum_out1"><p class="caption">
Figure&nbsp;14: (left) A simple loss function <span class="math inline">\(J = \texttt{abs}(\theta)\)</span>. (right) Optimization trajectory for three different settings of momentum <span class="math inline">\(\mu\)</span>. White line indicates value of the parameter at each iteration of optimization, starting at top and progressing to bottom. Color is value of the loss. Red dot is location where loss first reaches within <span class="math inline">\(0.01\)</span> of optimal value.
</p></section>
<section id="gradient-based-learning-algorithms-11" class="slide level2 scrollable smaller">
<h2>Gradient-Based Learning Algorithms</h2>

<img data-src="./img/gradient_descent/grad_descent.png" id="fig-gradient_descent-grad_descent_simple_examples" class="r-stretch quarto-figure-center"><p class="caption">
Figure&nbsp;15
</p></section>
<section id="gradient-based-learning-algorithms-12" class="slide level2">
<h2>Gradient-Based Learning Algorithms</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Alternatives</strong></p>
</div>
<div class="callout-content">
<p>What are some other good choices for <span class="math inline">\(\mathbf{v}\)</span>?</p>
<ol type="1">
<li><p>One common idea is to set <span class="math inline">\(\mathbf{v}\)</span> to be the gradient of a <strong>surrogate loss</strong> function, which is a function, <span class="math inline">\(J_{\texttt{surr}}\)</span>, with meaningful (non-zero) gradients that approximates <span class="math inline">\(J\)</span>. An example might be a smoothed version of <span class="math inline">\(J\)</span>.</p></li>
<li><p>Another way to get <span class="math inline">\(\mathbf{v}\)</span> is to compute it by sampling perturbations of <span class="math inline">\(\theta\)</span>, and seeing which perturbation leads to lower loss. In this strategy, we evaluate <span class="math inline">\(J(\theta+\epsilon)\)</span> for a set of perturbations <span class="math inline">\(\epsilon\)</span>, then move toward the <span class="math inline">\(\epsilon\)</span>’s that decreased the loss. Approaches of this kind are sometimes called <strong>evolution strategies</strong>.</p></li>
</ol>
</div>
</div>
</div>
</section>
<section id="gradient-based-learning-algorithms-13" class="slide level2">
<h2>Gradient-Based Learning Algorithms</h2>

<img data-src="./img/gradient_descent/alg4.png" class="r-stretch quarto-figure-center" id="alg-gradient_descent_ES"><p class="caption">Evolution strategy algorithm.</p></section>
<section id="gradient-based-learning-algorithms-14" class="slide level2">
<h2>Gradient-Based Learning Algorithms</h2>

<img data-src="./img/gradient_descent/sampling_out1.png" class="r-stretch quarto-figure-center" id="fig-gradient_descent-sampling_out1"><p class="caption">
Figure&nbsp;16: Using evolution strategies to minimize a nondifferentiable (zero-gradient) loss, using <span class="math inline">\(\sigma=1\)</span>, <span class="math inline">\(M=10\)</span>, and <span class="math inline">\(\eta=0.02\)</span>.
</p></section>
<section id="gradient-based-learning-algorithms-15" class="slide level2">
<h2>Gradient-Based Learning Algorithms</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Gradient Clipping</strong></p>
</div>
<div class="callout-content">
<div id="alg-gradient_descent_grad_clipping" class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="./img/gradient_descent/alg5.png"></p>
<figcaption>Gradient clipping algorithm.</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Note</strong></p>
</div>
<div class="callout-content">
<p><code>clip</code> is the “clipping” function: <span class="math inline">\(\texttt{clip}(v, -m, m) = \max(\min(v,m),-m)\)</span></p>
</div>
</div>
</div>
</section>
<section id="gradient-based-learning-algorithms-16" class="slide level2">
<h2>Gradient-Based Learning Algorithms</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Gradient Clipping</strong></p>
</div>
<div class="callout-content">
<div id="fig-gradient_descent-clipped_out1" class="quarto-float quarto-figure quarto-figure-center">
<figure class="quarto-float quarto-float-fig">
<div aria-describedby="fig-gradient_descent-clipped_out1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img data-src="./img/gradient_descent/clipped_out1.png" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gradient_descent-clipped_out1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: Using <code>GD</code> with clipping to minimize a loss with exploding gradients, using <span class="math inline">\(m=0.1\)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="stochastic-gradient-descent" class="slide level2">
<h2>Stochastic Gradient Descent</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Expensive computation</strong></p>
</div>
<div class="callout-content">
<p>For typical learning problems, <span class="math inline">\(\nabla_{\theta} J(\theta, \{\mathbf{x}^{(i)}, \mathbf{y}^{(i)}\}_{i=1}^N)\)</span> decomposes as follows: <span class="math display">\[
\begin{align}
    \nabla_{\theta} J(\theta, \{\mathbf{x}^{(i)}, \mathbf{y}^{(i)}\}_{i=1}^N) &amp;=
    \nabla_{\theta} \frac{1}{N}\sum_{i=1}^N \mathcal{L}(f_{\theta}(\mathbf{x}^{(i)}), \mathbf{y}^{(i)}) = \frac{1}{N}\sum_{i=1}^N \nabla_{\theta} \mathcal{L}(f_{\theta}(\mathbf{x}^{(i)}), \mathbf{y}^{(i)})
\end{align}
\]</span></p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Batching</strong></p>
</div>
<div class="callout-content">
<p>Suppose instead we randomly subsample a <em>batch</em> of terms from this sum, <span class="math inline">\(\{\mathbf{x}^{(b)}, \mathbf{y}^{(b)}\}_{b=1}^B\)</span>, where <span class="math inline">\(B\)</span> is the <strong>batch size</strong>. We then compute an <em>estimate</em> of the total gradient as the average gradient over this batch as follows: <span class="math display">\[
\begin{align}
    \tilde{\mathbf{g}} = \frac{1}{N}\sum_{b=1}^B \nabla_{\theta} \mathcal{L}(f_{\theta}(\mathbf{x}^{(b)}), \mathbf{y}^{(b)})
\end{align}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="stochastic-gradient-descent-1" class="slide level2">
<h2>Stochastic Gradient Descent</h2>

<img data-src="./img/gradient_descent/alg6.png" class="r-stretch quarto-figure-center" id="alg-gradient_descent_SGD"><p class="caption">Stochastic gradient descent algorithm. Stochastic gradient descent estimates the gradient from a stochastic subset (batch) of the full training data, and makes an update on that basis.</p></section>
<section id="stochastic-gradient-descent-2" class="slide level2">
<h2>Stochastic Gradient Descent</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Properties</strong></p>
</div>
<div class="callout-content">
<ul>
<li>Because each step of descent is somewhat random, <span class="math inline">\(\texttt{SGD}\)</span> can jump over small bumps in the loss landscape, as long those bumps disappear for some randomly sampled batches</li>
<li><span class="math inline">\(\texttt{SGD}\)</span> can implicitly regularize the learning problem. For example, for linear problems (i.e., <span class="math inline">\(f_\theta\)</span> is linear), then if there are multiple parameter settings that minimize the loss, <span class="math inline">\(\texttt{SGD}\)</span> will often converge to the solution with minimum parameter norm</li>
</ul>
</div>
</div>
</div>
</section></section>
<section>
<section id="the-problem-of-generalization" class="title-slide slide level1 center">
<h1>The Problem of Generalization</h1>

</section>
<section id="the-problem-of-generalization-1" class="slide level2">
<h2>The Problem of Generalization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Train vs test</strong></p>
</div>
<div class="callout-content">
<p>So far, we have described learning as an optimization problem: maximize an objective over the <em>training set</em>. But this is not our actual goal. Our goal is to maximize the objective over the <em>test set</em>.</p>
</div>
</div>
</div>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Overfitting</strong></p>
</div>
<div class="callout-content">
<p>Happens when we fit to properties in the training data that do not exist in the test data.</p>
<p>This means that what we learned about the training data does not <strong>generalize</strong> to the test data.</p>
</div>
</div>
</div>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Underfitting</strong></p>
</div>
<div class="callout-content">
<p>Learner failed to optimize the objective on the training data.</p>
</div>
</div>
</div>
</section>
<section id="the-problem-of-generalization-2" class="slide level2">
<h2>The Problem of Generalization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Polynomial regression</strong></p>
</div>
<div class="callout-content">
<p>The hypothesis space is polynomial functions rather than linear functions, that is, <span class="math display">\[
\begin{aligned}
y = f_{\theta}(x) = \sum_{k=0}^K \theta_k x^k
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(K\)</span>, the degree of the polynomial, is a hyperparameter of the hypothesis space.</p>
</div>
</div>
</div>
</section>
<section id="the-problem-of-generalization-3" class="slide level2">
<h2>The Problem of Generalization</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Equivalence of polynomial and linear regression</strong></p>
</div>
<div class="callout-content">
<p>Let us consider the setting where we use the least-squares (<span class="math inline">\(L_2\)</span>) loss function.</p>
<p>We can see this by rewriting the polynomial as:</p>
<p><span class="math display">\[
\sum_{k=0}^K \theta_k x^k = \theta^\mathsf{T}\phi(x), \; \phi(x) = \begin{bmatrix}
  1 \\ x \\ x^2 \\ \vdots \\ x^K
\end{bmatrix}
\]</span></p>
<p>Now the form of <span class="math inline">\(f_{\theta}\)</span> is <span class="math inline">\(f_{\theta}(x) = \theta^\mathsf{T}\phi(x)\)</span>, <em>which is a linear function in the parameters <span class="math inline">\(\theta\)</span></em>. Therefore, if we <em>featurize</em> <span class="math inline">\(x\)</span>, representing each datapoint <span class="math inline">\(x\)</span> with a feature vector <span class="math inline">\(\phi(x)\)</span>, then we have arrived at a linear regression problem in this feature space.</p>
</div>
</div>
</div>
</section>
<section id="the-problem-of-generalization-4" class="slide level2">
<h2>The Problem of Generalization</h2>
<div class="callout callout-note no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Equivalence of polynomial and linear regression</strong></p>
</div>
<div class="callout-content">
<p>So, the learning problem, and closed form optimizer, for <span class="math inline">\(L_2\)</span> polynomial regression looks almost identical to that of <span class="math inline">\(L_2\)</span> linear regression:</p>
<p>where <span class="math display">\[\mathbf{\Phi} =
     \begin{bmatrix}
        1 &amp; x^{(1)} &amp; x^{(1)^2} &amp; ... &amp; x^{(1)^K} \\
        1 &amp; x^{(2)} &amp; x^{(2)^2} &amp; ... &amp; x^{(2)^K} \\
        \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
        1 &amp; x^{(N)} &amp; x^{(N)^2} &amp; ... &amp; x^{(N)^K}  \\
    \end{bmatrix}\]</span></p>
<p>The matrix <span class="math inline">\(\mathbf{\Phi}\)</span> is an array of the features (columns) for each datapoint (rows). It plays the same role as data matrix <span class="math inline">\(\mathbf{X}\)</span> did earlier; in fact we often call matrices of the feature representations of each datapoint also as a <strong>data matrix</strong>.</p>
</div>
</div>
</div>
</section>
<section id="the-problem-of-generalization-5" class="slide level2">
<h2>The Problem of Generalization</h2>

<img data-src="./img/problem_of_generalization/under_and_overfitting.png" class="r-stretch quarto-figure-center" id="fig-under_and_overfitting"><p class="caption">
Figure&nbsp;18: Underfitting and overfitting.
</p></section>
<section id="the-problem-of-generalization-6" class="slide level2">
<h2>The Problem of Generalization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Data generating process</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[
\begin{aligned}
    Y &amp;= X^2 + 1 &amp;\triangleleft \quad\text{true underlying relationship}\\
    \epsilon &amp;\sim \mathcal{N}(0,1) &amp;\triangleleft \quad\text{observation noise}\\
    Y^\prime &amp;= Y + \epsilon &amp;\triangleleft \quad\text{noisy observations}\\
    x,y &amp;\sim p(X,Y^{\prime}) &amp;\triangleleft \quad\text{data-generating process}
\end{aligned}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="the-problem-of-generalization-7" class="slide level2">
<h2>The Problem of Generalization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Why does overfitting happen?</strong></p>
</div>
<div class="callout-content">
<p>It’s because for <span class="math inline">\(K=10\)</span> the curve can become wiggly enough to not just fit the true underlying relationship but also to <em>fit the noise</em>, the minor offsets <span class="math inline">\(\epsilon\)</span> around the green line.</p>
<p>This noise is <em>a property of the training data that does not generalize to the test data</em>; the test data will have different observation noise.</p>
</div>
</div>
</div>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Multiple hypotheses</strong></p>
</div>
<div class="callout-content">
<p>For <span class="math inline">\(K=10\)</span> there are many hypotheses (polynomial functions) that perfectly the data (true function + noise) – there is insufficient data for the objective to uniquely identify one of the hypotheses to be the best. Because of this, the hypothesis output by the optimizer may be an arbitrary one.</p>
</div>
</div>
</div>
</section>
<section id="the-problem-of-generalization-8" class="slide level2">
<h2>The Problem of Generalization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Approximation error</strong></p>
</div>
<div class="callout-content">
<p>Let <span class="math inline">\(\{x_{(\texttt{train})}^{(i)}, y_{(\texttt{train})}^{(i)}\}_{i=1}^N\)</span> be our training data set (the black points). Then the approximation error <span class="math inline">\(J_{\texttt{approx}}\)</span> is defined as the total cost incurred on this training data:</p>
<p><span class="math display">\[\begin{aligned}
    J_{\texttt{approx}} = \frac{1}{N} \sum_{i=1}^N \mathcal{L}(f_{\theta}(x_{(\texttt{train})}^{(i)}), y_{(\texttt{train})}^{(i)})
\end{aligned}\]</span> It is the gap between the black line and the training data points.</p>
</div>
</div>
</div>

<aside><div>
<p>Notice that approximation error is the cost function we minimize in empirical risk minimization.</p>
</div></aside></section>
<section id="the-problem-of-generalization-9" class="slide level2">
<h2>The Problem of Generalization</h2>
<div class="callout callout-warning no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Generalization error</strong></p>
</div>
<div class="callout-content">
<p>The expected cost we would incur if we sampled a new test point at random from the true data generating process. Generalization error is often approximated by measuring performance on a heldout , <span class="math inline">\(\{x_{(\texttt{val})}^{(i)}, y_{(\texttt{val})}^{(i)}\}_{i=1}^N\)</span>, which can simply be a subset of the data that we don’t use for training or testing:</p>
<p><span class="math display">\[\begin{aligned}
    J_{\texttt{gen}} &amp;= \mathbb{E}_{x,y \sim p_{\texttt{data}}} [ \mathcal{L}(f_{\theta}(x), y)]\\
                        &amp;\approx \frac{1}{N} \sum_{i=1}^N \mathcal{L}(f_{\theta}(x_{(\texttt{val})}^{(i)}), y_{(\texttt{val})}^{(i)})
\end{aligned}\]</span> It is the gap between the black line and the green line.</p>
</div>
</div>
</div>
</section>
<section id="the-problem-of-generalization-10" class="slide level2">
<h2>The Problem of Generalization</h2>

<img data-src="./img/problem_of_generalization/under_and_overfitting_vs_polyK.png" class="r-stretch quarto-figure-center" id="fig-under_and_overfitting_vs_polyK"><p class="caption">
Figure&nbsp;19: Approximation error <code>approx</code> versus generalization error <code>gen</code> for polynomial regression of order <span class="math inline">\(K\)</span>. Here we measured error as the proportion of validation points that are mispredicted (defined as having an <span class="math inline">\(L_2\)</span> prediction error greater than 0.25).
</p></section>
<section id="regularization" class="slide level2">
<h2>Regularization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Goldilocks principle</strong></p>
</div>
<div class="callout-content">
<p>We should prefer hypotheses (functions <span class="math inline">\(f\)</span>) that are sufficiently expressive to fit the data, but not so flexible that they can overfit the data.</p>
</div>
</div>
</div>

<img data-src="img/goldilocks.png" class="quarto-figure quarto-figure-center r-stretch"></section>
<section id="regularization-1" class="slide level2">
<h2>Regularization</h2>
<div class="callout callout-important no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Regularization</strong></p>
</div>
<div class="callout-content">
<p>Mechanisms that penalize function complexity so that we avoid learning too flexible a function that overfits. Regularizers embody the principle of <strong>Occam’s razor</strong>.</p>
<p>The general form of a regularized objective is: <span id="eq-problem_of_generalization-regularized_objective"><span class="math display">\[\begin{aligned}
    J(\theta) = \overbrace{\frac{1}{N} \sum^N_{i=1} \mathcal{L}(f_{\theta}(x)^{(i)}, y^{(i)})}^\text{data fit loss} + \underbrace{\lambda R(\theta)}_\text{regularizer} \quad\quad\triangleleft \quad\text{regularized objective function}
\end{aligned} \qquad(1)\]</span></span> where <span class="math inline">\(\lambda\)</span> is a hyperparameter that controls the strength of the regularization.</p>
</div>
</div>
</div>

<aside><div>
<p><strong>Bayesian Occam’s razor</strong>: more complex hypothesis spaces must cover more possible hypotheses, and therefore must assign less prior mass to any single hypothesis.</p>
</div></aside></section>
<section id="regularization-3" class="slide level2">
<h2>Regularization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Norm penalization</strong></p>
</div>
<div class="callout-content">
<p>One of the most common regularizers is to penalize the <span class="math inline">\(L_p\)</span> norm of the parameters of our model, <span class="math inline">\(\theta\)</span>: <span class="math display">\[
R(\theta) = \left\lVert\theta\right\rVert_{p}.
\]</span></p>
<p>The <span class="math inline">\(L_p\)</span>-norm of <span class="math inline">\(\mathbf{x}\)</span> is <span class="math inline">\((\sum_i |x_i|^{p})^{\frac{1}{p}}\)</span>. The <span class="math inline">\(L_2\)</span>-norm is the familiar least-squares objective.</p>
</div>
</div>
</div>
</section>
<section id="regularization-4" class="slide level2">
<h2>Regularization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Regularizers as Probabilistic Priors</strong></p>
</div>
<div class="callout-content">
<p>Regularizers can be interpreted as <strong>priors</strong> that prefer, a priori (before looking at the data), some solutions over others.</p>
<ul>
<li><p>Under this interpretation, the data fit loss (e.g., <span class="math inline">\(L_2\)</span> loss) is a likelihood function <span class="math inline">\(p(\{y^{(i)}\}^N_{i=1} \bigm | \{x^{(i)}\}^N_{i=1}, \theta)\)</span> and the regularizer is a prior <span class="math inline">\(p(\theta)\)</span>.</p></li>
<li><p>Bayes’ rule then states that the posterior <span class="math inline">\(p(\theta \bigm | \{x^{(i)}, y^{(i)}\}^N_{i=1})\)</span> is proportional to the product of the prior and the likelihood. The log posterior is then the <em>sum</em> of the log likelihood and the log prior, plus a constant.</p></li>
</ul>
</div>
</div>
</div>
</section>
<section id="regularization-5" class="slide level2">
<h2>Regularization</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Revisiting the <span class="math inline">\(\star\)</span> Problem</strong></p>
</div>
<div class="callout-content">
<p><span class="math display">\[\begin{aligned}
    3 \star 2 &amp;= 36\nonumber \\
    7 \star 1 &amp;= 49\nonumber \\
    5 \star 2 &amp;= 100\nonumber \\
    2 \star 2 &amp;= 16\nonumber
\end{aligned}\]</span></p>
<ul>
<li>maybe <span class="math inline">\(x \star y =  94.5x - 9.5x^2 + 4y^2 - 151\)</span>?</li>
<li>or maybe</li>
</ul>
<div class="sourceCode" id="cb1" data-xleftmargin="0.33" data-xrightmargin="0.33" data-fontsize="\fontsize{8.5}{9}" data-frame="single" data-framesep="2.5pt" data-baselinestretch="1.05"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a></a><span class="kw">def</span> star(x,y):</span>
<span id="cb1-2"><a></a>    <span class="cf">if</span> x<span class="op">==</span><span class="dv">2</span> <span class="op">&amp;&amp;</span> y<span class="op">==</span><span class="dv">3</span>:</span>
<span id="cb1-3"><a></a>        <span class="cf">return</span> <span class="dv">36</span></span>
<span id="cb1-4"><a></a>    <span class="cf">elif</span> x<span class="op">==</span><span class="dv">7</span> <span class="op">&amp;&amp;</span> y<span class="op">==</span><span class="dv">1</span>:</span>
<span id="cb1-5"><a></a>        <span class="cf">return</span> <span class="dv">49</span></span>
<span id="cb1-6"><a></a>    <span class="cf">elif</span> x<span class="op">==</span><span class="dv">5</span> <span class="op">&amp;&amp;</span> y<span class="op">==</span><span class="dv">2</span>:</span>
<span id="cb1-7"><a></a>        <span class="cf">return</span> <span class="dv">100</span></span>
<span id="cb1-8"><a></a>    <span class="cf">elif</span> x<span class="op">==</span><span class="dv">2</span> <span class="op">&amp;&amp;</span> y<span class="op">==</span><span class="dv">2</span>:</span>
<span id="cb1-9"><a></a>        <span class="cf">return</span> <span class="dv">16</span></span>
<span id="cb1-10"><a></a>    <span class="cf">else</span>:</span>
<span id="cb1-11"><a></a>        <span class="cf">return</span> <span class="dv">0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="data-priors-and-hypotheses" class="slide level2">
<h2>Data, Priors, and Hypotheses</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Three tools</strong></p>
</div>
<div class="callout-content">
<ol type="1">
<li><p><strong>data</strong>: observations of the world like photos and videos. Finding explanations consistent with the observed data is the centerpiece of learning-based vision.</p></li>
<li><p><strong>priors</strong> (a.k.a. <strong>regularizers</strong>): prefer some solutions over others a priori.</p></li>
<li><p>Set of <strong>hypotheses</strong> under consideration for what the true function may be. The hypothesis space constrains which solutions we can possibly find.</p></li>
</ol>
</div>
</div>
</div>
</section>
<section id="data-priors-and-hypotheses-1" class="slide level2">
<h2>Data, Priors, and Hypotheses</h2>
<!-- In this cartoon, we are learning a mapping from some domain -->
<!-- $\mathcal{X}$ to another domain $\mathcal{Y}$. The hypothesis space, -->
<!-- $\mathcal{F}$ (white circle; "the lamppost's light") places a hard -->
<!-- constraint on the subset of possible mappings under consideration, the -->
<!-- prior (yellow ellipse) places a soft constraint on which mappings are -->
<!-- preferred over which others, and the data (green ellipse) also places a -->
<!-- soft constraint on the space, preferring mappings that well fit the -->
<!-- data. -->

<!-- Approximation error is low within the green region. If we didn't care -->
<!-- about generalization, then it would be sufficient just to select any -->
<!-- solution in this green region. But since we do care about -->
<!-- generalization, we bias our picks toward the yellow region, which -->
<!-- corresponds to a prior that selects points we believe to be closer to -->
<!-- the true solution, even if they might not fit the data perfectly well. -->
<!-- These tools isolate the area outlined in bright yellow as the region -->
<!-- where we may find our needle of truth. A learning algorithm, which -->
<!-- searches over $\mathcal{F}$ in order to maximize the likelihood times -->
<!-- the prior, will find a solution somewhere in this outlined region. -->
<img data-src="./img/problem_of_generalization/search_space_tools.png" class="r-stretch quarto-figure-center" id="fig-problem_of_generalization-search_space_tools"><p class="caption">
Figure&nbsp;20: A cartoon of the tools for honing in on the truth.
</p></section>
<section id="data-priors-and-hypotheses-2" class="slide level2">
<h2>Data, Priors, and Hypotheses</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Experiment 1: Effect of Data</strong></p>
</div>
<div class="callout-content">
<p>Consider the following empirical risk minimization problem: <span id="eq-problem_of_generalization-error_fn_1"><span class="math display">\[
\begin{aligned}
    J(\theta; \{x^{(i)}, y^{(i)}\}^N_{i=1}) &amp;= \frac{1}{N}\sum_i \lvert f_{\theta}(x^{(i)}) - y^{(i)}\rvert^{0.25} \quad\quad \triangleleft \quad\text{objective}:error_fn_1\\
    f_{\theta}(x) &amp;= \theta_0 x + \theta_1 \sin(x)  \quad\quad \triangleleft \quad\text{hypothesis space}
\end{aligned}
\qquad(2)\]</span></span></p>
</div>
</div>
</div>
</section>
<section id="data-priors-and-hypotheses-3" class="slide level2">
<h2>Data, Priors, and Hypotheses</h2>

<img data-src="./img/problem_of_generalization/more_data_more_constraints.png" class="r-stretch quarto-figure-center" id="fig-problem_of_generalization-more_data_more_constraints"><p class="caption">
Figure&nbsp;21: <em>The more data you have, the less you need other modeling tools.</em> .
</p></section>
<section id="data-priors-and-hypotheses-4" class="slide level2">
<h2>Data, Priors, and Hypotheses</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Experiment 2: Effect of Priors</strong></p>
</div>
<div class="callout-content">
<p>We will use a slightly different hypothesis space and objective function <span class="math display">\[
\begin{aligned}
    J(\theta; \{x^{(i)}, y^{(i)}\}^N_{i=1}) &amp;= \frac{1}{N}\sum_i \left\lVert f_{\theta}(x^{(i)}) - y^{(i)}\right\rVert_2^2 + \lambda \left\lVert\theta\right\rVert_2^2 \quad\quad \triangleleft \quad\text{objective}\\
    f_{\theta}(x) &amp;= \theta_0 x + \theta_1 x \quad\quad \triangleleft \quad\text{hypothesis space}
\end{aligned}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="data-priors-and-hypotheses-5" class="slide level2">
<h2>Data, Priors, and Hypotheses</h2>

<img data-src="./img/problem_of_generalization/more_regularizer_more_constraints.png" class="r-stretch quarto-figure-center" id="fig-problem_of_generalization-more_regularizer_more_constraints"><p class="caption">
Figure&nbsp;22: More regularization, more (soft) constraints.
</p></section>
<section id="data-priors-and-hypotheses-6" class="slide level2">
<h2>Data, Priors, and Hypotheses</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Experiment 2: Effect of Priors</strong></p>
</div>
<div class="callout-content">
<p>You can take away a few lessons from this example:</p>
<ol type="1">
<li><p>Priors help only when they are good guesses as to the truth.</p></li>
<li><p>Overreliance on the prior means ignoring the data, and this is generally a bad thing.</p></li>
<li><p>For any given prior, there is a sweet spot where the strength is optimal. Sometimes this ideal strength can be derived from modeling assumptions and other times you may need to tune it as a hyperparameter.</p></li>
</ol>
</div>
</div>
</div>
</section>
<section id="data-priors-and-hypotheses-7" class="slide level2">
<h2>Data, Priors, and Hypotheses</h2>
<div class="callout callout-tip no-icon callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<p><strong>Experiment 3: Effect of the Hypothesis Space</strong></p>
</div>
<div class="callout-content">
<p>Consider the following three hypothesis spaces: <span class="math display">\[
\begin{aligned}
    f_{\theta}(x) &amp;= \theta_0 x + \theta_1 x^2 &amp;\triangleleft \quad\texttt{quadratic}\\
    f_{\theta}(x) &amp;= \theta_0 x &amp;\triangleleft \quad\texttt{linear}\\
    f_{\theta}(x) &amp;= 0 &amp;\triangleleft \quad\texttt{constant}
\end{aligned}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="data-priors-and-hypotheses-8" class="slide level2">
<h2>Data, Priors, and Hypotheses</h2>

<img data-src="./img/problem_of_generalization/fewer_hypotheses_more_constraints.png" class="r-stretch quarto-figure-center" id="fig-problem_of_generalization-fewer_hypotheses_more_constraints"><p class="caption">
Figure&nbsp;23: Fewer hypotheses, more (hard) constraints
</p></section>
<section id="summary-of-the-experiments" class="slide level2">
<h2>Summary of the Experiments</h2>
<div class="callout callout-important callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>General principle</strong></p>
</div>
<div class="callout-content">
<div class="center">
<p><em>What can be achieved with any one of our tools can also be achieved with any other.</em></p>
</div>
</div>
</div>
</div>



<aside><div>
<p>However, note that the hypothesis space places <em>hard</em> constraints on our search; we cannot violate them. Data and priors apply <em>soft</em> constraints; we can violate them but we will pay a penalty.</p>
</div></aside></section></section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/socket.io.js"></script>
  <script src="site_libs/revealjs/plugin/multiplex/multiplex.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'multiplex': {"url":"https://mplex.vitv.ly","secret":null,"id":"afdce9ed87450356e2b7f64b99f2a5bddb6163c45d788acddff890700ca45bd5"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp("https:\/\/dslnu\.github\.io\/compvision\/");
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>