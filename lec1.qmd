---
title: "Computer vision: intro"
author: 
  - name: Vitaly Vlasov
    affiliation: Lviv University
code-fold: false
execute:
  enabled: true
  cache: true
title-slide-attributes:
  data-background-image: img/ar_street.jpg
  data-background-size: contain
  data-background-opacity: "0.1"
format: 
  revealjs:
    slide-number: true
    theme: default
---

# History

## Euclid
Euclid (ca. 300 BCE): **natural perspective**: ray **OP** joining the center of projection **O** to the point **P**.

![](img/euclid.jpg){height=500}

## Aristotle
Thought that eyes are **emitting** vision (**emission theory**).
![](img/emission_theory){height=500}

## Medieval
á¸¤asan Ibn al-Haytham (*Alhazen*) - father of modern optics

:::: {.columns}
::: {.column width="50%"}
![](img/Hazan.png){height=500}
:::

::: {.column width="50%"}
![](img/alhazan_eye.jpg){height=500}
:::
::::

## Renaissance

### Leon Battista Alberti
![](img/alberti.jpg){height=360}

<!-- In 1435, Alberti codified the rules and inspired generations of artists. -->

> A truly universal genius (Jacob Burckhardt, The Civilization of the Renaissance in Italy)

## Descartes

### Camera eye
![](img/camera_eye.png){height=500}

## Earlier technologies
:::: {.columns}
::: {.column width="40%"}
- camera obscura
- the stereoscope
- film photography
:::

::: {.column width="60%"}
![](img/camera_obscura.jpg){.absolute left=500 top=60 height=320}
![](img/first_photo.jpg){.absolute left=560 top=400 height=320}
:::
::::

## Tank Detector
![](img/tank_detector.png){height=500}

::: aside
Recognition system design by statistical analysis (https://dl.acm.org/doi/pdf/10.1145/800257.808903)
:::

## Facial recognition

Woody Bledsoe, Charles Bisson and Helen Chan: facial recognition for military (1964)
![](img/rand_tablet.png){height=500}


## Summer Vision project
<!-- https://philippschmitt.com/archive/computer-vision-history/ -->
<!-- https://dspace.mit.edu/bitstream/handle/1721.1/6125/AIM-100.pdf?sequence=2 -->
![](img/summer_vision_project.png){height=500}

::: aside
Seymour Papert: https://dspace.mit.edu/bitstream/handle/1721.1/6125/AIM-100.pdf
:::

## Summer Vision project

What was the plan? **Divide and conquer**

#### Split teams doing different tasks:

::: {.incremental}
1. writing a program to detect edges, corners, and other pixel-level information
2. forming continous shapes out of these low-level features
3. arranging the shapes in three-dimensional space
4. etc. 
:::

## Perceptron
Frank Rosenblatt

:::: {.columns}
::: {.column width="50%"}
![](img/perceptron_facial_recognition.png)
:::

::: {.column width="50%"}
![](img/perceptron.jpg)
:::
::::

## Generalized cylinders
T.O. Binford (1970)

![](img/binford_cylinders.jpg)

::: aside
Applied in Rodney Brooks' ACRONYM (1981) - CIA aircraft detection
:::

## Deformable templates

Martin Fischler and Robert Elschlager (1972)
![](img/deformable_templates.jpg){height=500}

## Mobots

Rodney Brooks (1987): *perception as action*

:::: {.columns}

::: {.column width="50%" height=400}
![](img/brooks_robots.png)
:::

::: {.column width="50%" height=400}
![](img/roomba.jpg)
:::

::::


## Neocognitron
![](img/neocognitron.png){height=450}

::: aside
Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position (https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf)
:::

## LeCun CNN paper

:::: {.columns}

::: {.column width="50%"}
![](img/lecun_network1.png)
:::

::: {.column width="50%"}
![](img/cnn_demo.png)
:::
::::

::: aside
Handwritten Digit Recognition with a Back-Propagation Network (https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf)
:::

# Concepts

## What is vision?

Vision is a perceptual channel that accepts a stimulus and reports some representation of the
world.
![](img/vision.jpeg){height=500}

## Problem
Azriel Rosenfeld, Picture Processing by Computer (1969)

> If we want to give our computers eyes, we must first
> give them an education in the facts of life.

![](img/education.jpg)

## Sensing types

:::: {.columns}

::: {.column width="50%"}

### Passive
Not sending out light to see.
:::

::: {.column width="50%"}

### Active 
Sending out a signal and sensing a reflection

:::

::::
![](img/sensing_types.jpg){.absolute left=160 height=400}

## Active sensing examples
- bats (**ultrasound**),
- dolphins (**sound**)
- abyssal fishes (**light**)
- some robots (**light, sound, radar**)

## Features
A **feature** is a number obtained by applying simple computations to an image. Very useful information can be obtained directly from features. 

**Feature extraction**: simple, direct computations applied to sensor responses.

![](img/feature1.jpg)

![](img/feature2.png){.absolute top=350 left=600}

## Model-based approach

Two kinds of models:

- **object model**: precise and geometric
- **rendering model**: describes the physical, geometric, and statistical processes that produce the stimulus

## Core problems

The two core problems of computer vision are 

- *reconstruction* 
- *recognition*

## Reconstruction


An agent builds a model of the world from an image(s)
<!-- https://viso.ai/computer-vision/3d-computer-vision/ -->
![](img/3d_reconstruction.jpg){height=500}

## Recognition
Agent draws distinctions among the objects it encounters based on visual and other information
<!-- https://viso.ai/computer-vision/image-recognition/ -->
![](img/image_recognition.jpg){height=500}

## Goals
The **goal** of vision is to extract information needed for
tasks such as:

- manipulation
- navigation
- object recognition

## Computer Vision vs Graphics

:::: {.columns}
::: {.column width="50%" .fragment}

### Vision

Emphasis on **analyzing** images
![](img/tesla_autopilot.png)
:::

::: {.column width="50%" .fragment}

### Graphics
Emphasis on **creating** images
![](img/vertigo.png)
:::
::::

## Challenge

Geometry distortion

![](img/geometry_distortion.png)

## Challenge

Illumination effects

![](img/illumination_effects.png)

## Challenge

Appearance variation
![](img/image_classification.png)

## Aside on cameras

### Pinhole camera
![](img/pinhole_camera.png)

## Aside on cameras

### Lens camera
![](img/lens_camera.png)

## Aside on cameras

### Phone camera
![](img/phone_camera.png)

## Image properties
Four general properties of images and video

- edges
- texture
- optical flow 
- segmentation into regions

## Edges

![](img/edge_types.png)

1. depth discontinuities
2. surface orientation discontinuities
3. reflectance discontinuities
4. illumination discontinuities (shadows)

## Texture

![](img/textures.jpg)

## Optical flow
<!-- https://viso.ai/deep-learning/optical-flow/ -->
![](img/optical_flow.jpg)

## Segmentation

![](img/image_segmentation.png)

## Applications
::: {.absolute top=120}
- understanding human actions
- captioning
- geometry reconstruction
- image transformation
- movement control
:::

![](img/applications1.png){.absolute left=600 top=50 height=200}
![](img/applications3.png){.absolute left=500 top=200 height=300}
![](img/applications4.png){.absolute left=650 top=450 height=200}

## Augmented Reality
![](img/apple_vision_example.png)

## Augmented Reality
![](img/ar_dystopia.png)

## Augmented Reality

![](img/street_ar.jpg)

## Autonomous driving


<!-- https://opencv.org/blog/opencv-applications-in-2023/#Real-world-OpenCV-Applications -->

  - object detection and lane recognition
  - adaptive cruise control
  - real-time environmental perception and decision-making

![](img/autonomous_driving.png){height=360}

# Modern tools

## OpenCV

- **when?:** at Intel in 1999.

- **goal:** democratize computer vision

<!-- 2007: included in the Google Summer of Code program -->

![](img/opencv.png){.absolute left=300 height=400}

## Architecture

![](img/opencv_arch.jpg)

## OpenCV modules {.scrollable}
<!-- https://delftswa.gitbooks.io/desosa2016/content/opencv/Chapter.html#source-code-structure -->
![](img/opencv_modules.png){height=1000}

## Features

The library has more than **2500** optimized algorithms for:

- face recognition
- object identification
- human action classification
- object tracking
- 3D model extraction
- augmented reality

## Where is OpenCV 5?

> Will the future be open? Or will our algorithms be lost in time, like tears in rain?

![](img/tears_in_rain.jpg)

[https://opencv.org/blog/where-is-opencv-5](/https://opencv.org/blog/where-is-opencv-5/)


## YOLO
A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes
![](img/yolo_detection_system)

<!-- https://arxiv.org/pdf/1506.02640 -->
::: aside
You Only Look Once: Unified, Real-Time Object Detection (https://arxiv.org/pdf/1506.02640)
:::

## YOLO model

Detection as a regression problem

![](img/yolo_model.png)

## YOLO architecture
![](img/yolo_arch.png)

## DETRs
End-to-end Transformer-based detectors (DETRs)

![](img/detrs.png)

::: aside
End-to-End Object Detection with Transformers (https://arxiv.org/pdf/2005.12872)
:::

## RT-DETR
![](img/rt_detr.png)
<!-- https://arxiv.org/pdf/2304.08069 -->

## Segment Anything
Foundation model for image segmentation
<!-- https://arxiv.org/pdf/2304.02643 -->
![](img/segment_anything.png)

::: aside
Segment Anything (https://arxiv.org/pdf/2304.02643)
:::

## Segment Anything
![](img/sa_dataset.png)

## Natural Language Supervision
![](img/clip.png)

::: aside
Learning Transferable Visual Models From Natural Language Supervision (https://arxiv.org/pdf/2103.00020)
:::

# Examples

## Basic image operations {.scrollable}


### Reading

::: {.panel-tabset}

### Python code
```{python}
#| eval: false
#| echo: true
retval = cv2.imread( filename[, flags] )
```

### Example
```{python}
#| label: fig-reading
#| fig-cap: "Image reading example"
#| echo: true
#| code-line-numbers: 5,
import cv2
import numpy as np
import matplotlib.pyplot as plt

img=cv2.imread("img/yolo_detection_system.png", cv2.IMREAD_GRAYSCALE)

#Displaying image using plt.imshow() method
plt.imshow(img)
```
:::

### Writing

::: {.panel-tabset}

### Python code
```{python}
#| eval: false
#| echo: true
cv2.imwrite( filename, img[, params] )
```

### Example
```{python}
#| echo: true
cv2.imwrite("new_file.jpg", img)
```

:::

### Dimensions

:::{.r-stack .center}
```{python}
#| echo: true
print("Image size (H, W, C) is:", img.shape)
```
:::


## Image cropping

```{python}
#| echo: true
cropped = img[100:300, 500:800]
plt.imshow(cropped)
```

## Image resizing / rotation {.scrollable}


### Resizing

::: {.panel-tabset}


### Python code
```{python}
#| eval: false
#| echo: true
dst = resize( src, dsize[, dst[, fx[, fy[, interpolation]]]] )
```

### Example
```{python}
#| echo: true
resized = cv2.resize(img, None, fx=0.1, fy=0.1)
plt.imshow(resized)
```
:::

### Rotation

::: {.panel-tabset}

### Python code
```{python}
#| eval: false
#| echo: true
dst = cv.flip( src, flipCode )
```

### Example
```{python}
#| echo: true
rotated = cv2.flip(img, 0)
plt.imshow(rotated)
```

```{python}
#| echo: true
rotated = cv2.flip(img, 1)
plt.imshow(rotated)
```

```{python}
#| echo: true
rotated = cv2.flip(img, -1)
plt.imshow(rotated)
```
:::

## Image annotation {.scrollable}

::: {.panel-tabset}

### Python code
```{python}
#| eval: false
#| echo: true
img = cv2.line(img, pt1, pt2, color[, thickness[, lineType[, shift]]])
img = cv2.circle(img, center, radius, color[, thickness[, lineType[, shift]]])
img = cv2.rectangle(img, pt1, pt2, color[, thickness[, lineType[, shift]]])
```

### Example

```{python}
#| echo: true
annotated = cv2.line(img, (200, 100), (800, 100), (0, 255, 255), thickness=50, lineType=cv2.LINE_AA);

plt.imshow(annotated)
```

```{python}
#| echo: true
annotated2 = cv2.circle(annotated, (200,200), 100, (0, 0, 255), thickness=20, lineType=cv2.LINE_AA);
plt.imshow(annotated2)
```
:::

## Adding text {.scrollable}

::: {.panel-tabset}

### Python code
```{python}
#| eval: false
#| echo: true
img = cv2.putText(img, text, org, fontFace, fontScale, color[, thickness[, lineType[, bottomLeftOrigin]]])
```

### Example
```{python}
#| echo: true
#| code-line-numbers: 8,
imageText = img.copy()
text = "Random text"
fontScale = 5.3
fontFace = cv2.FONT_HERSHEY_PLAIN
fontColor = (0, 255, 0)
fontThickness = 5

cv2.putText(imageText, text, (100, 300), fontFace, fontScale, fontColor, fontThickness, cv2.LINE_AA);

# Display the image
plt.imshow(imageText)
```
:::

## Image Thresholding

::: {.panel-tabset}

### Python code
```{python}
#| eval: false
#| echo: true
retval, dst = cv2.threshold( src, thresh, maxval, type[, dst] )
```

### Example
```{python}
#| echo: true
#| code-line-numbers: 1,
retval, img_thresh = cv2.threshold(img, 100, 255, cv2.THRESH_BINARY)

# Show the images
plt.figure(figsize=[18, 5])

plt.subplot(121);plt.imshow(img, cmap="gray");  plt.title("Original")
plt.subplot(122);plt.imshow(img_thresh, cmap="gray");plt.title("Thresholded")

print(img_thresh.shape)
```
:::

## Haar cascade classifiers
A Haar feature is essentially calculations that are performed on adjacent rectangular regions at a specific location in a detection window.
![](img/cascade_classifier.png)

## Code example {.scrollable}

::: {.panel-tabset}

### Preparation
```{python}
#| echo: true
# Load the Haar Cascade Classifier
face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')

# Read the image
img = cv2.imread('diverse_faces.jpg')
plt.imshow(img)
```

### Execution {.scrollable}
```{python}
#| echo: true
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# Detect faces
faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)

# Draw rectangles around faces
for (x, y, w, h) in faces:
    cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)
plt.imshow(img)
```
:::

## Pose estimation {.scrollable}
<!-- https://arxiv.org/pdf/1812.08008 -->
![](img/pose_estimation)

## Pose estimation {.scrollable}
```{python}
#| echo: true
import os
from IPython.display import YouTubeVideo, display, Image

protoFile   = "pose_deploy_linevec_faster_4_stages.prototxt"
weightsFile = os.path.join("model", "pose_iter_160000.caffemodel")
net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)
im = cv2.imread("jump.png") #"Tiger_Woods_crop.png")
im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)

inWidth  = im.shape[1]
inHeight = im.shape[0]

nPoints = 15
POSE_PAIRS = [
    [0, 1],
    [1, 2],
    [2, 3],
    [3, 4],
    [1, 5],
    [5, 6],
    [6, 7],
    [1, 14],
    [14, 8],
    [8, 9],
    [9, 10],
    [14, 11],
    [11, 12],
    [12, 13],
]

netInputSize = (368, 368)
inpBlob = cv2.dnn.blobFromImage(im, 1.0 / 255, netInputSize, (0, 0, 0), swapRB=True, crop=False)
net.setInput(inpBlob)

# Forward Pass
output = net.forward()

# Display probability maps
plt.figure(figsize=(20, 5))
for i in range(nPoints):
    probMap = output[0, i, :, :]
    displayMap = cv2.resize(probMap, (inWidth, inHeight), cv2.INTER_LINEAR)

    plt.subplot(2, 8, i + 1)
    plt.axis("off")
    plt.imshow(displayMap, cmap="jet")
```

## Pose estimation {.scrollable}

```{python}
#| echo: true
# X and Y Scale
scaleX = inWidth  / output.shape[3]
scaleY = inHeight / output.shape[2]

# Empty list to store the detected keypoints
points = []

# Treshold
threshold = 0.1

for i in range(nPoints):
    # Obtain probability map
    probMap = output[0, i, :, :]

    # Find global maxima of the probMap.
    minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)

    # Scale the point to fit on the original image
    x = scaleX * point[0]
    y = scaleY * point[1]

    if prob > threshold:
        # Add the point to the list if the probability is greater than the threshold
        points.append((int(x), int(y)))
    else:
        points.append(None)

imPoints = im.copy()
imSkeleton = im.copy()

# Draw points
for i, p in enumerate(points):
    cv2.circle(imPoints, p, 8, (255, 255, 0), thickness=-1, lineType=cv2.FILLED)
    cv2.putText(imPoints, "{}".format(i), p, cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, lineType=cv2.LINE_AA)

# Draw skeleton
for pair in POSE_PAIRS:
    partA = pair[0]
    partB = pair[1]

    if points[partA] and points[partB]:
        cv2.line(imSkeleton, points[partA], points[partB], (255, 255, 0), 2)
        cv2.circle(imSkeleton, points[partA], 8, (255, 0, 0), thickness=-1, lineType=cv2.FILLED)

plt.figure() #figsize=(50, 50))

plt.subplot(121)
plt.axis("off")
plt.imshow(imPoints)

plt.subplot(122)
plt.axis("off")
plt.imshow(imSkeleton)
```

## EAST
<!-- https://github.com/ZER-0-NE/EAST-Detector-for-text-detection-using-OpenCV/blob/master/opencv_text_detection_image.py -->
<!-- https://pyimagesearch.com/2018/08/20/opencv-text-detection-east-text-detector/ -->
<!-- EAST paper: https://arxiv.org/pdf/1704.03155 -->

### EAST: An Efficient and Accurate Scene Text Detector

![](img/east.png)

## EAST

:::{.r-stack .center}
![](img/chocolate.png)
:::

## EAST

```{python}
#| echo: true
# load the input image and grab the image dimensions
image = cv2.imread("img/chocolate.png")
orig = image.copy()
(H, W) = image.shape[:2]

width = 320
height = 320
# set the new width and height and then determine the ratio in change
# for both the width and height
(newW, newH) = (width, height)
rW = W / float(newW)
rH = H / float(newH)

# resize the image and grab the new image dimensions
image = cv2.resize(image, (newW, newH))
(H, W) = image.shape[:2]
```

## EAST

```{python}
#| echo: true
# define the two output layer names for the EAST detector model that
# we are interested -- the first is the output probabilities and the
# second can be used to derive the bounding box coordinates of text
layerNames = [
    "feature_fusion/Conv_7/Sigmoid",
    "feature_fusion/concat_3"]

# load the pre-trained EAST text detector
print("[INFO] loading EAST text detector...")
net = cv2.dnn.readNet("frozen_east_text_detection.pb")

# construct a blob from the image and then perform a forward pass of
# the model to obtain the two output layer sets
blob = cv2.dnn.blobFromImage(image, 1.0, (W, H),
                             (123.68, 116.78, 103.94), swapRB=True, crop=False)

```


## EAST

```{python}
#| echo: true
import time
from imutils.object_detection import non_max_suppression


confidence = 0.5

start = time.time()
net.setInput(blob)
(scores, geometry) = net.forward(layerNames)
end = time.time()

# show timing information on text prediction
print("[INFO] text detection took {:.6f} seconds".format(end - start))

# grab the number of rows and columns from the scores volume, then
# initialize our set of bounding box rectangles and corresponding
# confidence scores
(numRows, numCols) = scores.shape[2:4]
rects = []
confidences = []
```

## EAST

```{python}
#| echo: true
# loop over the number of rows
for y in range(0, numRows):
    # extract the scores (probabilities), followed by the geometrical
    # data used to derive potential bounding box coordinates that
    # surround text
    scoresData = scores[0, 0, y]
    xData0 = geometry[0, 0, y]
    xData1 = geometry[0, 1, y]
    xData2 = geometry[0, 2, y]
    xData3 = geometry[0, 3, y]
    anglesData = geometry[0, 4, y]

    # loop over the number of columns
    for x in range(0, numCols):
        # if our score does not have sufficient probability, ignore it
        if scoresData[x] < confidence:
            continue

        # compute the offset factor as our resulting feature maps will
        # be 4x smaller than the input image
        (offsetX, offsetY) = (x * 4.0, y * 4.0)

        # extract the rotation angle for the prediction and then
        # compute the sin and cosine
        angle = anglesData[x]
        cos = np.cos(angle)
        sin = np.sin(angle)

        # use the geometry volume to derive the width and height of
        # the bounding box
        h = xData0[x] + xData2[x]
        w = xData1[x] + xData3[x]

        # compute both the starting and ending (x, y)-coordinates for
        # the text prediction bounding box
        endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))
        endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))
        startX = int(endX - w)
        startY = int(endY - h)

        # add the bounding box coordinates and probability score to
        # our respective lists
        rects.append((startX, startY, endX, endY))
        confidences.append(scoresData[x])
```

## EAST {.scrollable}

::: {.r-stack .center}
```{python}
#| echo: true
# apply non-maxima suppression to suppress weak, overlapping bounding
# boxes
boxes = non_max_suppression(np.array(rects), probs=confidences)

# loop over the bounding boxes
for (startX, startY, endX, endY) in boxes:
    # scale the bounding box coordinates based on the respective
    # ratios
    startX = int(startX * rW)
    startY = int(startY * rH)
    endX = int(endX * rW)
    endY = int(endY * rH)

    # draw the bounding box on the image
    cv2.rectangle(orig, (startX, startY), (endX, endY), (0, 255, 0), 2)

# show the output image
plt.imshow(orig)
```
:::


## Image segmentation {.scrollable}
<!-- https://www.geeksforgeeks.org/image-segmentation-with-watershed-algorithm-opencv-python/ -->

::: {.panel-tabset}

### Preparation

```{python}
#| echo: true
import cv2
import numpy as np
from IPython.display import Image, display
from matplotlib import pyplot as plt

# Plot the image
def imshow(img, ax=None):
	if ax is None:
		plt.imshow(img)
	else:
		ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
		ax.axis('off')

#Image loading
img = cv2.imread("img/coins.png")
# Show image
imshow(img)
```

### Grayscale

```{python}
#| echo: true
#image grayscale conversion
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
imshow(gray)
```

### Otsu

```{python}
#| echo: true
#Threshold Processing
ret, bin_img = cv2.threshold(gray,
							0, 255, 
							cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
imshow(bin_img)
```

### Noise removal

```{python}
#| echo: true
kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
bin_img = cv2.morphologyEx(bin_img, 
						cv2.MORPH_OPEN,
						kernel,
						iterations=2)
imshow(bin_img)
```
:::

## Image segmentation {.scrollable}

::: {.panel-tabset}

### Bg/fg/unknown

```{python}
#| echo: true
# Create subplots with 1 row and 2 columns
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(8, 8))
# sure background area
sure_bg = cv2.dilate(bin_img, kernel, iterations=3)
imshow(sure_bg, axes[0,0])
axes[0, 0].set_title('Sure Background')

# Distance transform
dist = cv2.distanceTransform(bin_img, cv2.DIST_L2, 5)
imshow(dist, axes[0,1])
axes[0, 1].set_title('Distance Transform')

#foreground area
ret, sure_fg = cv2.threshold(dist, 0.5 * dist.max(), 255, cv2.THRESH_BINARY)
sure_fg = sure_fg.astype(np.uint8) 
imshow(sure_fg, axes[1,0])
axes[1, 0].set_title('Sure Foreground')

# unknown area
unknown = cv2.subtract(sure_bg, sure_fg)
imshow(unknown, axes[1,1])
axes[1, 1].set_title('Unknown')

plt.show()
```

### Markers

```{python}
#| echo: true
# Marker labelling
# sure foreground 
ret, markers = cv2.connectedComponents(sure_fg)

# Add one to all labels so that background is not 0, but 1
markers += 1
# mark the region of unknown with zero
markers[unknown == 255] = 0

fig, ax = plt.subplots(figsize=(6, 6))
ax.imshow(markers, cmap="tab20b")
ax.axis('off')
plt.show()
```

### Final

```{python}
#| echo: true
# watershed Algorithm
markers = cv2.watershed(img, markers)

fig, ax = plt.subplots(figsize=(5, 5))
ax.imshow(markers, cmap="tab20b")
ax.axis('off')
plt.show()


labels = np.unique(markers)

coins = []
for label in labels[2:]: 

# Create a binary image in which only the area of the label is in the foreground 
#and the rest of the image is in the background 
	target = np.where(markers == label, 255, 0).astype(np.uint8)

# Perform contour extraction on the created binary image
	contours, hierarchy = cv2.findContours(
		target, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
	)
	coins.append(contours[0])

# Draw the outline
img = cv2.drawContours(img, coins, -1, color=(0, 23, 223), thickness=2)
imshow(img)
```
:::

## Overlays {.scrollable}

<!-- https://github.com/austinjoyal/haar-cascade-files/blob/master/haarcascade_eye.xml -->
<!-- https://medium.com/@karanguptagireesh163/to-add-accessories-such-as-sunglasses-or-other-fun-elements-to-the-detected-faces-using-haar-12fdde5b429f -->

::: {.panel-tabset}

### Python code

```{python}
#| echo: true
#| eval: false
import cv2
import numpy as np

# Load the Haar Cascade XML file for face detection
face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')

# Load the accessory image
accessory_image = cv2.imread('img/ar_overlay.png', cv2.IMREAD_UNCHANGED)

# Initialize the video capture
video_capture = cv2.VideoCapture(0)

while True:
    # Read the video frame
    ret, frame = video_capture.read()

    # Convert the frame to grayscale
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Perform face detection
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

    # Iterate over detected faces
    for (x, y, w, h) in faces:
        # Resize the accessory image to fit the face
        resized_accessory = cv2.resize(accessory_image, (w, h))
        
        # Calculate the region of interest (ROI) for the accessory
        roi = frame[y:y+h, x:x+w]

        # Create a mask for the accessory
        accessory_mask = resized_accessory[:, :, 3] / 255.0
        bg_mask = 1.0 - accessory_mask

        # Blend the accessory and the frame
        accessory_pixels = resized_accessory[:, :, 0:3]
        bg_pixels = roi[:, :, 0:3]

        blended_pixels = (accessory_pixels * accessory_mask[:, :, np.newaxis]) + (bg_pixels * bg_mask[:, :, np.newaxis])

        # Replace the ROI with the blended image
        frame[y:y+h, x:x+w] = blended_pixels

    # Display the resulting frame
    cv2.imshow('Face Detection with Accessories', frame)

    # Break the loop if 'q' is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the video capture
video_capture.release()

# Close all OpenCV windows
cv2.destroyAllWindows()
```

### Output
![](img/ar_overlay_shot.png)

### Output 2
![](img/ar_overlay_shot2.png)

:::

## YOLO object recognition
<!-- https://www.aranacorp.com/en/object-recognition-with-yolo-and-opencv/ -->

::: {.panel-tabset}

### Initialization

```{python}
#| echo: true
#| eval: true
import matplotlib.pyplot as plt

import datetime
from ultralytics import YOLO
import cv2
from imutils.video import VideoStream

# define some constants
CONFIDENCE_THRESHOLD = 0.8
GREEN = (0, 255, 0)

# load the pre-trained YOLOv8n model
model = YOLO("yolov8n.pt")
```

### Detection

```{python}
#| echo: true
#| eval: true
frame = cv2.imread('./img/yolo_test.png')
detections = model(frame)[0]
for box in detections.boxes:
    #extract the label name
    label=model.names.get(box.cls.item())
        
    # extract the confidence (i.e., probability) associated with the detection
    data=box.data.tolist()[0]
    confidence = data[4]

    # filter out weak detections by ensuring the
    # confidence is greater than the minimum confidence
    if float(confidence) < CONFIDENCE_THRESHOLD:
        continue

    # if the confidence is greater than the minimum confidence,
    # draw the bounding box on the frame
    xmin, ymin, xmax, ymax = int(data[0]), int(data[1]), int(data[2]), int(data[3])
    cv2.rectangle(frame, (xmin, ymin) , (xmax, ymax), GREEN, 2)

    #draw confidence and label
    y = ymin - 15 if ymin - 15 > 15 else ymin + 15
    cv2.putText(frame, "{} {:.1f}%".format(label,float(confidence*100)), (xmin, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, GREEN, 2)
```

### Result

```{python}
#| echo: true
#| eval: true
plt.imshow(frame)
```
:::


## Thank you

![](img/entertained.jpg)
