---
title: "Basic concepts"
author: 
  - name: Vitaly Vlasov
    affiliation: Lviv University
code-fold: false
execute:
  enabled: false
  cache: true
diagram:
  cache: true
  cache-dir: ./cache
  engine:
    tikz:
      execpath: lualatex
      additional-packages: |
        \usepackage{neuralnetwork}
        \usepackage{mathtools}
        \usepackage{amsmath}
        \pgfplotsset{compat=1.16}
        \usepackage{pgfplots}
        \newcommand\mybox[2][]{\tikz[overlay]\node[fill=blue!20,inner sep=2pt, anchor=text, rectangle, rounded corners=1mm,#1] {#2};\phantom{#2}}
        \usetikzlibrary{arrows.meta}
        \usetikzlibrary{positioning}
        \usetikzlibrary{shapes.misc}
        \usetikzlibrary{decorations.pathreplacing}
filters:
  - diagram
format: 
  revealjs:
    preview-links: auto
    slide-number: true
    theme: default
    multiplex:
      url: 'https://mplex.vitv.ly'
      secret: ''
      id: ''
---

# Introduction

## Goal

- simulate original research from 1960s
- hand-design an end-to-end visual system. 
- cover some of the main concepts

## History
:::{.callout-important icon=false}
## Seymour Papert (1966)
> "The summer vision project is an attempt to use
our summer workers effectively in the construction of a significant part
of a visual system." 
:::

:::{.callout-warning icon=false}
Too optimistic!
:::

## Problem setup

Vision has many different goals:

- object recognition
- scene interpretation
- **three-dimensional \[3D\] interpretation**

# A Simple World: The Blocks World

## Simple world
:::{.callout-tip icon=false}
## Block world
Introduced by Larry G. Roberts in 1963.

![](img/simplesystem/roberts_phd_figure.png){height=400}
:::

## Simple world

:::{.callout-note icon=false}
## Description

- world is composed of a very simple (yet varied) set of objects.
- these simple objects are composed of flat surfaces that can be horizontal or vertical
- these objects will be resting on a white horizontal ground plane
- we can build these objects by cutting, folding, and gluing together some pieces of colored paper 

![A world of simple objects. Print, cut, and build your own blocks
world!](img/simplesystem/simpleObjects.png){#fig-simpleObjects
width="80%"}
:::

# A Simple Image Formation Model

## A Simple Image Formation Model
:::{.callout-important icon=false}
## Parallel of orthographic projection

- light rays travel parallel to each other and perpendicular to the camera
plane
- this type of projection produces images in which objects do not
change size as they move closer or farther from the camera
- parallel lines in 3D remain parallel in the 2D image. 
:::


## A Simple Image Formation Model
:::{.callout-tip icon=false}
## Perspective projection

- the image is formed by the convergence of the light rays into a single point (**focal point**).
- most pictures taken with a camera will be better described by **perspective projection**
:::

## A Simple Image Formation Model

![](img/simplesystem/parallelProjection.png){#fig-parallelProjection
width="100%"}

::: aside
(a) Note that near edges are larger than far edges, and parallel lines in 3D are not parallel.

(b) Picture taken from far away using zoom resulting in an image that can be
described by parallel projection.
:::

## A Simple Image Formation Model {.smaller}

- how a point in world coordinates $(X,Y,Z)$ projects into the image plane
- camera center is inside the 3D plane $X=0$
- the horizontal axis of the camera ($x$) is parallel to the ground plane ($Y=0$)
- the camera is tilted so that the line connecting the origin of the world coordinates
system and the image center is perpendicular to the image plane
- the angle $\theta$ is the angle between this line and the $Z$-axis
- the image is parameterized by coordinates $(x,y)$.

## A Simple Image Formation Model
![](img/simplesystem/projection2.png){#fig-projection width="100%"}

::: aside
The $Z$-axis is identical to the $Y$-axis up to a sign change and a scaling.
:::

## A Simple Image Formation Model
:::{.callout-important icon=false}
## Features
- the origin of the world coordinates projects on the origin of the image coordinates
- therefore, the world point $(0,0,0)$ projects into $(0,0)$
- the resolution of the image  will also affect the transformation from world
coordinates to image coordinates via a constant factor $\alpha$
- for now we assume that pixels are square and we will see a more general form in
) and that this constant is $\alpha=1$. 
:::

## A Simple Image Formation Model
:::{.callout-tip icon=false}
## Coordinate transformation
The transformation between world coordinates and image
coordinates can be written as follows: 

$$\begin{aligned}
x &=& X \\
y &=&  \cos(\theta) Y - \sin(\theta) Z 
\end{aligned}$${#eq-projection}
:::

::: aside
With this particular parametrization of the world and camera coordinate
systems, the world coordinates $Y$ and $Z$ are mixed after projection.
From the camera, a point moving parallel to the $Z$-axis will be
indistinguishable from a point moving parallel to the $Y$-axis.
:::

## A Simple Goal

:::{.callout-tip icon=false}
## Our goal

- recovering the **world coordinates** of all the pixels seen by the camera.
:::


:::{.callout-warning icon=false}
## Non-goal

- recover the **actual color** of the surface seen by each pixel $(x,y)$

(requires discounting for illumination effects as the color of the pixel is a
combination of the surface albedo and illumination (color of the light
sources and interreflections).
:::

# From Images to Edges and Useful Features {#sec-algo_simple_world}

## From Images to Edges

The observed image is a function:
$$\ell(x,y)$$

:::{.callout-note icon=false}
## Description

- **input**: location, $(x,y)$
- **output**: the intensity at that location.
:::
:::{.callout-important icon=false}
## What is an image?
In this **representation**, the image is an array of intensity values
(color values) indexed by location.
:::

## From Images to Edges
![Image as a surface. The vertical axis corresponds to image intensity.
For clarity here, we have reversed the vertical axis. Dark values are
shown higher than lighter
values.](img/simplesystem/imageAsSurface.jpg){#fig-imageAsSurface
width="100%"}

## From Images to Edges

:::{.callout-tip icon=false}
## Benefits

- this representation is ideal for determining the light intensity
originating from different directions in space and striking the camera
plane, as it provides explicit representation of this information
- the array of pixel intensities, $\ell(x,y)$, is a reasonable representation
as input to the early stages of visual processing because, although we
do not know the distance of surfaces in the world, the direction of each
light ray in the world is well defined. 
:::

:::{.callout-important icon=false}
## Alternative options
Other initial representations:

- images could be coded in the Fourier domain
- pixels could combine light coming in different directions.
:::

## From Images to Edges
:::{.callout-note}
We are interested in interpreting the 3D structure of the scene and the objects within.
:::

:::{.callout-tip icon=false}
## Challenges
Need to represent:

- boundaries between objects
- changes in the surface orientation
:::

## From Images to Edges
:::{.callout-important icon=false}
## Alternative representation
For scene interpretation:

- collections of small image patches
- regions of uniform properties
- **edges**
:::

## From Images to Edges

:::{.callout-important icon=false}
## Alternative initial representations 
Gekko's eye. Their pupil has a four-diamond-shaped pinhole aperture that could allow them to encode distance to a target in the retinal image.
![](img/simplesystem/Tokay-Gecko-Eye-1440x954.jpg){width="60%"}
:::

## From Images to Edges

:::{.callout-note}
## What is an edge
An image regions where there are strong changes of the image with respect to location.
:::


:::{.callout-important icon=false}
## Edge factors

- occlusion boundaries
- changes in surface orientation
- changes in surface albedo
- shadows
:::

## From Images to Edges
![](img/simplesystem/edgeLabeling.png){#fig-edgeLabeling width="90%"}

## From Images to Edges and Useful Features
:::{.callout-note icon=false}
## Image edge classification

-   **Object boundaries**: These indicate pixels that delineate the
    boundaries of any object. Boundaries between objects generally
    correspond to changes in surface color, texture, and orientation.

-   Changes in **surface orientation**: These indicate locations where
    there are strong image variations due to changes in the surface
    orientations. A change in surface orientation produces changes in
    the image intensity because intensity is a function of the angle
    between the surface and the incident light.

-   **Shadow edges**: This can be harder than it seems. In this simple
    world, shadows are soft, creating slow transitions between dark and
    light.
:::

## From Images to Edges and Useful Features
:::{.callout-tip icon=false}
## Object boundary classification

-   **Contact edges**: This is a boundary between two objects that are
    in physical contact. Therefore, there is no depth discontinuity.

-   **Occlusion boundaries**: Occlusion boundaries happen when an object
    is partially in front of another. Occlusion boundaries generally
    produce depth discontinuities. In this simple world, we will
    position the objects in such a way that objects do not occlude each
    other but they will occlude the background.
:::

## From Images to Edges

:::{.callout-important}
## Challenges

- in most natural scenes, this classification is very hard
- requires the interpretation of the scene at different levels.
:::

## From Images to Edges {.smaller}

First step: detecting candidate edges in the image.

If we think of the image as a function $\ell (x,y) \in C(\mathbb{R}^2)$, we can measure the degree
of variation using the gradient: 

$$\begin{aligned}
\nabla \ell = \left( \frac{\partial \ell}{\partial x}, \frac{\partial \ell}{\partial y} \right)
\end{aligned}$$ 

![](img/simplesystem/gradient2.png){width="25%"}

::: aside
The direction of the gradient indicates the direction in
which the variation of intensities is larger. If we are on top of an
edge, the direction of larger variation will be in the direction
perpendicular to the edge.
:::


## From Images to Edges
:::{.callout-important icon=false}
The image is not a continuous function as we only know the
values of the $\ell (x,y)$ at discrete locations (pixels).
:::

:::{.callout-tip icon=false}
## Approximation
We approximate the partial derivatives by: 

$$\begin{aligned}
\frac{\partial \ell}{\partial x} &\simeq \ell(x,y) - \ell(x-1,y) \\
\frac{\partial \ell}{\partial y} &\simeq \ell(x,y) - \ell(x,y-1) 
\end{aligned}$${#eq-image_partial_derivatives_aprox}
:::

## From Images to Edges
:::{.callout-important icon=false}
## Better approximation (TBD)
Combine the image pixels around $(x,y)$ with the weights:
$$\begin{aligned}
\frac{1}{4} \times
\left [
\begin{matrix}
-1 & 0 & 1 \\
-2 & 0 & 2 \\
-1 & 0 & 1 
\end{matrix}
\right ] \nonumber
\end{aligned}$$
:::

## From Images to Edges

:::{.callout-important icon=false}
## Edge properties

From the image gradient, we can extract a number of interesting
quantities: 
$$\begin{aligned}
    e(x,y) &= \lVert \nabla \ell(x,y) \rVert   & \quad\quad \triangleleft \quad \texttt{edge strength}\\
    \theta(x,y) &= \angle \nabla \ell =  \arctan \left( \frac{\partial \ell / \partial y}{\partial \ell / \partial x} \right) & \quad\quad \triangleleft \quad \texttt{edge orientation}
\end{aligned}$$ 
:::

:::{.callout-note}

- **edge strength** is the gradient magnitude
- **edge orientation** is perpendicular to the gradient direction
:::

## From Images to Edges
The unit norm vector perpendicular to an edge is: 

$$\begin{aligned}
{\bf n} = \frac{\nabla \ell}{\lVert \nabla \ell \rVert}
\end{aligned}$$


## From Images to Edges
:::{.callout-important icon=false}
## Decision
Which pixels belong to:

- edges (regions of the image with sharp intensity
variations)
- uniform regions (flat surfaces)
:::

:::{.callout-tip icon=false}
## How?
By thresholding the edge strength $e(x,y)$.

In the pixels with edges, we can also measure the edge orientation
$\theta(x,y)$.
:::

## From Images to Edges

![Gradient and edge types.](img/simplesystem/gradient.png){#fig-gradient width=70%}

![](img/simplesystem/edgeTypes.png){#fig-edge width=70%}

::: aside
@fig-gradient and @fig-edge visualize the edges and the normal vector on each edge.
:::

# From Edges to Surfaces

## From Edges to Surfaces 

:::{.callout-important icon=false}
## Objective
We want to recover world coordinates $X(x,y)$, $Y(x,y)$, and $Z(x,y)$
for each image location $(x,y)$
:::

:::{.callout-tip icon=false}
## Steps

- recovering the $X$ world coordinates is trivial as
they are directly observed: for each pixel with image coordinates $(x,y)$ the world coordinate is $X(x,y) = x$
- recovering $Y$ and $Z$ will be harder as we only observe a mixture of the two world
coordinates.
:::

::: aside
Here we have written the world coordinates
as functions of image location $(x,y)$ to make explicit that we want to
recover the 3D locations of the visible points.

In this simple world, we will formulate this problem as a set of linear
equations.
:::



## From Edges to Surfaces


:::{.callout-important icon=false}
## Figure/Ground Segmentation

Segmentation of an image into figure and ground is a classical problem
in human perception and computer vision that was introduced by **Gestalt
psychology**.
:::

![](img/simplesystem/figure-ground.png){width="30%"}

::: aside
The classical visual illusion ``two faces or a vase'' is an example of **figure-ground** segmentation problem.
:::

## From Edges to Surfaces

:::{.callout-note}
## Segmentation: goal
Decide whether a pixel belongs to one of the
**foreground** objects or to the **background**.
:::

:::{.callout-tip}
## Segmentation: basic approach
We can simply look at the **color values** of each pixel

- bright pixels that have low saturation (similar values of the red-blue-green \[RBG\] components)
correspond to the white ground plane
- and the rest of the pixels are likely to belong to the colored blocks that compose our simple world
:::

::: aside
In general, the problem of **image segmentation** into distinct objects is
a very challenging task.
:::

## From Edges to Surfaces
:::{.callout-important icon=false}
## Background
If we assume that
the background corresponds to a horizontal ground plane, then for all
pixels that belong to the ground we can set $Y(x,y)=0$.
:::


:::{.callout-tip icon=false}
## Objects
For pixels that
belong to objects we will have to measure additional image properties
before we can deduce any geometric scene constraints.
:::

## From Edges to Surfaces
:::{.callout-tip icon=false}
## Occlusion Edges
An **occlusion boundary** separates two different surfaces at different
distances from the observer.

The object in front is the one owning the boundary.
:::

![](img/simplesystem/border_ownership.png){width="30%"}

::: aside
Knowing who owns the boundary is important as an
edge provides cues about the 3D geometry, but those cues only apply to
the surface that owns the boundary.
:::

## From Edges to Surfaces

:::{.callout-important icon=false}
## Assumption
In this simple world, we will assume that objects do not occlude each
other (this can be relaxed) and that the only occlusion boundaries are
the boundaries between the objects and the ground.
:::

:::{.callout-note}
## Note
Not all boundaries between the objects and the
ground correspond to depth gradients.
:::

## From Edges to Surfaces
:::{.callout-tip icon=false}
## Contact Edges
Contact edges are boundaries between two distinct objects but where
there exists no depth discontinuity.

Despite that there is not a depth
discontinuity, there is an occlusion here (as one surface is hidden
behind another), and the edge shape is only owned by one of the two
surfaces.
:::


:::{.callout-important icon=false}
## Calculation
If we assume that all the objects rest on the
ground plane, then we can set $Y(x,y)=0$ on the contact edges.

Contact
edges can be detected as transitions between the object (above) and
ground (below). In our simple world only horizontal edges can be contact
edges.
:::

## From Edges to Surfaces
![For each vertical line (shown in red), scanning from top to bottom,
transitions from ground to figure are occlusion boundaries, and
transitions from figure to ground are contact edges. This heuristic will
fails when an object occludes
another.](img/simplesystem/ground_figure_points.png){#fig-ground_figure_points
width="50%"}


## From Edges to Surfaces
:::{.callout-note}
## Invariant Scene Properties (world $\rightarrow$ image)

-   **Collinearity**: a straight 3D line will project into a straight line
    in the image.

-   **Cotermination**: if two or more 3D lines terminate at the same point,
    the corresponding projections will also terminate at a common point.

-   **Smoothness**: a smooth 3D curve will project into a smooth 2D curve.
:::

![](img/simplesystem/invariants.png)


## From Edges to Surfaces
:::{.callout-important icon=false}
## Reverse invariants (image $\rightarrow$ world)

- a straight line in the image could correspond to a curved line
in the 3D world but that happens to be precisely aligned with respect to
the viewers point of view to appear as a straight line
- two lines that intersect in the image plane could be disjointed in the 3D space.
:::

## From Edges to Surfaces

:::{.callout-important icon=false}
## Reverse invariants (image $\rightarrow$ world)

- if two lines coterminate in
the image, then, one can conclude that it is very likely that they also
touch each other in 3D
- if the 3D lines do not touch each other, then it
will require a very specific alignment between the observer and the
lines for them to appear to coterminate in the image. Therefore, one can
safely conclude that the lines might also touch in 3D.
:::

## From Edges to Surfaces

:::{.callout-tip icon=false}
## Nonaccidental properties
These properties are called **nonaccidental properties**
because they will only be observed in the image if they also exist in
the world or by accidental alignments between the observer and scene
structures.
:::

:::{.callout-important icon=false}
## Generic view
Under a **generic view**, nonaccidental properties will be
shared by the image and the 3D world.
:::

## From Edges to Surfaces
:::{.callout-note icon=false}
## Application to simple world

- in the simple world all 3D edges are either vertical or horizontal
- under parallel
projection and with the camera having its horizontal axis parallel to
the ground, we know that vertical 3D lines will project into vertical 2D
lines in the image
- horizontal lines will, in general, project into oblique lines
- therefore, we can assume than any vertical line in the image is also a vertical line in the world.
:::

## From Edges to Surfaces
:::{.callout-important}
## Challenge
The assumption that vertical 2D lines are also 3D vertical
lines will not always work!
:::

![](img/simplesystem/accidentalAlignments.png){#fig-accidentalAlignments
width="100%"}

::: aside
In the case of the cube, there is a particular viewpoint that will make an
horizontal line project into a vertical line, but this will require an
accidental alignment between the cube and the line of sight of the
observer. 
:::

## From Edges to Surfaces
:::{.callout-note icon=false}
## Formulation
We can now translate the inferred 3D edge orientation into linear
constraints on the global 3D structure. We will formulate these
constraints in terms of $Y(x,y)$. Once $Y(x,y)$ is recovered we can also
recover $Z(x,y)$ from @eq-projection.

In a 3D vertical edge, using the projection equations, the derivative of
$Y$ along the edge will be 

$$\begin{aligned}
\partial Y / \partial y &= 1/ \cos(\theta)\end{aligned}
$${#eq-derivative_Y_along_edge}
:::

## From Edges to Surfaces
:::{.callout-note icon=false}
## Formulation
In a 3D horizontal edge, the coordinate $Y$ will not change. Therefore,
the derivative along the edge should be zero: 

$$\begin{aligned}
\partial Y / \partial {\bf t} &= 0
\end{aligned}
$${#eq-derivative_Y_along_edge_hor} 

where the vector $\bf t$ denotes direction tangent to
the edge, ${\bf t}=(-n_y, n_x)$. 
:::

![](img/simplesystem/tangent.png)


## From Edges to Surfaces
:::{.callout-note icon=false}
## Formulation
We can write this derivative as a
function of derivatives along the $x$ and $y$ image coordinates:
$$\begin{aligned}
\partial Y / \partial {\bf t} =  \nabla Y \cdot {\bf t} = -n_y \partial Y / \partial x + n_x \partial Y / \partial y
\end{aligned}$${#eq-derivative_Y_along_edge_tan}

When the edges coincide with occlusion edges, special care should be
taken so that these constraints are only applied to the surface that
owns the boundary.
:::

## From Edges to Surfaces
:::{.callout-tip}
## Approximation
In this chapter, we represent the world coordinates $X(x,y)$, $Y(x,y)$,
and $Z(x,y)$ as images where the coordinates $x,y$ correspond to pixel
locations.

Therefore, it is useful to **approximate** the partial
derivatives in the same way that we approximated the image partial
derivatives in equations (@eq-image_partial_derivatives_aprox). Using
this approximation, @eq-derivative_Y_along_edge can be written as
follows: $$\begin{aligned}
Y(x,y)-Y(x,y-1) &= 1/ \cos(\theta)
\end{aligned}$$

Similar relationships can be obtained from equations
@eq-derivative_Y_along_edge_hor and
@eq-derivative_Y_along_edge_tan.
:::

## From Edges to Surfaces
### Constraint Propagation {#sec-constraint}

Most of the image consists of flat regions where we do not have such
edge constraints and we thus don't have enough local information to
infer the surface orientation. Therefore, we need some criteria in order
to propagate information from the boundaries, where we do have
information about the 3D structure, into the interior of flat image
regions. This problem is common in many visual domains.



## From Edges to Surfaces
:::{.column-margin}
An image patch without context is not enough to infer its 3D shape.

![](img/simplesystem/cube_propagartion_1.png){width="44%"} 

The same patch shown in the original image (right):

![](img/simplesystem/cube_propagartion_2.png){width="44%"}

Information about its 3D orientation its propagated from the surrounding edges.
:::

## From Edges to Surfaces

In this case we will assume that the object faces are planar. Thus, flat
image regions impose the following constraints on the local 3D
structure: $$\begin{aligned}
\partial^2 Y / \partial x^2 &= 0  \\
\partial^2 Y / \partial y^2 &= 0 \\  
\partial^2 Y / \partial y \partial x &= 0 
\end{aligned}$$ That is, the second order derivative of $Y$ should be
zero. As before, we want to approximate the continuous partial
derivatives. The approximation to the second derivative can be obtained
by applying twice the first order derivative approximated by equations
(@eq-image_partial_derivatives_aprox). The result is
$\partial^2 Y / \partial x^2 \simeq 2Y(x,y)-Y(x+1,y)-Y(x-1,y)$, and
similarly for $\partial^2 X / \partial x^2$.


## From Edges to Surfaces
### A Simple Inference Scheme

All the different constraints described previously can be written as an
overdetermined system of linear equations. Each equation will have the
form: $$\begin{aligned}
\mathbf{a}_i \mathbf{Y} = b_i
\end{aligned}$$ where $\mathbf{Y}$ is a vectorized version of the image
$Y$ (i.e., all rows of pixels have been concatenated into a flat
vector). Note that there might be many more equations than there are
image pixels.

## From Edges to Surfaces
We can translate all the constraints described in the previous sections
into this form. For instance, if the index $i$ corresponds to one of the
pixels inside one of the planar faces of a foreground object, then there
will be three equations. One of the planarity constraint can be written
as $\mathbf{a}_i = [0, \dots, 0, -1, 2, -1, 0, \dots, 0]$, $b_i=0$, and
analogous equations can be written for the other two.

## From Edges to Surfaces
We can solve the system of equations by minimizing the following cost
function: 
$$\begin{aligned}
J = \sum_i (\mathbf{a}_i\mathbf{Y} - b_i)^2
\end{aligned}$$ where the sum is over all the constraints.

If some constraints are more important than others, it is possible to
also add a weight $w_i$. 
$$\begin{aligned}
J = \sum_i w_i (\mathbf{a}_i \mathbf{Y} - b_i)^2
\end{aligned}$$

## From Edges to Surfaces
Our formulation has resulted on a big system of linear constraints
(there are more equations than there are pixels in the image). It is
convenient to write the system of equations in matrix form:
$$\begin{aligned}
\mathbf{A} \mathbf{Y}  = \mathbf{b}
\end{aligned}$$ 

where row $i$ of the matrix ${\bf A}$ contains the
constraint coefficients $\mathbf{a}_i$. The system of equations is
overdetermined ($\mathbf{A}$ has more rows than columns). We can use the
pseudoinverse to compute the solution: 

$$\begin{aligned}
\bf Y = (\mathbf{A}^T \mathbf{A})^{-1} \mathbf{A}^T \mathbf{b}
\end{aligned}$$ This problem can be solved efficiently as the matrix
$\mathbf{A}$ is very sparse (most of the elements are zero).

## From Edges to Surfaces

### Results

@fig-worldCoordinates shows the resulting world coordinates $X(x,y)$,
$Y(x,y)$, $Z(x,y)$ for each pixel. The world coordinates are shown here
as images with the gray level coding the value of each coordinate (black
corresponds to the value 0).

![The solution to our vision problem for the input image of
@fig-edgeLabeling. World coordinates $X$, $Y$, and $Z$ are shown as
images.](img/simplesystem/worldCoordinates.png){#fig-worldCoordinates
width="100%"}

## From Edges to Surfaces
There are a few things to reflect on:

-   It works. At least it seems to work pretty well. Knowing how well it
    works will require having some way of evaluating performance. This
    will be important.

-   But it cannot possibly work all the time. We have made lots of
    assumptions that will work only in this simple world. The rest of
    the book will involve upgrading this approach to apply to more
    general input images.

Despite that this approach will not work on general images, many of the
general ideas will carry over to more sophisticated solutions (e.g.,
gather and propagate local evidence). One thing to think about is if
information about 3D orientation is given in the edges, how does that
information propagate to the flat areas of the image in order to produce
the correct solution in those regions?

## From Edges to Surfaces
Evaluation of performance is a very important topic. Here, one simple
way to visually verify that the solution is correct is to render the
objects under new view points. @fig-views shows the reconstructed 3D
scene rendered under different view points to show that the 3D structure
has been accurately captured.

## From Edges to Surfaces
![To show that the algorithm for 3D interpretation gives reasonable
results we can re-render the inferred 3D structure from different
viewpoints.](img/simplesystem/views.png){#fig-views width="100%"}

You may also try the interactive demo below to see the 3D structure. (The demo supports mouse zoom in and out, pan, and rotate.)
<iframe src="./nb/demo_simple_sys/surf_with_colors.html" width="100%" height="500px" frameborder="0"></iframe>



# Generalization
## Generalization

One desired property of any vision system is it ability to
**generalize** outside of the domain for which it was designed to
operate. In the approach presented in this chapter, the domain of images
the algorithm is expected to work is defined by the assumptions
described in . Later in the book, when we describe learning-based
approaches, the training dataset will specify the domain. What happens
when assumptions are violated? Or when the images contain configurations
that we did not consider while designing the algorithm?


## Generalization
:::{.column-margin}
**Out of domain generalization** refers to the ability of a system to operate outside the **domain** for which it was designed.
:::

Let's run the algorithm with shapes that do not satisfy the assumptions
that we have made for the simple world. @fig-out_of_domain shows the
result when the input image violates several assumptions we made
(shadows are not soft and the green cube occludes the red one) and when
it has one object on top of the other, a configuration that we did not
consider while designing the algorithm. In this case the result happens
to be good, but it could have gone wrong.

## Generalization
![Reconstruction of an out-of-domain example. The input image violates
several assumptions we made (shadows are not soft and the green cube
occludes the red one) and it has one object on top of the other, a
configuration that we did not consider while designing the algorithm.
](img/simplesystem/out_of_domain_example.png){#fig-out_of_domain
width="100%"}

@fig-impossible shows the *impossible steps* inspired from @Adelson99.
On the left side, this shape looks rectangular and the stripes appear to
be painted on the surface. On the right side, the shape looks as though
it has steps, with the stripes corresponding to shading due to the
surface orientation. In the middle, the shape is ambiguous.
@fig-impossible shows the reconstructed 3D scene for this unlikely
image. The system has tried to approximate the constraints, but for this
shape it is not possible to exactly satisfy all the constraints.

## Generalization
![Reconstruction of an impossible figure (inspired from @Adelson99): the
reconstruction agrees with our 3D
perception.](img/simplesystem/steps.png){#fig-impossible
width="100%"}

# Concluding Remarks
## Concluding Remarks

Despite having a 3D representation of the structure of the scene, the
system is still unaware of the fact that the scene is composed of a set
of distinct objects. For instance, as the system lacks a representation
of which objects are actually present in the scene, we cannot visualize
the occluded parts. The system cannot do simple tasks like counting the
number of cubes.

## Concluding Remarks
A different approach to the one discussed here is model-based scene
interpretation, where we could have a set of predefined models of the
objects that can be present in the scene and the system can try to
decide if they are present or not in the image, and recover their
parameters (i.e., pose, color).

## Concluding Remarks
Recognition allows indexing properties that are not directly available
in the image. For instance, we can infer the shape of the invisible
surfaces. Recognizing each geometric figure also implies extracting the
parameters of each figure (i.e., pose, size).

## Concluding Remarks
Given a detailed representation of the world, we could render the image
back, or at least some aspects of it. We can check that we are
understanding things correctly and judge if we can make verifiable
predictions, such as what we would see if we looked behind the object.
Closing the loop between interpretation and input will be productive at
some point.

In this chapter, besides introducing some useful vision concepts, we
also made use of mathematical tools from algebra and calculus that you
should be familiar with.
