---
title: "Intro to learning"
code-fold: false
execute:
  enabled: false
  cache: true
diagram:
  cache: true
  cache-dir: ./cache
  engine:
    tikz:
      execpath: lualatex
      additional-packages: |
        \usepackage{neuralnetwork}
        \usepackage{mathtools}
        \usepackage{amsmath}
        \pgfplotsset{compat=1.16}
        \usepackage{pgfplots}
        \newcommand\mybox[2][]{\tikz[overlay]\node[fill=blue!20,inner sep=2pt, anchor=text, rectangle, rounded corners=1mm,#1] {#2};\phantom{#2}}
        \usetikzlibrary{arrows.meta}
        \usetikzlibrary{positioning}
        \usetikzlibrary{shapes.misc}
        \usetikzlibrary{decorations.pathreplacing}
filters:
  - diagram
format: 
  revealjs:
    preview-links: auto
    slide-number: true
    theme: default
    multiplex:
      url: 'https://mplex.vitv.ly'
      secret: '45a3fc3fd7bd6e12fb2ee330f2a2bb5b'
      id: '0eeddbfb642a8eb99f1980947a12ab30c31c6520ca4b825ef73a04eeaa411db9'
---

## Notation

- The algorithms we will see apply to many kinds of signals, not just images. Therefore, in this part we will use $\mathbf{x}$ to represent model inputs rather than ${\boldsymbol\ell}$. A model's final output will usually be represented by $\mathbf{y}$.
  
- Neural networks consist of a sequence of layers that perform a sequence of transformations $\mathbf{x}_0 \rightarrow \mathbf{x}_1 \rightarrow \ldots \rightarrow \mathbf{y}$. When we consider a single layer in isolation, we will generically refer to its input as ${\mathbf{x}_{\texttt{in}}}$ and its output as ${\mathbf{x}_{\texttt{out}}}$. We will also use the variables $\mathbf{h}$ and $\mathbf{z}$ to represent certain kinds of intermediate representations in neural nets, which will be defined when they are first used.


## Introduction

:::{.callout-tip icon=false}
## Goal
The goal of learning is to extract lessons from past experience in order
to solve future problems.

Typically, this involves searching for an
algorithm that solves past instances of the problem.
:::

:::{.callout-note}
Past and future do not necessarily refer to the calendar date; instead they refer to what the \textit{learner} has previously seen and what the learner will see next.
:::


## Introduction
:::{.callout-tip icon=false}
## Algorithm
Because learning is itself an algorithm, it can be understood as a
meta-algorithm: an algorithm that outputs algorithms
(@fig-learning_as_meta_algorithm).
:::

![Learning is an algorithm that outputs algorithms.](./img/intro_to_learning/learning_as_meta_algorithm.png){#fig-learning_as_meta_algorithm width="75%"}

## Introduction
:::{.callout-tip icon=false}
## Phases
Learning usually consists of two phases:

i the **training**  phase, where we search for
an algorithm that performs well on past instances of the problem
(training data)
- the **testing** phase, where we deploy our learned algorithm to
solve new instances of the problem.
:::


## Learning from Examples

:::{.callout-tip icon=false}
## Example
Imagine you find an ancient mathematics text, with marvelous looking
proofs, but there is a symbol you do not recognize, "$\star$\". You see
it being used here and there in equations, and you note down examples of
its behavior: 
$$\begin{aligned}
    2 \star 3 &= 36\nonumber \\
    7 \star 1 &= 49\nonumber \\
    5 \star 2 &= 100\nonumber \\
    2 \star 2 &= 16\nonumber
\end{aligned}
$$ 
:::

:::{.callout-warning icon=false}
## Question
What do you think $\star$ represents? 
:::

## Learning from Examples
![How your brain may have solved the star problem.](./img/intro_to_learning/star_symbol_learning.png){#fig-star_symbol_learning}


## Learning from Examples
:::{.callout-note}
This kind of learning, where you observe example input-output behavior and infer a functional mapping that explains this behavior, is called **supervised learning**.

Another name for this kind of learning is **fitting a model** to data.
:::

:::{.callout-note}
## Non-computability
Some things are not learnable from examples, such as
noncomputable functions. An example of a noncomputable function is a
function that takes as input a program and outputs a 1 if the program
will eventually finish running, and a 0 if it will run forever. 
:::

## Learning from Examples
:::{.callout-tip icon=false}
## Example - formal definition
A formal definition of *example*, is an
{`input`, `output`} pair.

The examples you were given for $\star$
consisted of four such pairs: 

$$\begin{aligned}
    &\{\texttt{input:} [2,3], \texttt{output:} 36\}\nonumber \\
    &\{\texttt{input:} [7,1], \texttt{output:} 49\}\nonumber \\
    &\{\texttt{input:} [5,2], \texttt{output:} 100\}\nonumber \\
    &\{\texttt{input:} [2,2], \texttt{output:}16\}\nonumber
\end{aligned}$$ 
:::


## Learning from Examples
![A complicated function that could be learned from examples. This example is from @hays2007scene. ](./img/intro_to_learning/inpainting_example.png){#fig-intro_to_learning-inpainting_example}

:::{.callout-note icon=false}
## Answer
$F$ fills in the missing pixels.

But - how exactly does $F$ fill in the missing pixels?
:::


## Learning without Examples

:::{.callout-tip icon=false}
## Unsupervised learning

We are given examples of *input data*
$\{x^{(i)}\}^N_{i=1}$ but we are not told the target outputs
$\{y^{(i)}\}^N_{i=1}$. Instead the learner has to come up with a model
or representation of the input data that has useful properties, as
measured by some **objective function**. 
:::


:::{.callout-note icon=false}
## Reinforcement learning
We suppose that we are given a **reward function** that
explicitly measures the quality of the learned function's output. To be
precise, a reward function is a mapping from outputs to scores:
$$
r: \mathcal{Y} \rightarrow \mathbb{R}
$$.

The learner tries to come up with a function that **maximizes rewards**. 
:::

:::{.callout-important}
## Difference
Unsupervised learning has access to training data whereas reinforcement learning usually does not; instead the reinforcement learner has to collect its own training data.
:::

## Key Ingredients

:::{.callout-tip icon=false}
## Ingredients
A learning algorithm consists of three key ingredients:

1.  What does it mean for the learner to succeed, or, at least, to
    perform well?

2.  What is the set of possible mappings from inputs to outputs that
    we will we search over?

3.  *How*, exactly, do we search the hypothesis space for a specific
    mapping that maximizes the objective?
:::

::: {.centered}
![](./img/intro_to_learning/key_ingredients.png){}
:::

## Key Ingredients

:::{.callout-tip icon=false}
## Learner's algorithm
$$
f: \mathcal{X} \rightarrow \mathcal{Y},
$$
Commonly, $f$ is referred to as the **learned function**.
:::

:::{.callout-note icon=false}
## Learner's objective
Function that scores model outputs:
$$
\mathcal{L}: \mathcal{Y} \rightarrow \mathbb{R},
$
or function that compares model outputs to target answers:
$$
\mathcal{L}: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}.$$

We will interchangeably call this $\mathcal{L}$ either the **objective function**, the **loss function**, or the **loss**.
:::

::: aside
A loss almost always refers to an objective we seek to *minimize*, whereas an objective function can be used to describe objectives we seek to minimize as well as those we seek to maximize.
:::

## Hypothesis space

:::{.callout-tip icon=false}
## Description
The hypothesis space can be described by a set $\mathcal{F}$ of all the
possible functions under consideration by the learner.

Examples:

1. All mappings from $\mathbb{R}^2 \rightarrow \mathbb{R}$ 
2. All functions $\mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}_{\geq 0}$
that satisfy the conditions of being a distance metric.
:::

:::{.callout-note icon=false}
## Parameterization
We may say that our *parameterized*
hypothesis space is 
$$
y = \theta_1 x + \theta_0m
$$
where $\theta_0$ and $\theta_1$ are the parameters.

This example corresponds to the space of affine functions from $\mathbb{R} \rightarrow \mathbb{R}$, but this is
not the only way to parameterize that space.

Another choice could be
$y = \theta_2\theta_1 x + \theta_0$, with parameters $\theta_0$,
$\theta_1$, and $\theta_2$: **same space**, but **different parameterizations**!
:::

:::{.column-margin}
**Overparameterized** models, where you use more parameters than the minimum necessary to fit the data, are especially important in modern computer vision; most neural networks are overparameterized.
:::


## Empirical Risk Minimization

:::{.callout-tip icon=false}
## ERM model
Supervised setting:

we are learning a function that predicts $\mathbf{y}$ from $\mathbf{x}$ given many training examples
$\{\mathbf{x}^{(i)},\mathbf{y}^{(i)}\}^N_{i=1}$.

The idea is to minimize the average error (i.e., risk) we incur over all the training data
(i.e., empirical distribution). The ERM problem is stated as follows:

$$
\begin{aligned}
    \mathop{\mathrm{arg\,min}}_{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^N \mathcal{L}(f(\mathbf{x}^{(i)}),\mathbf{y}^{(i)}) \quad\triangleleft\quad \text{ERM}
\end{aligned}
$$

Here, $\mathcal{F}$ is the hypothesis space, $\mathcal{L}$ is the loss function, and $\{\mathbf{x}^{(i)}, \mathbf{y}^{(i)}\}_{i=1}^N$ is the training data (example {`input`, `output`} pairs), and $f$ is the learned function.
:::

## Learning as Probabilistic Inference

:::{.callout-tip icon=false}
## Probabilistic inference
We can interpret ERM as doing maximum likelihood probabilistic inference.

In this interpretation, we are trying to infer the hypothesis $f$ that assigns the highest probability to the data. For a model that predicts $\mathbf{y}$ given $\mathbf{x}$, the max likelihood $f$ is:

$$\begin{aligned}
    \mathop{\mathrm{arg\,max}}_f p\big(\{\mathbf{y}^{(i)}\}_{i=1}^N \bigm | \{\mathbf{x}^{(i)}\}_{i=1}^N, f\big) \quad\quad \triangleleft \quad\text{Max likelihood learning}
\end{aligned}
$$
:::

## Learning as Probabilistic Inference
:::{.callout-tip icon=false}
## Probabilistic inference
The term
$p\big(\{\mathbf{y}^{(i)}\}_{i=1}^N \bigm | \{\mathbf{x}^{(i)}\}_{i=1}^N, f\big)$
is called the **likelihood** of the $\mathbf{y}$ values given the model
$f$ and the observed $\mathbf{x}$ values, and maximizing this quantity
is called **maximum likelihood learning**.
:::

:::{.column-margin}
To fully specify this model, we have to define the form of this conditional distribution. One common choice is that the prediction errors, $(\mathbf{y} - f(\mathbf{x}))$, are Gaussian distributed, which leads to the least-squares objective.
:::

## Learning as Probabilistic Inference
:::{.callout-tip icon=false}
## MAP learning
**Priors** $p(f)$ can also be used
for inferring the most probable hypothesis. When a prior is used in
conjunction with a likelihood function, we arrive at **maximum a
posteriori learning** (**MAP learning**), which infers the most probable
hypothesis given the training data: 

$$\begin{aligned}
    &\mathop{\mathrm{arg\,max}}_f p\big(f \bigm | \{\mathbf{x}^{(i)}, \mathbf{y}^{(i)}\}_{i=1}^N\big) \quad\quad \triangleleft \quad \text{MAP learning}\\
    & = \mathop{\mathrm{arg\,max}}_f p\big(\{\mathbf{y}^{(i)}\}_{i=1}^N \bigm | \{\mathbf{x}^{(i)}\}_{i=1}^N, f\big)p\big(f\big) \quad\quad \triangleleft \quad \text{by Bayes' rule}
\end{aligned}$$
:::

## Linear Least-Squares Regression

:::{.callout-tip icon=false}
## Description
One of the simplest learning problems is known as **linear least-squares
regression**. In this setting, we aim to model the relationship between
two variables, $x$ and $y$, with a line.
:::

:::{.callout-note icon=false}
## Example
As a concrete example, let's imagine $x$ represents the temperature
outside, and $y$ represents the number people at the beach. As before,
we train (i.e., fit) our model on many observed examples of
{`temperature outside`, `number of people at the beach`} pairs, denoted
as $\{x^{(i)},y^{(i)}\}_{i=1}^N$. At test time, this model can be
applied to predict the $y$ value of a new query $x'$, as shown in
@fig-intro_to_learning-ols_train_test.
:::

## Linear Least-Squares Regression
![The goal of learning is to use the training data to predict the $y$ value of the test query. In our example we find that for every 1 degree increase in temperature, we can expect $\sim 10$ more people to go to the beach.](./img/intro_to_learning/ols_train_test.png){#fig-intro_to_learning-ols_train_test width="70%"}

## Linear Least-Squares Regression

:::{.callout-tip icon=false}
## Hypothesis space
The relationship between $x$ and our predictions $\hat{y}$ of $y$ has the form
$\hat{y} = f_{\theta}(x) = \theta_1 x + \theta_0$.
:::

:::{.callout-tip icon=false}
## Parameterization
This hypothesis space
is parameterized by a two scalars, $\theta_0, \theta_1 \in \mathbb{R}$,
the intercept and slope of the line.

We denote $\theta = [\theta_0, \theta_1]$. Learning consists of
finding the value of these parameters that maximizes the objective.
:::

## Linear Least-Squares Regression
:::{.callout-tip icon=false}
## Objective
Our *objective* is that predictions should be near ground truth targets
in a least-squares sense, that is, $(\hat{y}^{(i)} - y^{(i)})^2$ should
be small for all training examples $\{x^{(i)}, y^{(i)}\}_{i=1}^N$. We
call this objective the $L_2$ loss: 

$$\begin{aligned}
    J(\theta) &= \sum_i \mathcal{L}(\hat{y}^{(i)}, y^{(i)})\\
    &\quad \mathcal{L}(\hat{y}, y) = (\hat{y} - y)^2 \quad\quad \triangleleft \quad L_2 \text{ loss}
\end{aligned}$$ 
:::

:::{.callout-note}
We will use $J(\theta)$ to denote the total objective, over all training datapoints, as a function of the parameters; we will use $\mathcal{L}$ to denote the loss per datapoint. That is, $J(\theta) = \sum_{i=1}^N \mathcal{L}(f_{\theta}(x^{(i)}), y^{(i)})$.
:::


## Linear Least-Squares Regression
:::{.callout-tip icon=false}
## Problem statement
The full learning problem is as follows: 
$$\begin{aligned}
    \theta^* = \mathop{\mathrm{arg\,min}}_{\theta} \sum_{i=1}^N (\theta_1 x^{(i)} + \theta_0 - y^{(i)})^2.
\end{aligned}
$$
:::

:::{.callout-warning icon=false}
## Random solution?
A first idea might be "try a bunch of random values for $\theta$ and return the one that maximizes the objective." 

Will be slow!
:::

## Linear Least-Squares Regression
:::{.callout-tip icon=false}
## Calculus way
We are trying to find the minimum of the objective $J(\theta)$: 

$$\begin{aligned}
    J(\theta) = \sum_{i=1}^N (\theta_1 x^{(i)} + \theta_0 - y^{(i)})^2.
\end{aligned}$$ 

This function can be rewritten as 
$$\begin{aligned}
    J(\theta) = (\mathbf{y} - \mathbf{X}\theta)^\mathsf{T}(\mathbf{y} - \mathbf{X}\theta)
\end{aligned}
$$ 
with 
$$\begin{aligned}
\mathbf{X} = 
 \begin{bmatrix}
    1 & x^{(1)}  \\
    1 & x^{(2)} \\
    \vdots & \vdots \\
    1 & x^{(N)}
\end{bmatrix}
\quad
\mathbf{y} = 
 \begin{bmatrix}
    y^{(1)}  \\
    y^{(2)} \\
    \vdots \\
    y^{(N)}
\end{bmatrix}
\quad
\theta = 
 \begin{bmatrix}
    \theta_0 \\
    \theta_1
\end{bmatrix}.
\end{aligned}$$
:::

## Linear Least-Squares Regression
:::{.callout-tip icon=false}
## Calculus way
The $J$ is a quadratic form, which has a single global minimum where the derivative is zero, and no other points where the derivative is zero. Therefore, we can solve for the $\theta^*$ that minimizes $J$ by finding the point where the derivative is zero. The derivative is:
$$\begin{aligned}
    \frac{\partial J(\theta)}{\partial \theta} =  2(\mathbf{X}^\mathsf{T}\mathbf{X} \theta - \mathbf{X}^\mathsf{T}\mathbf{y}).
\end{aligned}
$$ 

We set this derivative to zero and solve for $\theta^*$:
$$
\begin{aligned}
    2(\mathbf{X}^\mathsf{T}\mathbf{X} \theta^* - \mathbf{X}^\mathsf{T}\mathbf{y}) &= 0\\
\mathbf{X}^\mathsf{T}\mathbf{X} \theta^* &= \mathbf{X}^\mathsf{T}\mathbf{y}\\
\theta^* &= (\mathbf{X}^\mathsf{T}\mathbf{X})^{-1}\mathbf{X}^\mathsf{T}\mathbf{y}.
\end{aligned}
$$
:::

## Linear Least-Squares Regression

![The $\theta^*$ defines the best fitting line to our data. A best fit line is a visualization of a function $f_{\theta}$, that predicts the $y$-value for each input $x$-value.](./img/intro_to_learning/ols_fit.png){#fig-intro_to_learning-ols_fit}

## Linear Least-Squares Regression

![Linear regression finds a line that predicts the training data's $y$-values from its $x$-values.](./img/intro_to_learning/ols_summary.png){#fig-ols_summary #fig-intro_to_learning-ols_system_diagram}

::: aside
In these diagrams, we will sometimes describe the objective just in terms of $\mathcal{L}$, in which case it should be understood that this implies $J(\theta) = \sum_{i=1}^N \mathcal{L}(f_{\theta}(x^{(i)}), y^{(i)})$.
:::


## Program Induction

:::{.callout-tip icon=false}
## Program induction
**Program induction**: one of the broadest classes of learning algorithm.
In this setting, our hypothesis space may be all Python programs.
:::

![Linear regression finds a line that predicts the training data's $y$-values from its $x$-values.](./img/intro_to_learning/ols_system_diagram.png){#fig-intro_to_learning-ols_system_diagram}


## Program Induction
![Python program induction finds a Python program that predicts the training data's $y$-values from its $x$-values.](./img/intro_to_learning/program_induction_system_diagram.png){#fig-intro_to_learning-program_induction_system_diagram}

## Classification and Softmax Regression

:::{.callout-tip icon=false}
## Definition
A common problem in computer vision is to recognize objects. Our input is an image $\mathbf{x}$, and our target output is a
class label $\mathbf{y}$ (@fig-intro_to_learning-image_classification).

![Image classification.](./img/intro_to_learning/image_classification.png){#fig-intro_to_learning-image_classification width="70%"}
:::

## Classification and Softmax Regression
:::{.callout-tip icon=false}
## Input
$$\mathbf{x} \in \mathbb{R}^{H \times W \times 3},
$$
where $H$ is image height and $W$ is image width.
:::

:::{.callout-note icon=false}
## Output
Let $\mathbf{y}$ be a $K$-dimensional vector, for
$K$ possible classes, with $y_k = 1$ if $\mathbf{y}$ represents class
$k$, and $y_k = 0$ otherwise.

This representation is called a **one-hot code**.
:::


## Classification and Softmax Regression
:::{.callout-tip icon=false}
## Goal
Learn a function $f_{\theta}$ that output
vectors $\hat{\mathbf{y}}$ that match the one-hot codes, thereby
correctly classifying the input images. 
:::

![An example of one-hot codes for representing $K$=5 different classes.](img/intro_to_learning/one_hot_codes.png){}


## Classification and Softmax Regression
:::{.callout-warning icon=false}
## Loss function - version 1
Perhaps we should minimize misclassifications? That would correspond to the so
called **0-1 loss**: 

$$
\begin{aligned}
    \mathcal{L}(\hat{\mathbf{y}},\mathbf{y}) = \mathbb{1}(\hat{\mathbf{y}}\neq\mathbf{y}),
\end{aligned}
$$ where $\mathbb{1}$ is the indicator function that
evaluates to 1 if and only if its argument is true, and 0 otherwise.
Unfortunately, minimizing this loss is a discrete optimization problem,
and it is **NP-hard**.
:::

:::{.callout-note icon=false}
## Loss function - version 2
Instead, people commonly use the cross-entropy loss, which is
continuous and differentiable (making it easier to optimize):
$$\begin{aligned}
    \mathcal{L}(\hat{\mathbf{y}},\mathbf{y}) = H(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{k=1}^K y_k \log \hat{y}_k \quad\quad \triangleleft \quad \text{cross-entropy loss}
\end{aligned}$$ 
:::


## Classification and Softmax Regression
:::{.callout-tip icon=false}
## Interpretation
$\hat{y}_k$ should *represent the probability* we think the image is an image of class $k$.
Under that interpretation, minimizing cross-entropy maximizes the log
likelihood of the ground truth observation $\mathbf{y}$ under our
model's prediction $\hat{\mathbf{y}}$.

For that interpretation to be valid, we require that $\hat{\mathbf{y}}$
represent a (**pmf**). A pmf $\mathbf{p}$, over $K$ classes, is defined
as a $K$-dimensional vector with elements in the range $[0,1]$ that sums
to 1. In other words, $\mathbf{p}$ is a point on the
$(K-1)$-**simplex**, which we denote as
$\mathbf{p} \in \vartriangle^{K-1}$.
:::


::: aside
The $(K-1)$-simplex, $\vartriangle^{K-1}$, is the set of all $K$-dimensional vectors whose elements sum to 1. $K$-dimensional one-hot codes live on the vertices of $\vartriangle^{K-1}$.
:::


## Classification and Softmax Regression
:::{.callout-tip icon=false}
## Procedure
To ensure that the output of our learned function $f_{\theta}$ has this
property, i.e., $f_{\theta} \in \vartriangle^{K-1}$, we can compose two
steps: (1) first apply a function
$z_{\theta}: \mathcal{X} \rightarrow \mathbb{R}^K$, (2) then squash the
output into the range $[0,1]$ and normalize it to sum to 1. 
:::


:::{.callout-note icon=false}
## Squashing
A popular way to squash is via the **softmax** function:

$$\begin{aligned}
    &\mathbf{z} = z_{\theta}(\mathbf{x})\\
    &\hat{\mathbf{y}} = \texttt{softmax}(\mathbf{z})\\
    &\quad \quad \hat{y}_j = \frac{e^{-z_j}}{\sum_{i=1}^K e^{-z_k}}.
\end{aligned}$$ 

The values in $\mathbf{z}$ are called the **logits** and
can be interpreted as the unnormalized log probabilities of each class.
:::


Now we have, 

$$\begin{aligned}
    \hat{\mathbf{y}} = f_{\theta}(\mathbf{x}) = \texttt{softmax}(z_{\theta}(\mathbf{x}))
\end{aligned}$$ 

## Classification and Softmax Regression

![Softmax regression for image classification. The $\odot$ symbol represents an elementwise product. The cross-entropy loss is the negative of the sum over elementwise agreements between the prediction vector $\hat{\mathbf{y}}$ and the label vector $\mathbf{y}$, that is, if $\mathbf{s} = \mathbf{y} \odot \log \hat{\mathbf{y}}$ is the vector of scores for how well our prediction agrees with the label, then our cross-entropy loss is $H(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{k=1}^K s_k$.](./img/intro_to_learning/softmax_regression_diagram.png){#fig-softmax_regression_diagram}

The prediction placed about 40 percent probability on the true class,
"guitarfish," so we are 60 percent off from an ideal prediction
(indicated by the red bar; an ideal prediction would place 100 percent
probability on the true class). Our loss is $-\log 0.4$.

## Classification and Softmax Regression
This learning problem, which is also called , can be summarized as
follows:

![Learning is a meta-algorithm, an algorithm that outputs algorithms; metalearning is just learning applied to learning, and therefore it is a meta-meta-algorithm.](./img/intro_to_learning/softmax_regression_learning_problem.png){#fig-softmax_regression_learning_problem #fig-intro_to_learning-meta_learning_diagram}

## Learning to Learn
:::{.callout-tip icon=false}
## Metalearning

It's a special case of learning where the hypothesis space is learning algorithms.

The goal of metalearning is to handle the case where the future problem we will
encounter is itself a learning problem.
:::

:::{.callout-note icon=false}
## Example
Suppose that we are given the following {`input`, `output`}
examples: 

$$\begin{aligned}
    &\{\texttt{input:} \big(x:[1,2], y:[1,2]\big), &&\texttt{output:} y = x\}\nonumber \\
    &\{\texttt{input:} \big(x:[1,2], y:[2,4]\big), &&\texttt{output:} y = 2x\}\nonumber \\
    &\{\texttt{input:} \big(x:[1,2], y:[0.5,1]\big), &&\texttt{output:} y = \frac{x}{2}\}\nonumber
\end{aligned}$$

The learner can fit these examples by learning to
perform least-squares regression.
:::

## Learning to Learn

![Learning is a meta-algorithm, an algorithm that outputs algorithms; metalearning is learning applied to learning: a **meta-meta-algorithm**.](./img/intro_to_learning/meta_learning_diagram.png){#fig-intro_to_learning-meta_learning_diagram}

## Learning to Learn
:::{.callout-tip icon=false}
## Recursion
Notice that you can apply this idea recursively, constructing
meta-meta-\...-metalearners.

Humans perform at least three levels of
this process, if not more: we have *evolved* to be *taught* in school
how to *learn* quickly on our own. 
:::

::: aside
**Evolution** is a learning algorithm according to our present definition.
:::

# Gradient-Based Learning Algorithms

## Gradient-Based Learning Algorithms

:::{.callout-tip icon=false}
## Overview
Once you have specified a learning problem (loss function, hypothesis space, parameterization), the next step is to find the parameters that minimize the loss.

This is an optimization problem, and the most common optimization algorithm we will use is **gradient descent**. 

There are many varieties of gradient descent, and we will call this whole family **gradient-based learning algorithms**. All share the same basic idea: at some operating point, calculate the direction of steepest descent, then use this direction to find a new operating point with lower loss.
:::

::: aside
We use the term **operating point** to refer to a particular point (setting of the parameters) where we are currently evaluating the loss.
:::

## Gradient-Based Learning Algorithms

:::{.callout-tip icon=false}
## Setting
We consider the task of minimizing a cost function $J: \cdot \rightarrow \mathbb{R}$, which is a function that maps some arbitrary input to a scalar cost.

In learning problems, the domain of $J$ is the training data and the parameters $\theta$. We will often consider the training data to be fixed and only denote the objective as a function of the parameters, $J(\theta)$. Our goal is to solve:
$$
\theta^* = \arg\min_{\theta} J(\theta)
$$
:::

## Gradient-Based Learning Algorithms

![General optimization loop.](./img/gradient_descent/optimization_schematic.png){#fig-gradient_descent-optimization_schematic width=50%}

## Gradient-Based Learning Algorithms
:::{.callout-tip icon=false}
## Zeroth-order optimization**
The update function only gets to observe the value $J(\theta)$. The only way, then, to find $\theta$'s that minimize the loss is to sample different values for $\theta$ and move toward the values that are lower.
:::

:::{.callout-note icon=false}
## First-order optimization
Also called **gradient-based optimization**: the update function takes as input the gradient of the cost with respect to the parameters at the current operating point, $\nabla_{\theta}J(\theta)$. This reveals hugely useful information about the loss that directly tells us how to minimize it: just move in the direction of steepest descent, that is, the gradient direction.
:::

::: aside
Higher-order optimization methods observe higher-order derivatives of the loss, such as the Hessian $H$, which tells you how the landscape is locally curving.
:::

## Gradient-Based Learning Algorithms

![Gradient descent `GD`. Optimizing a cost function $J: \theta \rightarrow \mathbb{R}$ by descending the gradient $\nabla_{\theta} J$.](./img/gradient_descent/alg1.png){#alg-gradient_descent_basic_gradient_descent}

## Gradient-Based Learning Algorithms
:::{.callout-tip icon=false}
## Hyperparameters

- **learning rate** $\eta$, which controls the step size (learning rate times gradient magnitude)
- the number of steps $K$.

If the learning rate is sufficiently small and the initial parameter vector $\theta^0$ is random, then this algorithm will almost surely converge to a local minimum of $J$ as $K \rightarrow \infty$~\cite{lee2016gradient}. However, to descend more quickly, it can be useful to set the learning rate to a higher value. 
:::


## Gradient-Based Learning Algorithms
:::{.callout-tip icon=false}
## Learning Rate Schedules
We are calling some function $\texttt{lr}(\eta^0,k)$ to get the learning rate on each iteration of descent:
$$
\eta^{k} = \texttt{lr}(\eta^0,k)
$$
Generally, we want an update rule where $\eta^{k+1} < \eta^k$ (making smaller steps).
:::

:::{.callout-note icon=false}
## Examples
$$
\begin{aligned}
    \texttt{lr}(\eta^0,k) &= \beta^{-k} \eta^0 &\quad\quad \triangleleft\quad \text{exponential decay}\\
    \texttt{lr}(\eta^0,k) &= \beta^{-\lfloor k/M \rfloor} \eta^0 &\quad\quad \triangleleft\quad \text{stepwise exponential decay}\\
    \texttt{lr}(\eta^0,k) &= \frac{(K - k)}{K} \eta^0 &\quad\quad \triangleleft\quad \text{linear decay}
\end{aligned}
$$
:::

## Gradient-Based Learning Algorithms

![Gradient descent with learning rate decay algorithm.](./img/gradient_descent/alg2.png){#alg-gradient_descent_gradient_descent_with_lr_decay}

## Gradient-Based Learning Algorithms
:::{.callout-tip icon=false}
## Momentum

Momentum means that we set the parameter update to be a direction $\mathbf{v}^{k+1}$, given by a weighted combination of the previous update direction, $\mathbf{v}^{k}$, plus the current negative gradient:
$$
\mathbf{v}^{k+1} = \mu \mathbf{v}^{k} - \eta\nabla_{\theta} J(\theta^k)
$$
The weight $\mu$ in this combination is a new hyperparameter, sometimes simply called the momentum.
:::

## Gradient-Based Learning Algorithms
![Gradient descent with momentum algorithm.](./img/gradient_descent/alg3.png){#alg-gradient_descent_gradient_descent_with_momentum}

## Gradient-Based Learning Algorithms
![(left) A simple loss function $J = \texttt{abs}(\theta)$. (right) Optimization trajectory for three different settings of momentum $\mu$. White line indicates value of the parameter at each iteration of optimization, starting at top and progressing to bottom. Color is value of the loss. Red dot is location where loss first reaches within $0.01$ of optimal value.](./img/gradient_descent/momentum_out1.png){#fig-gradient_descent-momentum_out1}

## Gradient-Based Learning Algorithms {.scrollable}

![](./img/gradient_descent/grad_descent.png){#fig-gradient_descent-grad_descent_simple_examples}

<!-- ![How gradient descent behaves on various functions.** In each subplot, the left shows the function $J$, with the red point representing the solution found by gradient descent (GD) with $\eta=0.01$ and $\mu=0.9$. The right shows the trajectory of $x$ values over iterations of GD, plotted on top of $J$ at each iteration. (a) As $\eta$ goes to zero, GD converges for convex functions. (b) Discontinuities pose no essential problem, as long as the gradient is defined on either side. (c) A nearly flat function will exhibit very slow descent. (d) Piecewise constant functions are problematic because the gradient completely vanishes. (e) For the function $J=\texttt{sqrt}(\texttt{abs}(\theta))-0.25$, the gradient goes to infinity at the minimizer, causing instability. (f) When $J$ has multiple local minima, we may not find the global minimum.](./img/gradient_descent/grad_descent.png){#fig-gradient_descent-grad_descent_simple_examples} -->


## Gradient-Based Learning Algorithms
:::{.callout-tip icon=false}
## Alternatives
What are some other good choices for $\mathbf{v}$?

1. One common idea is to set $\mathbf{v}$ to be the gradient of a **surrogate loss** function, which is a function, $J_{\texttt{surr}}$, with meaningful (non-zero) gradients that approximates $J$. An example might be a smoothed version of $J$.

2. Another way to get $\mathbf{v}$ is to compute it by sampling perturbations of $\theta$, and seeing which perturbation leads to lower loss. In this strategy, we evaluate $J(\theta+\epsilon)$ for a set of perturbations $\epsilon$, then move toward the $\epsilon$'s that decreased the loss. Approaches of this kind are sometimes called **evolution strategies**.
:::

## Gradient-Based Learning Algorithms
![Evolution strategy algorithm.](./img/gradient_descent/alg4.png){#alg-gradient_descent_ES}


## Gradient-Based Learning Algorithms
![Using @alg-gradient_descent_ES) to minimize a nondifferentiable (zero-gradient) loss, using $\sigma=1$, $M=10$, and $\eta=0.02$.](./img/gradient_descent/sampling_out1.png){#fig-gradient_descent-sampling_out1}

## Gradient-Based Learning Algorithms
:::{.callout-tip icon=false}
## Gradient Clipping


![Gradient clipping algorithm.](./img/gradient_descent/alg5.png){#alg-gradient_descent_grad_clipping}
:::

:::{.callout-note}
`clip` is the "clipping" function: $\texttt{clip}(v, -m, m) = \max(\min(v,m),-m)$
:::

## Gradient-Based Learning Algorithms
:::{.callout-tip icon=false}
## Gradient Clipping

![Using `GD` with clipping to minimize a loss with exploding gradients, using $m=0.1$.](./img/gradient_descent/clipped_out1.png){width=80% #fig-gradient_descent-clipped_out1}
:::


## Stochastic Gradient Descent

:::{.callout-important icon=false}
## Expensive computation
To make this clear, we will write out $J$ as an explicit function of the training data $\{\mathbf{x}^{(i)}, \mathbf{y}^{(i)}\}_{i=1}^N$. For typical learning problems, $\nabla_{\theta} J(\theta, \{\mathbf{x}^{(i)}, \mathbf{y}^{(i)}\}_{i=1}^N)$ decomposes as follows:
$$
\begin{align}
    \nabla_{\theta} J(\theta, \{\mathbf{x}^{(i)}, \mathbf{y}^{(i)}\}_{i=1}^N) &= 
    \nabla_{\theta} \frac{1}{N}\sum_{i=1}^N \mathcal{L}(f_{\theta}(\mathbf{x}^{(i)}), \mathbf{y}^{(i)})\\
    &= \frac{1}{N}\sum_{i=1}^N \nabla_{\theta} \mathcal{L}(f_{\theta}(\mathbf{x}^{(i)}), \mathbf{y}^{(i)})
\end{align}
$$
:::

## Stochastic Gradient Descent
:::{.callout-tip icon=false}
## Batching

Suppose instead we randomly subsample (without replacement) a *batch* of terms from this sum, $\{\mathbf{x}^{(b)}, \mathbf{y}^{(b)}\}_{b=1}^B$, where $B$ is the **batch size**. We then compute an *estimate* of the total gradient as the average gradient over this batch as follows:
$$
\begin{align}
    \tilde{\mathbf{g}} = \frac{1}{N}\sum_{b=1}^B \nabla_{\theta} \mathcal{L}(f_{\theta}(\mathbf{x}^{(b)}), \mathbf{y}^{(b)})
\end{align}
$$

If we sample a large batch, where $B$ is almost as large as $N$, then the average over the $B$ terms should be roughly the same as the average over all $N$ terms. If we sample a smaller batch, then our estimate of the gradient will be less accurate but faster to compute. Therefore we have a tradeoff between accuracy and speed, and we can navigate this tradeoff with the hyperparameter $B$. 
:::

## Stochastic Gradient Descent

![Stochastic gradient descent algorithm. Stochastic gradient descent estimates the gradient from a stochastic subset (batch) of the full training data, and makes an update on that basis.](./img/gradient_descent/alg6.png){#alg-gradient_descent_SGD}

## Stochastic Gradient Descent
:::{.callout-tip icon=false}
## Properties

- Because each step of descent is somewhat random, $\texttt{SGD}$ can jump over small bumps in the loss landscape, as long those bumps disappear for some randomly sampled batches
- $\texttt{SGD}$ can implicitly regularize the learning problem. For example, for linear problems (i.e., $f_\theta$ is linear), then if there are multiple parameter settings that minimize the loss, $\texttt{SGD}$ will often converge to the solution with minimum parameter norm
:::

# The Problem of Generalization

## The Problem of Generalization

:::{.callout-tip icon=false}
## Train vs test
So far, we have described learning as an optimization problem: maximize
an objective over the *training set*. But this is not our actual goal.
Our goal is to maximize the objective over the *test set*. 
:::

:::{.callout-tip icon=false}
## Overfitting
Happens when we fit to properties in the training data that do not exist in the
test data.

This means that what we learned about the training data does not **generalize** to the
test data.
:::
:::{.callout-warning icon=false}
## Underfitting
Learner failed to optimize the objective on the training data.
:::

## The Problem of Generalization

:::{.callout-tip icon=false}
## Polynomial regression
The hypothesis space
is polynomial functions rather than linear functions, that is,
$$
\begin{aligned}
y = f_{\theta}(x) = \sum_{k=0}^K \theta_k x^k
\end{aligned}
$$ 

where $K$, the degree of the polynomial, is a hyperparameter of the hypothesis space.

:::

## The Problem of Generalization
:::{.callout-note icon=false}
## Equivalence of polynomial and linear regression
Let us consider the setting where we use the least-squares ($L_2$) loss function.

We can see this by rewriting the polynomial as: 

$$
\sum_{k=0}^K \theta_k x^k = \theta^\mathsf{T}\phi(x), \; \phi(x) = \begin{bmatrix}
  1 \\ x \\ x^2 \\ \vdots \\ x^K
\end{bmatrix}
$$ 

Now the form of $f_{\theta}$ is $f_{\theta}(x) = \theta^\mathsf{T}\phi(x)$, *which is a linear function in the parameters $\theta$*. Therefore, if we *featurize* $x$, representing each datapoint $x$ with a feature vector $\phi(x)$, then we have arrived at a linear regression problem in this feature space.
:::

## The Problem of Generalization
:::{.callout-note icon=false}
## Equivalence of polynomial and linear regression
So, the learning problem, and closed form optimizer, for $L_2$ polynomial regression looks almost identical to that of $L_2$ linear regression:


where 
$$\mathbf{\Phi} = 
     \begin{bmatrix}
        1 & x^{(1)} & x^{(1)^2} & ... & x^{(1)^K} \\
        1 & x^{(2)} & x^{(2)^2} & ... & x^{(2)^K} \\
        \vdots & \vdots & \vdots & \vdots & \vdots \\
        1 & x^{(N)} & x^{(N)^2} & ... & x^{(N)^K}  \\
    \end{bmatrix}$$ 

The matrix $\mathbf{\Phi}$ is an array of the features (columns) for
each datapoint (rows). It plays the same role as data matrix
$\mathbf{X}$ did earlier; in fact we often call matrices of the feature representations of each datapoint also as a **data matrix**.
:::

## The Problem of Generalization

![Underfitting and overfitting.](./img/problem_of_generalization/under_and_overfitting.png){#fig-under_and_overfitting}

## The Problem of Generalization
:::{.callout-tip icon=false}
## Data generating process
$$
\begin{aligned}
    Y &= X^2 + 1 &\triangleleft \quad\text{true underlying relationship}\\
    \epsilon &\sim \mathcal{N}(0,1) &\triangleleft \quad\text{observation noise}\\
    Y^\prime &= Y + \epsilon &\triangleleft \quad\text{noisy observations}\\
    x,y &\sim p(X,Y^{\prime}) &\triangleleft \quad\text{data-generating process}
\end{aligned}
$$
:::

## The Problem of Generalization
:::{.callout-tip icon=false}
## Why does overfitting happen?
It's because for $K=10$ the curve can become wiggly enough to
not just fit the true underlying relationship but also to *fit the
noise*, the minor offsets $\epsilon$ around the green line.

This noise
is *a property of the training data that does not generalize to the test
data*; the test data will have different observation noise.
:::


:::{.callout-warning icon=false}
## Multiple hypotheses
For $K=10$ there are many
hypotheses (polynomial functions) that perfectly the data (true
function + noise) -- there is insufficient data for the objective to
uniquely identify one of the hypotheses to be the best. Because of this,
the hypothesis output by the optimizer may be an arbitrary one
:::

## The Problem of Generalization
:::{.callout-tip icon=false}
## Approximation error 
The gap between the black line and the training data points. Let $\{x_{(\texttt{train})}^{(i)}, y_{(\texttt{train})}^{(i)}\}_{i=1}^N$ be
our training data set (the black points). Then the approximation error
$J_{\texttt{approx}}$ is defined as the total cost incurred on this
training data: 

$$\begin{aligned}
    J_{\texttt{approx}} = \frac{1}{N} \sum_{i=1}^N \mathcal{L}(f_{\theta}(x_{(\texttt{train})}^{(i)}), y_{(\texttt{train})}^{(i)})
\end{aligned}$$ 

Notice that approximation error is the cost function we minimize in empirical risk minimization.
:::

:::{.callout-warning icon=false}
## Generalization error
The gap between the black line and the green line, that is, the expected cost we would incur if we sampled a new test point at random
from the true data generating process. Generalization error is often
approximated by measuring performance on a heldout ,
$\{x_{(\texttt{val})}^{(i)}, y_{(\texttt{val})}^{(i)}\}_{i=1}^N$, which
can simply be a subset of the data that we don't use for training or
testing: 

$$\begin{aligned}
    J_{\texttt{gen}} &= \mathbb{E}_{x,y \sim p_{\texttt{data}}} [ \mathcal{L}(f_{\theta}(x), y)]\\
                        &\approx \frac{1}{N} \sum_{i=1}^N \mathcal{L}(f_{\theta}(x_{(\texttt{val})}^{(i)}), y_{(\texttt{val})}^{(i)})
\end{aligned}$$
:::

## The Problem of Generalization

![Approximation error `approx` versus generalization error `gen` for polynomial regression of order $K$. Here we measured error as the proportion of validation points that are mispredicted (defined as having an $L_2$ prediction error greater than 0.25).](./img/problem_of_generalization/under_and_overfitting_vs_polyK.png){#fig-under_and_overfitting_vs_polyK}

## Regularization

:::{.callout-tip icon=false}
## Goldilocks principle
The previous example suggests a kind of "Goldilocks principle." We
should prefer hypotheses (functions $f$) that are sufficiently
expressive to fit the data, but not so flexible that they can overfit
the data.
:::

:::{.callout-important icon=false}
## Regularization
Mechanisms that penalize function complexity so that we avoid
learning too flexible a function that overfits. Typically, regularizers
are terms we add to the objective that prefer simple functions in the
hypothesis space, all else being equal. They therefore embody the
principle of **Occam's razor**. 

The general form of a regularized objective is:
$$\begin{aligned}
    J(\theta) = \overbrace{\frac{1}{N} \sum^N_{i=1} \mathcal{L}(f_{\theta}(x)^{(i)}, y^{(i)})}^\text{data fit loss} + \underbrace{\lambda R(\theta)}_\text{regularizer} \quad\quad\triangleleft \quad\text{regularized objective function}
\end{aligned}$${#eq-problem_of_generalization-regularized_objective} 
where $\lambda$ is a hyperparameter that controls the strength of the regularization.
:::

## Regularization
:::{.callout-tip icon=false}
## Norm penalization
One of the most common regularizers is to penalize the $L_p$ norm of the
parameters of our model, $\theta$:
$$
R(\theta) = \left\lVert\theta\right\rVert_{p}.
$$ 

The $L_p$-norm of $\mathbf{x}$ is $(\sum_i |x_i|^{p})^{\frac{1}{p}}$. The $L_2$-norm is the familiar least-squares objective.

When $p=2$, the regularizer is called **Tikonov regression**. 
:::


## Regularization
:::{.callout-tip icon=false}
## Regularizers as Probabilistic Priors

Regularizers can be interpreted as **priors** that prefer, a priori
(before looking at the data), some solutions over others.

- Under this
interpretation, the data fit loss (e.g., $L_2$ loss) is a likelihood
function $p(\{y^{(i)}\}^N_{i=1} \bigm | \{x^{(i)}\}^N_{i=1}, \theta)$
and the regularizer is a prior $p(\theta)$.

- Bayes' rule then states that
the posterior $p(\theta \bigm | \{x^{(i)}, y^{(i)}\}^N_{i=1})$ is
proportional to the product of the prior and the likelihood. The log
posterior is then the *sum* of the log likelihood and the log prior,
plus a constant.
:::

## Regularization
:::{.callout-tip icon=false}
## Revisiting the $\star$ Problem

Remember the $\star$ problem:

$$\begin{aligned}
    3 \star 2 &= 36\nonumber \\
    7 \star 1 &= 49\nonumber \\
    5 \star 2 &= 100\nonumber \\
    2 \star 2 &= 16\nonumber
\end{aligned}$$ 

- $x \star y = (xy)^2$
- maybe $x \star y =  94.5x - 9.5x^2 + 4y^2 - 151$?
- or maybe
``` {.python xleftmargin="0.33" xrightmargin="0.33" fontsize="\\fontsize{8.5}{9}" frame="single" framesep="2.5pt" baselinestretch="1.05"}
def star(x,y):
    if x==2 && y==3:
        return 36
    elif x==7 && y==1:
        return 49
    elif x==5 && y==2:
        return 100
    elif x==2 && y==2:
        return 16
    else:
        return 0
```
:::

## Regularization
:::{.callout-tip icon=false}
## Bayesian Occam's razor** 
More complex hypothesis spaces must cover more possible hypotheses, and therefore
must assign less prior mass to any single hypothesis (the prior
probability of all possible hypotheses in the hypothesis space must sum
to 1) @jefferys1992ockham, @mackay2003information. This is why,
probabilistically, simpler hypotheses are more likely to be true.
:::

## Data, Priors, and Hypotheses

:::{.callout-tip icon=false}
## Three tools
1. **data**: observations of the world like photos and videos. Finding explanations
consistent with the observed data is the centerpiece of learning-based
vision.

2. **priors** (a.k.a. **regularizers**): prefer some solutions over others a priori.

3 set of **hypotheses** under
consideration for what the true function may be. The hypothesis space
constrains which solutions we can possibly find. 
:::

## Data, Priors, and Hypotheses
<!-- In this cartoon, we are learning a mapping from some domain -->
<!-- $\mathcal{X}$ to another domain $\mathcal{Y}$. The hypothesis space, -->
<!-- $\mathcal{F}$ (white circle; "the lamppost's light") places a hard -->
<!-- constraint on the subset of possible mappings under consideration, the -->
<!-- prior (yellow ellipse) places a soft constraint on which mappings are -->
<!-- preferred over which others, and the data (green ellipse) also places a -->
<!-- soft constraint on the space, preferring mappings that well fit the -->
<!-- data. -->

![A cartoon of the tools for honing in on the truth.](./img/problem_of_generalization/search_space_tools.png){#fig-problem_of_generalization-search_space_tools}

<!-- Approximation error is low within the green region. If we didn't care -->
<!-- about generalization, then it would be sufficient just to select any -->
<!-- solution in this green region. But since we do care about -->
<!-- generalization, we bias our picks toward the yellow region, which -->
<!-- corresponds to a prior that selects points we believe to be closer to -->
<!-- the true solution, even if they might not fit the data perfectly well. -->
<!-- These tools isolate the area outlined in bright yellow as the region -->
<!-- where we may find our needle of truth. A learning algorithm, which -->
<!-- searches over $\mathcal{F}$ in order to maximize the likelihood times -->
<!-- the prior, will find a solution somewhere in this outlined region. -->

## Data, Priors, and Hypotheses
:::{.callout-tip icon=false}
## Experiment 1: Effect of Data
Consider the following
empirical risk minimization problem:
$$
\begin{aligned}
    J(\theta; \{x^{(i)}, y^{(i)}\}^N_{i=1}) &= \frac{1}{N}\sum_i \lvert f_{\theta}(x^{(i)}) - y^{(i)}\rvert^{0.25} \quad\quad \triangleleft \quad\text{objective}:error_fn_1\\
    f_{\theta}(x) &= \theta_0 x + \theta_1 \sin(x)  \quad\quad \triangleleft \quad\text{hypothesis space}
\end{aligned}
$${#eq-problem_of_generalization-error_fn_1} 
:::


## Data, Priors, and Hypotheses
![*The more data you have, the less you need other modeling tools.*
.](./img/problem_of_generalization/more_data_more_constraints.png){#fig-problem_of_generalization-more_data_more_constraints}

## Data, Priors, and Hypotheses
:::{.callout-tip icon=false}
## Experiment 2: Effect of Priors

We will use a slightly different hypothesis space and
objective function
$$
\begin{aligned}
    J(\theta; \{x^{(i)}, y^{(i)}\}^N_{i=1}) &= \frac{1}{N}\sum_i \left\lVert f_{\theta}(x^{(i)}) - y^{(i)}\right\rVert_2^2 + \lambda \left\lVert\theta\right\rVert_2^2 \quad\quad \triangleleft \quad\text{objective}\\
    f_{\theta}(x) &= \theta_0 x + \theta_1 x \quad\quad \triangleleft \quad\text{hypothesis space}
\end{aligned}
$$ 
:::

## Data, Priors, and Hypotheses
![More regularization, more (soft) constraints.](./img/problem_of_generalization/more_regularizer_more_constraints.png){#fig-problem_of_generalization-more_regularizer_more_constraints}

## Data, Priors, and Hypotheses
:::{.callout-tip icon=false}
## Experiment 2: Effect of Priors

You can take away a few lessons from this example:

1.  Priors help only when they are good guesses as to the truth.

2.  Overreliance on the prior means ignoring the data, and this is
    generally a bad thing.

3.  For any given prior, there is a sweet spot where the strength is
    optimal. Sometimes this ideal strength can be derived from modeling
    assumptions and other times you may need to tune it as a
    hyperparameter.
:::

## Data, Priors, and Hypotheses
:::{.callout-tip icon=false}
## Experiment 3: Effect of the Hypothesis Space
Consider the
following three hypothesis spaces:
$$
\begin{aligned}
    f_{\theta}(x) &= \theta_0 x + \theta_1 x^2 &\triangleleft \quad\texttt{quadratic}\\
    f_{\theta}(x) &= \theta_0 x &\triangleleft \quad\texttt{linear}\\
    f_{\theta}(x) &= 0 &\triangleleft \quad\texttt{constant}
\end{aligned}
$$
:::

## Data, Priors, and Hypotheses

![Fewer hypotheses, more (hard) constraints](./img/problem_of_generalization/fewer_hypotheses_more_constraints.png){#fig-problem_of_generalization-fewer_hypotheses_more_constraints}

## Summary of the Experiments

:::{.callout-tip icon=false}
## General principle
::: center
*What can be achieved with any one of our tools can also be achieved
with any other.*
:::
:::


::: aside
However, note that the hypothesis space places *hard* constraints
    on our search; we cannot violate them. Data and priors apply *soft*
    constraints; we can violate them but we will pay a penalty.
:::
