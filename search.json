[
  {
    "objectID": "nb/cv24_pset1/pset1_2024.html",
    "href": "nb/cv24_pset1/pset1_2024.html",
    "title": "PSET 1",
    "section": "",
    "text": "# Ignore the pip dependency error\n\nimport cv2\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import convolve2d as conv2d\nimport scipy.sparse as sps\nfrom PIL import Image\n\n# # Package for fast equation solving\nfrom sys import platform\nprint(platform)\nif platform == \"linux\" or platform == \"linux2\":\n    ! apt-get install libsuitesparse-dev\nelif platform == \"darwin\":\n    ! brew install suite-sparse\n\n! pip3 install sparseqr\nimport sparseqr\n\ndarwin\n==&gt; Auto-updating Homebrew...\nAdjust how often this is run with HOMEBREW_AUTO_UPDATE_SECS or disable with\nHOMEBREW_NO_AUTO_UPDATE. Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n==&gt; Auto-updated Homebrew!\nUpdated 3 taps (oven-sh/bun, homebrew/core and homebrew/cask).\n==&gt; New Formulae\nab-av1                     gnome-online-accounts      node-red\nfalcosecurity-libs         keyutils                   oven-sh/bun/bun@1.2.5\nfpm                        krep                       sentry-native\ngcr                        libgoa                     yalantinglibs\ngit-graph                  libgudev\n==&gt; New Casks\nkate                                     mouseless@preview\n\nYou have 14 outdated formulae installed.\n\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/suite-sparse/manifests/7.10.1\n######################################################################### 100.0%\n==&gt; Fetching dependencies for suite-sparse: isl, libmpc, gcc and metis\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/isl/manifests/0.27\n######################################################################### 100.0%\n==&gt; Fetching isl\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/isl/blobs/sha256:de143fddb0e20b\n######################################################################### 100.0%\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/libmpc/manifests/1.3.1\n######################################################################### 100.0%\n==&gt; Fetching libmpc\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/libmpc/blobs/sha256:5c8cdc4d460\n######################################################################### 100.0%\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/gcc/manifests/14.2.0_1\n######################################################################### 100.0%\n==&gt; Fetching gcc\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/gcc/blobs/sha256:96d8bf02f621cf\n######################################################################### 100.0%\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/metis/manifests/5.1.0\n######################################################################### 100.0%\n==&gt; Fetching metis\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/metis/blobs/sha256:60ef633238eb\n######################################################################### 100.0%\n==&gt; Fetching suite-sparse\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/suite-sparse/blobs/sha256:edc57\n######################################################################### 100.0%\n==&gt; Installing dependencies for suite-sparse: isl, libmpc, gcc and metis\n==&gt; Installing suite-sparse dependency: isl\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/isl/manifests/0.27\nAlready downloaded: /Users/vitvly/Library/Caches/Homebrew/downloads/40b1c5526f95db33208143fa79887179e758121659d8877597f553e6e6188879--isl-0.27.bottle_manifest.json\n==&gt; Pouring isl--0.27.arm64_sequoia.bottle.tar.gz\nüç∫  /opt/homebrew/Cellar/isl/0.27: 74 files, 7.6MB\n==&gt; Installing suite-sparse dependency: libmpc\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/libmpc/manifests/1.3.1\nAlready downloaded: /Users/vitvly/Library/Caches/Homebrew/downloads/fdfa98e0f8bb3ce075cb32776ac2345aa2f89252706c162aecfc841085fa76be--libmpc-1.3.1.bottle_manifest.json\n==&gt; Pouring libmpc--1.3.1.arm64_sequoia.bottle.tar.gz\nüç∫  /opt/homebrew/Cellar/libmpc/1.3.1: 13 files, 492.3KB\n==&gt; Installing suite-sparse dependency: gcc\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/gcc/manifests/14.2.0_1\nAlready downloaded: /Users/vitvly/Library/Caches/Homebrew/downloads/cece94dbe926093c968a24f66b9a0172afe5cc2ef22253029bc591147237045b--gcc-14.2.0_1.bottle_manifest.json\n==&gt; Pouring gcc--14.2.0_1.arm64_sequoia.bottle.tar.gz\nüç∫  /opt/homebrew/Cellar/gcc/14.2.0_1: 1,914 files, 459.8MB\n==&gt; Installing suite-sparse dependency: metis\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/metis/manifests/5.1.0\nAlready downloaded: /Users/vitvly/Library/Caches/Homebrew/downloads/f3ed9b4299ad5a23ac4e265012f58bed7574a85759c297dbea217aeb89707128--metis-5.1.0.bottle_manifest.json\n==&gt; Pouring metis--5.1.0.arm64_sequoia.bottle.tar.gz\nüç∫  /opt/homebrew/Cellar/metis/5.1.0: 19 files, 12.1MB\n==&gt; Installing suite-sparse\n==&gt; Pouring suite-sparse--7.10.1.arm64_sequoia.bottle.tar.gz\nüç∫  /opt/homebrew/Cellar/suite-sparse/7.10.1: 217 files, 49.3MB\n==&gt; Running `brew cleanup suite-sparse`...\nDisable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\nHide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\nCollecting sparseqr\n  Downloading sparseqr-1.4.1.tar.gz (18 kB)\n  Installing build dependencies ... \u001b[?done\n  Getting requirements to build wheel ... done\n  Installing backend dependencies ... done\n  Preparing metadata (pyproject.toml) ... done\nRequirement already satisfied: numpy&gt;1.2 in /Users/vitvly/c/lnu/md/compvision/venv/lib/python3.12/site-packages (from sparseqr) (2.1.1)\nRequirement already satisfied: scipy&gt;=1.0 in /Users/vitvly/c/lnu/md/compvision/venv/lib/python3.12/site-packages (from sparseqr) (1.15.1)\nRequirement already satisfied: cffi&gt;=1.0 in /Users/vitvly/c/lnu/md/compvision/venv/lib/python3.12/site-packages (from sparseqr) (1.17.1)\nRequirement already satisfied: setuptools&gt;35 in /Users/vitvly/c/lnu/md/compvision/venv/lib/python3.12/site-packages (from sparseqr) (75.8.0)\nRequirement already satisfied: pycparser in /Users/vitvly/c/lnu/md/compvision/venv/lib/python3.12/site-packages (from cffi&gt;=1.0-&gt;sparseqr) (2.22)\nBuilding wheels for collected packages: sparseqr\n  Building wheel for sparseqr (pyproject.toml) ... done\n  Created wheel for sparseqr: filename=sparseqr-1.4.1-cp312-cp312-macosx_15_0_arm64.whl size=29345 sha256=67d260633bd09b0e824e5dcafa1863f8d27d981c3594c2ad702b5efc20153c61\n  Stored in directory: /Users/vitvly/Library/Caches/pip/wheels/f9/95/5b/d45847ffa40c431a57bbd40628576944de186e3544cc6edca2\nSuccessfully built sparseqr\nInstalling collected packages: sparseqr\nSuccessfully installed sparseqr-1.4.1\n\n[notice] A new release of pip is available: 25.0 -&gt; 25.0.1\n[notice] To update, run: pip install --upgrade pip\n\n\n\n# Get images for plots (images are also included in the pset folder)\n! wget http://6.869.csail.mit.edu/fa19/psets19/pset1/img1.png\n! wget http://6.869.csail.mit.edu/fa19/psets19/pset1/img2.png\n! wget http://6.869.csail.mit.edu/fa19/psets19/pset1/img3.png\n! wget http://6.869.csail.mit.edu/fa19/psets19/pset1/img4.png\n! wget http://6.869.csail.mit.edu/sp23/pset1/pset_1_reference.png"
  },
  {
    "objectID": "nb/cv24_pset1/pset1_2024.html#import-the-necessary-dependencies",
    "href": "nb/cv24_pset1/pset1_2024.html#import-the-necessary-dependencies",
    "title": "PSET 1",
    "section": "",
    "text": "# Ignore the pip dependency error\n\nimport cv2\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import convolve2d as conv2d\nimport scipy.sparse as sps\nfrom PIL import Image\n\n# # Package for fast equation solving\nfrom sys import platform\nprint(platform)\nif platform == \"linux\" or platform == \"linux2\":\n    ! apt-get install libsuitesparse-dev\nelif platform == \"darwin\":\n    ! brew install suite-sparse\n\n! pip3 install sparseqr\nimport sparseqr\n\ndarwin\n==&gt; Auto-updating Homebrew...\nAdjust how often this is run with HOMEBREW_AUTO_UPDATE_SECS or disable with\nHOMEBREW_NO_AUTO_UPDATE. Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n==&gt; Auto-updated Homebrew!\nUpdated 3 taps (oven-sh/bun, homebrew/core and homebrew/cask).\n==&gt; New Formulae\nab-av1                     gnome-online-accounts      node-red\nfalcosecurity-libs         keyutils                   oven-sh/bun/bun@1.2.5\nfpm                        krep                       sentry-native\ngcr                        libgoa                     yalantinglibs\ngit-graph                  libgudev\n==&gt; New Casks\nkate                                     mouseless@preview\n\nYou have 14 outdated formulae installed.\n\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/suite-sparse/manifests/7.10.1\n######################################################################### 100.0%\n==&gt; Fetching dependencies for suite-sparse: isl, libmpc, gcc and metis\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/isl/manifests/0.27\n######################################################################### 100.0%\n==&gt; Fetching isl\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/isl/blobs/sha256:de143fddb0e20b\n######################################################################### 100.0%\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/libmpc/manifests/1.3.1\n######################################################################### 100.0%\n==&gt; Fetching libmpc\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/libmpc/blobs/sha256:5c8cdc4d460\n######################################################################### 100.0%\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/gcc/manifests/14.2.0_1\n######################################################################### 100.0%\n==&gt; Fetching gcc\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/gcc/blobs/sha256:96d8bf02f621cf\n######################################################################### 100.0%\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/metis/manifests/5.1.0\n######################################################################### 100.0%\n==&gt; Fetching metis\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/metis/blobs/sha256:60ef633238eb\n######################################################################### 100.0%\n==&gt; Fetching suite-sparse\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/suite-sparse/blobs/sha256:edc57\n######################################################################### 100.0%\n==&gt; Installing dependencies for suite-sparse: isl, libmpc, gcc and metis\n==&gt; Installing suite-sparse dependency: isl\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/isl/manifests/0.27\nAlready downloaded: /Users/vitvly/Library/Caches/Homebrew/downloads/40b1c5526f95db33208143fa79887179e758121659d8877597f553e6e6188879--isl-0.27.bottle_manifest.json\n==&gt; Pouring isl--0.27.arm64_sequoia.bottle.tar.gz\nüç∫  /opt/homebrew/Cellar/isl/0.27: 74 files, 7.6MB\n==&gt; Installing suite-sparse dependency: libmpc\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/libmpc/manifests/1.3.1\nAlready downloaded: /Users/vitvly/Library/Caches/Homebrew/downloads/fdfa98e0f8bb3ce075cb32776ac2345aa2f89252706c162aecfc841085fa76be--libmpc-1.3.1.bottle_manifest.json\n==&gt; Pouring libmpc--1.3.1.arm64_sequoia.bottle.tar.gz\nüç∫  /opt/homebrew/Cellar/libmpc/1.3.1: 13 files, 492.3KB\n==&gt; Installing suite-sparse dependency: gcc\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/gcc/manifests/14.2.0_1\nAlready downloaded: /Users/vitvly/Library/Caches/Homebrew/downloads/cece94dbe926093c968a24f66b9a0172afe5cc2ef22253029bc591147237045b--gcc-14.2.0_1.bottle_manifest.json\n==&gt; Pouring gcc--14.2.0_1.arm64_sequoia.bottle.tar.gz\nüç∫  /opt/homebrew/Cellar/gcc/14.2.0_1: 1,914 files, 459.8MB\n==&gt; Installing suite-sparse dependency: metis\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/metis/manifests/5.1.0\nAlready downloaded: /Users/vitvly/Library/Caches/Homebrew/downloads/f3ed9b4299ad5a23ac4e265012f58bed7574a85759c297dbea217aeb89707128--metis-5.1.0.bottle_manifest.json\n==&gt; Pouring metis--5.1.0.arm64_sequoia.bottle.tar.gz\nüç∫  /opt/homebrew/Cellar/metis/5.1.0: 19 files, 12.1MB\n==&gt; Installing suite-sparse\n==&gt; Pouring suite-sparse--7.10.1.arm64_sequoia.bottle.tar.gz\nüç∫  /opt/homebrew/Cellar/suite-sparse/7.10.1: 217 files, 49.3MB\n==&gt; Running `brew cleanup suite-sparse`...\nDisable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\nHide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\nCollecting sparseqr\n  Downloading sparseqr-1.4.1.tar.gz (18 kB)\n  Installing build dependencies ... \u001b[?done\n  Getting requirements to build wheel ... done\n  Installing backend dependencies ... done\n  Preparing metadata (pyproject.toml) ... done\nRequirement already satisfied: numpy&gt;1.2 in /Users/vitvly/c/lnu/md/compvision/venv/lib/python3.12/site-packages (from sparseqr) (2.1.1)\nRequirement already satisfied: scipy&gt;=1.0 in /Users/vitvly/c/lnu/md/compvision/venv/lib/python3.12/site-packages (from sparseqr) (1.15.1)\nRequirement already satisfied: cffi&gt;=1.0 in /Users/vitvly/c/lnu/md/compvision/venv/lib/python3.12/site-packages (from sparseqr) (1.17.1)\nRequirement already satisfied: setuptools&gt;35 in /Users/vitvly/c/lnu/md/compvision/venv/lib/python3.12/site-packages (from sparseqr) (75.8.0)\nRequirement already satisfied: pycparser in /Users/vitvly/c/lnu/md/compvision/venv/lib/python3.12/site-packages (from cffi&gt;=1.0-&gt;sparseqr) (2.22)\nBuilding wheels for collected packages: sparseqr\n  Building wheel for sparseqr (pyproject.toml) ... done\n  Created wheel for sparseqr: filename=sparseqr-1.4.1-cp312-cp312-macosx_15_0_arm64.whl size=29345 sha256=67d260633bd09b0e824e5dcafa1863f8d27d981c3594c2ad702b5efc20153c61\n  Stored in directory: /Users/vitvly/Library/Caches/pip/wheels/f9/95/5b/d45847ffa40c431a57bbd40628576944de186e3544cc6edca2\nSuccessfully built sparseqr\nInstalling collected packages: sparseqr\nSuccessfully installed sparseqr-1.4.1\n\n[notice] A new release of pip is available: 25.0 -&gt; 25.0.1\n[notice] To update, run: pip install --upgrade pip\n\n\n\n# Get images for plots (images are also included in the pset folder)\n! wget http://6.869.csail.mit.edu/fa19/psets19/pset1/img1.png\n! wget http://6.869.csail.mit.edu/fa19/psets19/pset1/img2.png\n! wget http://6.869.csail.mit.edu/fa19/psets19/pset1/img3.png\n! wget http://6.869.csail.mit.edu/fa19/psets19/pset1/img4.png\n! wget http://6.869.csail.mit.edu/sp23/pset1/pset_1_reference.png"
  },
  {
    "objectID": "nb/cv24_pset1/pset1_2024.html#define-the-sparse-matrix",
    "href": "nb/cv24_pset1/pset1_2024.html#define-the-sparse-matrix",
    "title": "PSET 1",
    "section": "Define the Sparse Matrix",
    "text": "Define the Sparse Matrix\n\ndef sparseMatrix(i, j, Aij, imsize):\n    \"\"\" Build a sparse matrix containing 2D linear neighborhood operators\n    Input:\n        Aij = [ni, nj, nc] nc: number of neighborhoods with contraints\n        i: row index\n        j: column index\n        imsize: [nrows ncols]\n    Returns:\n        A: a sparse matrix. Each row contains one 2D linear operator\n    \"\"\"\n    ni, nj, nc = Aij.shape\n    nij = ni*nj\n\n    a = np.zeros((nc*nij))\n    m = np.zeros((nc*nij))\n    n = np.zeros((nc*nij))\n    grid_range = np.arange(-(ni-1)/2, 1+(ni-1)/2)\n    jj, ii = np.meshgrid(grid_range, grid_range)\n    ii = ii.reshape(-1,order='F')\n    jj = jj.reshape(-1,order='F')\n\n\n    k = 0\n    for c in range(nc):\n        # Get matrix index\n        x = (i[c]+ii) + (j[c]+jj)*nrows\n        a[k:k+nij] = Aij[:,:,c].reshape(-1,order='F')\n        m[k:k+nij] = c\n        n[k:k+nij] = x\n\n        k += nij\n\n    m = m.astype(np.int32)\n    n = n.astype(np.int32)\n    A = sps.csr_matrix((a, (m,  n)))\n\n    return A"
  },
  {
    "objectID": "nb/cv24_pset1/pset1_2024.html#define-world-parameters-and-plot-the-edges",
    "href": "nb/cv24_pset1/pset1_2024.html#define-world-parameters-and-plot-the-edges",
    "title": "PSET 1",
    "section": "Define world parameters and plot the edges",
    "text": "Define world parameters and plot the edges\n\n# World parameters\ntheta = 35*math.pi/180;\n\nimg = cv2.imread('img1.png')\nimg = img[:, :, ::-1].astype(np.float32)\n\nnrows, ncols, colors = img.shape\nground = (np.min(img, axis=2) &gt; 110).astype(np.float32)\nprint('ground', ground.shape, ground)\nforeground = (ground == 0).astype(np.float32)\n\nm = np.mean(img, 2)\nkern = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=np.float32)\ndmdx = conv2d(m, kern, 'same')\ndmdy = conv2d(m, kern.transpose(), 'same')\n\nmag = np.sqrt(dmdx**2 + dmdy**2)\nmag[0, :] = 0\nmag[-1, :] = 0\nmag[:, 0] = 0\nmag[:, -1] = 0\n\nedges = mag &gt;= 30\nedges = edges * foreground\n\n## Occlusion and contact edges\n\n####################################################################\n### COMPLETE THE CODE BELOW (TODOs) AND COPY IT INTO YOUR REPORT ###\n####################################################################\n\npi = math.pi\nedge_orientation = ## TODO\nvertical_edges = ## TODO\nhorizontal_edges = ## TODO\n\n####################################################################\n###################### STOP COPYING HERE ###########################\n####################################################################\n\nkern = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=np.float32)\nhorizontal_ground_to_foreground_edges = (conv2d(ground, kern, 'same'))&gt;0;\nhorizontal_foreground_to_ground_edges = (conv2d(foreground, kern, 'same'))&gt;0;\nvertical_ground_to_foreground_edges = vertical_edges*np.abs(conv2d(ground, kern.transpose(), 'same'))&gt;0\n\n\nocclusion_edges = edges*(vertical_ground_to_foreground_edges + horizontal_ground_to_foreground_edges)\ncontact_edges   = horizontal_edges*(horizontal_foreground_to_ground_edges);\n\n\nE = np.concatenate([vertical_edges[:,:,None],\n                    horizontal_edges[:,:,None],\n                    np.zeros(occlusion_edges.shape)[:,:,None]], 2)\n\n\n# Plot\nplt.figure()\nplt.subplot(2,2,1)\nplt.imshow(img.astype(np.uint8))\nplt.axis('off')\nplt.title('Input image')\nplt.subplot(2,2,2)\nplt.imshow(edges == 0, cmap='gray')\nplt.axis('off')\nplt.title('Edges')\n\n# Normals\nK = 3\ney, ex = np.where(edges[::K, ::K])\nex *= K\ney *= K\nplt.figure()\nplt.subplot(2,2,3)\nplt.imshow(np.max(mag)-mag, cmap='gray')\ndxe = dmdx[::K, ::K][edges[::K, ::K] &gt; 0]\ndye = dmdy[::K, ::K][edges[::K, ::K] &gt; 0]\nn = np.sqrt(dxe**2 + dye**2)\ndxe = dxe/n\ndye = dye/n\nplt.quiver(ex, ey, dxe, -dye, color='r')\nplt.axis('off')\nplt.title('Normals')\n\n\nplt.subplot(2,2,4)\nplt.imshow(np.max(mag)-mag, cmap='gray')\n# Recreate the normals plot using sin and cos\n# Note: -dye_mod  used in plot because 0 is upper right corner here\ndxe_mod = nx = np.cos(edge_orientation[::K, ::K][edges[::K, ::K] &gt; 0])\ndye_mod = ny = np.sin(edge_orientation[::K, ::K][edges[::K, ::K] &gt; 0])\nplt.quiver(ex, ey, dxe_mod, -dye_mod, color='r') # at ex, ey location, plot dx, -dy\nplt.axis('off')\nplt.title('Recreated Normals')\nplt.show()\n\n\n# Edges and boundaries\nplt.figure()\nplt.subplot(2,2,1)\nplt.imshow(img.astype(np.uint8))\nplt.axis('off')\nplt.title('Input image')\n\n\nplt.subplot(2,2,2)\nplt.imshow(E+(edges == 0)[:, :, None])\nplt.axis('off')\nplt.title('Edges')\n\n\nplt.subplot(2,2,3)\nplt.imshow(1-(occlusion_edges&gt;0), cmap='gray')\nplt.axis('off')\nplt.title('Occlusion boundaries')\n\nplt.subplot(2,2,4)\nplt.imshow(1-contact_edges, cmap='gray')\nplt.axis('off')\nplt.title('Contact boundaries');"
  },
  {
    "objectID": "nb/cv24_pset1/pset1_2024.html#populate-edge-variables",
    "href": "nb/cv24_pset1/pset1_2024.html#populate-edge-variables",
    "title": "PSET 1",
    "section": "Populate edge variables",
    "text": "Populate edge variables\n\nNconstraints = nrows*ncols*20\nAij = np.zeros((3, 3, Nconstraints))\nb = np.zeros((Nconstraints, 1))\n\n#Indices and counters\nii = np.zeros((Nconstraints, 1))\njj = np.zeros((Nconstraints, 1))\nglobal c\nc = 0\n\n# These will always be updated with the current indices\ndef update_indices():\n  global c\n  ii[c] = i\n  jj[c] = j\n  c += 1\n\n# Create linear contraints\nfor i in range(1, nrows-1):\n  for j in range(1, ncols-1):\n    # Y = 0\n    if ground[i,j]:\n      Aij[:,:,c] = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]])\n      b[c]       = 0\n      update_indices()\n    else:\n      # Check if current neighborhood touches an edge\n      edgesum = np.sum(edges[i-1:i+2,j-1:j+2])\n      # Check if current neighborhood touches ground pixels\n      groundsum = np.sum(ground[i-1:i+2,j-1:j+2])\n      # Check if current neighborhood touches vertical pixels\n      verticalsum = np.sum(vertical_edges[i-1:i+2,j-1:j+2])\n      # Check if current neighborhood touches horizontal pixels\n      horizontalsum = np.sum(horizontal_edges[i-1:i+2,j-1:j+2])\n\n      ####################################################################\n      ### COMPLETE THE CODE BELOW (TODOs) AND COPY IT INTO YOUR REPORT ###\n      ####################################################################\n\n      # TODO: edge orientation (average of edge pixels in current neighborhood)\n      # Populate Aij, ii, jj, b, and c using theta, edge_orientation, and\n      # the constraint/transform matrices you derived in the written segment\n\n      # Contact edge: Y = 0\n      # Requires: a transform matrix\n      # TODO: Complete the code\n      if contact_edges[i, j]:\n        Aij[:,:,c] =\n        b[c]       =\n        update_indices()\n\n      # Vertical edge: dY/dy = 1/cos(theta)\n      # Requires: a transform matrix, theta\n      # The 1/8 is for normalization\n      # TODO: Complete the code\n      if verticalsum &gt; 0 and groundsum == 0:\n        Aij[:,:,c] =\n        b[c]       =\n        update_indices()\n\n      # Horizontal edge: dY/dt = 0\n      # Note: You'll have to express t using other variables\n      # Requires: a transform matrix, i, j, edge_orientation\n      # TODO: Complete the code\n      if horizontalsum &gt; 0 and groundsum == 0 and verticalsum == 0:\n        Aij[:,:,c] =\n        b[c]       =\n        update_indices()\n\n      # Second derivative = 0 (weighted by 0.1 to reduce constraint strength)\n      # Requires: multiple transform matrices\n      # TODO: Complete the code\n      if groundsum == 0:\n        Aij[:,:,c] =\n        b[c]       =\n        update_indices()\n\n        Aij[:,:,c] =\n        b[c]       =\n        update_indices()\n\n        Aij[:,:,c] =\n        b[c]       =\n        update_indices()\n\n      ####################################################################\n      ###################### STOP COPYING HERE ###########################\n      ####################################################################\n\n# Solve for constraints\nii = ii[:c]\njj = jj[:c]\nAij = Aij[:,:,:c]\nb = b[:c]\nA = sparseMatrix(ii, jj, Aij, nrows)\nY = sparseqr.solve( A, b)\n\n# Transform vector into image\nY = np.reshape(Y, [nrows, ncols], order='F')\n\n# Recover 3D world coordinates\nx, y = np.meshgrid(np.arange(ncols), np.arange(nrows))\nx = x.astype(np.float32)\ny = y.astype(np.float32)\nx -= nrows/2\ny -= ncols/2\n\n# Final coordinates\nX = x\nZ = Y*np.cos(theta)/np.sin(theta) - y/np.sin(theta)\nY = -Y\nY = np.maximum(Y, 0);\n\nE = occlusion_edges.astype(np.float32);\nE[E &gt; 0] = np.nan;\nZ = Z+E; #  remove occluded edges\n\n# Visualize solution\nplt.figure()\nplt.subplot(2,2,1)\nplt.imshow(img[1:-1, 1:-1].astype(np.uint8))\nplt.axis('off')\nplt.title('Edges')\n\nplt.subplot(2,2,2)\nplt.imshow(Z[1:-1, 1:-1], cmap='gray')\nplt.axis('off')\nplt.title('Z')\n\n\nplt.subplot(2,2,3)\nplt.imshow(Y[1:-1, 1:-1], cmap='gray', vmin=0, vmax=175)\nplt.axis('off')\nplt.title('Y')\n\nplt.subplot(2,2,4)\nplt.imshow(X[1:-1, 1:-1], cmap='gray')\nplt.axis('off')\nplt.title('X')\n\n# 3D projection\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.set_zlim(0, 175)\n\n# TODO for Problem 5\n# Rerun the script with at least two more of the provided images and for each\n# image try at least two different view angles.\n# Include the generated plots in your report (under Problem 5).\n# Note that we expect results to be quite brittle -- in answering Problem 6,\n# think about the strong assumptions that this approach makes.\n# We'll see more robust methods for similar problems later in the course\n\n# Specify here the angle you want to see\nax.view_init(20, -120)\nax.plot_surface(X,Z,Y, facecolors=img/255., shade=False)\n\n\nReference solution\nYours should render on a white background (note that the outline on Z is transparency) but be identical otherwise\nUploading the reference will be considered an honor code violation\n\nplt.imshow(cv2.cvtColor(cv2.imread('pset_1_reference.png'), cv2.COLOR_BGR2RGB))\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Computer vision course"
  },
  {
    "objectID": "lab2.html",
    "href": "lab2.html",
    "title": "Lab 2",
    "section": "",
    "text": "Complete the following items in the course:\n\nImage Annotation.\nImage Enhancement."
  },
  {
    "objectID": "lec3.html#introduction",
    "href": "lec3.html#introduction",
    "title": "Filtering",
    "section": "Introduction",
    "text": "Introduction\n\n\n\nImportant\n\n\nHuman visual system is complex.\n\n\n\n\n\n\nTip\n\n\nWe have a fairly good idea of what happens at the initial stages of visual processing.\n\n\n\n\n\n\nNote\n\n\nWe will describe some mathematically simple processing that will help us to parse an image into useful tokens, or low-level features that will be useful later to construct visual interpretations."
  },
  {
    "objectID": "lec3.html#introduction-1",
    "href": "lec3.html#introduction-1",
    "title": "Filtering",
    "section": "Introduction",
    "text": "Introduction\n\n\n\nProcessing features\n\n\n\nenhance image structures of use for subsequent interpretation\nremove variability within the image that makes more difficult comparisons with previously learned visual signals.\n\n\n\n\n\n\n\nLinear filters\n\n\nThe simplest mathematical processing we can think of.\n\nnicely model some aspects of the processing carried our by early visual areas such as the retina, the lateral geniculate nucleus (LGN) and primary visual cortex (V1) @Hubel62."
  },
  {
    "objectID": "lec3.html#signals-and-images",
    "href": "lec3.html#signals-and-images",
    "title": "Filtering",
    "section": "Signals and Images",
    "text": "Signals and Images\n\n\n\nSignal\n\n\nA signal is a measurement of some physical quantity (light, sound, height, temperature, etc.) as a function of another independent quantity (time, space, wavelength, etc.).\n\n\n\n\n\n\nSystem\n\n\nA system is a process/function that transforms a signal into another."
  },
  {
    "objectID": "lec3.html#signals-and-images-1",
    "href": "lec3.html#signals-and-images-1",
    "title": "Filtering",
    "section": "Signals and Images",
    "text": "Signals and Images\nContinuous and Discrete Signals\n\nmost of the signals that exist in nature are infinite continuous signals\nhowever, when we introduce them into a computer they are sampled and transformed into a finite sequence of numbers, also called a discrete signal.\n\n\n\n\n\n\n\nNote\n\n\nSampling is the process of transforming a continuous signal into a discrete one."
  },
  {
    "objectID": "lec3.html#signals-and-images-2",
    "href": "lec3.html#signals-and-images-2",
    "title": "Filtering",
    "section": "Signals and Images",
    "text": "Signals and Images\n\n\n\nSignal representation\n\n\nIf we consider the light that reaches one camera photo sensor, we could write it as a one dimensional function of time, \\(\\ell(t)\\), where \\(\\ell(t)\\) denotes the incident \\(\\ell\\)ight at time \\(t\\), and \\(t\\) is a continuous variable that can take on any real value.\nThe signal \\(\\ell(t)\\) is then sampled in time (as is done in a video) by the camera where the values are only captured at discrete times (e.g., 30 times per second). In that case, the signal \\(\\ell\\) will be defined only on discrete time instants and we will write the sequence of measured values as: \\(\\ell\\left[ n \\right]\\), where \\(n\\) can only take on integer values."
  },
  {
    "objectID": "lec3.html#signals-and-images-3",
    "href": "lec3.html#signals-and-images-3",
    "title": "Filtering",
    "section": "Signals and Images",
    "text": "Signals and Images\n\n\n\nDiscrete-continuous Relationship\n\n\nThe relationship between the discrete and the continuous signals is given by the sampling equation: \\[\n\\ell\\left[n\\right] = \\ell\\left(n~\\Delta T \\right)\n\\] where \\(\\Delta T\\)is the sampling period.\nFor instance, \\(\\Delta T = 1/30\\)¬†s in the case of sampling the signal 30 times per second."
  },
  {
    "objectID": "lec3.html#signals-and-images-4",
    "href": "lec3.html#signals-and-images-4",
    "title": "Filtering",
    "section": "Signals and Images",
    "text": "Signals and Images\n\n\n\n\n\n\nNote\n\n\nAs is common in signal processing, we will use parenthesis to indicate continuous variables and brackets to denote discrete variables.\nFor instance, if we sample the signal shown in Figure¬†1 (a) once every second (\\(\\Delta T = 1\\)¬†s) we get the discrete signal shown in Figure¬†1 (b).\n\n\n\n\n\nFigure¬†1: Fig (a) A continuous signal, and (b) a discrete signal obtained by sampling the continuous signal at the times \\(t=n\\)."
  },
  {
    "objectID": "lec3.html#signals-and-images-5",
    "href": "lec3.html#signals-and-images-5",
    "title": "Filtering",
    "section": "Signals and Images",
    "text": "Signals and Images\n\n\n\nSignal values\n\n\nThe signal in Figure¬†1 (b) is a function that takes on the values \\(\\ell\\left[0\\right] = 3\\), \\(\\ell\\left[1\\right] = 2\\), \\(\\ell\\left[2\\right] = 1\\) and \\(\\ell\\left[3\\right] = 4\\) and all other values are zero, \\(\\ell\\left[n\\right] = 0\\). In most of the book we will work with discrete signals.\n\n\n\n\n\n\nVector notation\n\n\nIn many cases it will be convenient to write discrete signals as vectors. Using vector notation we will write the previous signal, in the interval \\(n \\in \\left[0,6 \\right],\\) as a column vector \\(\\boldsymbol\\ell= \\left[3, 2, 1, 4, 0, 0, 0\\right]^T\\), where \\(T\\) denotes transpose."
  },
  {
    "objectID": "lec3.html#signals-and-images-6",
    "href": "lec3.html#signals-and-images-6",
    "title": "Filtering",
    "section": "Signals and Images",
    "text": "Signals and Images\n\n\n\nImage\n\n\nAn image is a two dimensional array of values: \\[\n\\boldsymbol\\ell\\in \\mathbb{R}^{M \\times N},\n\\] where \\(N\\) is the image width and \\(M\\) is the image height."
  },
  {
    "objectID": "lec3.html#signals-and-images-7",
    "href": "lec3.html#signals-and-images-7",
    "title": "Filtering",
    "section": "Signals and Images",
    "text": "Signals and Images\n\n\n\nExample\n\n\nA grayscale image is then just an array of numbers such as the following (in this example, intensity values are scaled between 0 and 256):\n\\[\\begin{array}{cc}\n\\boldsymbol\\ell= &\n\\left[\n\\begin{smallmatrix} 160 & 175 & 171 & 168 & 168 & 172 & 164 & 158 & 167 & 173 & 167 & 163 & 162 & 164 & 160 & 159 & 163 & 162\\\\ 149 & 164 & 172 & 175 & 178 & 179 & 176 & 118 & 97 & 168 & 175 & 171 & 169 & 175 & 176 & 177 & 165 & 152\\\\ 161 & 166 & 182 & 171 & 170 & 177 & 175 & 116 & 109 & 169 & 177 & 173 & 168 & 175 & 175 & 159 & 153 & 123\\\\ 171 & 174 & 177 & 175 & 167 & 161 & 157 & 138 & 103 & 112 & 157 & 164 & 159 & 160 & 165 & 169 & 148 & 144\\\\ 163 & 163 & 162 & 165 & 167 & 164 & 178 & 167 & 77 & 55 & 134 & 170 & 167 & 162 & 164 & 175 & 168 & 160\\\\ 173 & 164 & 158 & 165 & 180 & 180 & 150 & 89 & 61 & 34 & 137 & 186 & 186 & 182 & 175 & 165 & 160 & 164\\\\ 152 & 155 & 146 & 147 & 169 & 180 & 163 & 51 & 24 & 32 & 119 & 163 & 175 & 182 & 181 & 162 & 148 & 153\\\\ 134 & 135 & 147 & 149 & 150 & 147 & 148 & 62 & 36 & 46 & 114 & 157 & 163 & 167 & 169 & 163 & 146 & 147\\\\ 135 & 132 & 131 & 125 & 115 & 129 & 132 & 74 & 54 & 41 & 104 & 156 & 152 & 156 & 164 & 156 & 141 & 144\\\\ 151 & 155 & 151 & 145 & 144 & 149 & 143 & 71 & 31 & 29 & 129 & 164 & 157 & 155 & 159 & 158 & 156 & 148\\\\ 172 & 174 & 178 & 177 & 177 & 181 & 174 & 54 & 21 & 29 & 136 & 190 & 180 & 179 & 176 & 184 & 187 & 182\\\\ 177 & 178 & 176 & 173 & 174 & 180 & 150 & 27 & 101 & 94 & 74 & 189 & 188 & 186 & 183 & 186 & 188 & 187\\\\ 160 & 160 & 163 & 163 & 161 & 167 & 100 & 45 & 169 & 166 & 59 & 136 & 184 & 176 & 175 & 177 & 185 & 186\\\\ 147 & 150 & 153 & 155 & 160 & 155 & 56 & 111 & 182 & 180 & 104 & 84 & 168 & 172 & 171 & 164 & 168 & 167\\\\ 184 & 182 & 178 & 175 & 179 & 133 & 86 & 191 & 201 & 204 & 191 & 79 & 172 & 220 & 217 & 205 & 209 & 200\\\\ 184 & 187 & 192 & 182 & 124 & 32 & 109 & 168 & 171 & 167 & 163 & 51 & 105 & 203 & 209 & 203 & 210 & 205\\\\ 191 & 198 & 203 & 197 & 175 & 149 & 169 & 189 & 190 & 173 & 160 & 145 & 156 & 202 & 199 & 201 & 205 & 202\\\\ 153 & 149 & 153 & 155 & 173 & 182 & 179 & 177 & 182 & 177 & 182 & 185 & 179 & 177 & 167 & 176 & 182 & 180 \\end{smallmatrix}\\right]\n\\end{array}\\]"
  },
  {
    "objectID": "lec3.html#signals-and-images-8",
    "href": "lec3.html#signals-and-images-8",
    "title": "Filtering",
    "section": "Signals and Images",
    "text": "Signals and Images\n\nGrayscale image showing a person walking in the street. This tiny image has only \\(18\\times18\\) pixels."
  },
  {
    "objectID": "lec3.html#signals-and-images-9",
    "href": "lec3.html#signals-and-images-9",
    "title": "Filtering",
    "section": "Signals and Images",
    "text": "Signals and Images\n\n\n\nPixel location\n\n\nWhen we want to make explicit the location of a pixel we will write \\(\\ell\\left[n, m \\right]\\), where \\(n \\in \\left[0,N-1 \\right]\\) and \\(m \\in \\left[0,M-1 \\right]\\) are the indices for the horizontal and vertical dimensions, respectively.\n\neach value in the array indicates the intensity of the image in that location.\nfor color images we will have three channels, one for each color."
  },
  {
    "objectID": "lec3.html#signals-and-images-10",
    "href": "lec3.html#signals-and-images-10",
    "title": "Filtering",
    "section": "Signals and Images",
    "text": "Signals and Images\n\n\n\nApproximation\n\n\nWorking on the continuous domain simplifies the derivation of analytical solutions. In those cases we will write images as \\[\n\\ell(x,y)\n\\] and video sequences as \\[\n\\ell(x,y,t)\n\\]."
  },
  {
    "objectID": "lec3.html#signal-properties",
    "href": "lec3.html#signal-properties",
    "title": "Filtering",
    "section": "Signal Properties",
    "text": "Signal Properties\n\n\n\nSignal length\n\n\nInfinite length signals are signals that extend over the entire support \\(\\ell\\left[n \\right]\\) for \\(n \\in (-\\infty, \\infty)\\).\nFinite length signals have non-zero values on a compact time interval and they are zero outside, i.e.¬†\\(\\ell\\left[n \\right] = 0\\) for \\(n \\notin S\\) where \\(S\\) is a finite length interval.\n\n\n\n\n\n\nPeriodicity\n\n\nA signal \\(\\ell\\left[n \\right]\\) is periodic if there is a value \\(N\\) such that \\(\\ell\\left[n \\right] = \\ell\\left[n + k N\\right]\\) for all \\(n\\) and \\(k\\). A periodic signal is an infinite length signal."
  },
  {
    "objectID": "lec3.html#signal-properties-1",
    "href": "lec3.html#signal-properties-1",
    "title": "Filtering",
    "section": "Signal Properties",
    "text": "Signal Properties\n\n\n\nMean value\n\n\nMean value of a signal often called the DC value.\nIn the case of an image, the DC component is the average intensity of the image.\nThe DC value is computed as \\[\n\\mu = \\begin{cases}\n\\frac{1}{N} \\sum_{n=0}^{N-1} \\ell\\left[n\\right], \\, \\text{finite case} \\\\\n\\lim_{N \\xrightarrow{} \\infty} \\frac{1}{2N+1} \\sum_{n=-N}^{N} \\ell\\left[n\\right], \\, \\text{infinite case}\n\\end{cases}\n\\]\n\n\n\n\n\nDC means direct current and it comes from electrical engineering. Although most signals have nothing to do with currents, the term DC is still commonly used."
  },
  {
    "objectID": "lec3.html#signal-properties-2",
    "href": "lec3.html#signal-properties-2",
    "title": "Filtering",
    "section": "Signal Properties",
    "text": "Signal Properties\n\n\n\nEnergy\n\n\nThe energy of a signal is defined as the sum of squared magnitude values: \\[\nE = \\sum_{n = -\\infty}^{\\infty} \\left| \\ell\\left[n\\right] \\right| ^2\n\\]\n\n\n\n\n\n\n\n\n\nNote\n\n\nSignal are further classified as finite energy and infinite energy signals\n\nfinite length signals are finite energy\nperiodic signals are infinite energy signals when measuring the energy in the whole time axis."
  },
  {
    "objectID": "lec3.html#signal-properties-3",
    "href": "lec3.html#signal-properties-3",
    "title": "Filtering",
    "section": "Signal Properties",
    "text": "Signal Properties\n\n\n\nComparison\n\n\nIf we want to compare two signals, we can use the squared Euclidean distance (squared \\(L_2\\) norm) between them: \\[\nD^2 = \\frac{1}{N} \\sum_{n=0}^{N-1} \\left| \\ell_1 \\left[n\\right] - \\ell_2 \\left[n\\right] \\right| ^2\n\\]\n\n\n\n\n\n\nWarning\n\n\nHowever, the euclidean distance (\\(L_2\\)) is a poor metric when we are interested in comparing the content of the two images and the building better metrics is an important area of research. Sometimes, the metric is \\(L_2\\) but in a different representation space than pixel values."
  },
  {
    "objectID": "lec3.html#signal-properties-4",
    "href": "lec3.html#signal-properties-4",
    "title": "Filtering",
    "section": "Signal Properties",
    "text": "Signal Properties\n\n\n\nContinuous signal properties\n\n\nAll the equations are analogous by replacing the sums with integrals.\nFor instance, in the case of the energy of a continuous signal, we can write\n\\[\nE = \\int_{-\\infty}^{\\infty} \\left| \\ell\\left( t \\right) \\right|^2 dt,\n\\] which assumes that the integral is finite.\nMost natural signals will have infinite energy."
  },
  {
    "objectID": "lec3.html#systems-1",
    "href": "lec3.html#systems-1",
    "title": "Filtering",
    "section": "Systems",
    "text": "Systems\n\n\n\nSystems flow\n\n\n\ninput: \\(\\boldsymbol\\ell_{\\texttt{in}}\\)\ncomputation: \\(f\\)\noutput: another signal \\(\\boldsymbol\\ell_{\\texttt{out}}= f(\\boldsymbol\\ell_{\\texttt{in}})\\).\n\n\n\n\n\n\nFigure¬†2: System processing one signal."
  },
  {
    "objectID": "lec3.html#systems-2",
    "href": "lec3.html#systems-2",
    "title": "Filtering",
    "section": "Systems",
    "text": "Systems\n\n\n\nWhat can this do?\n\n\nAll kinds of things:\n\ndetect edges in images\nrecognize objects\ndetect motion in sequences\napply aesthetic transformations to a picture.\n\n\n\n\n\n\nThe function \\(f\\) could be specified as a mathematical operation or as an algorithm. As a consequence, it is very difficult to find a simple way of characterizing what the function \\(f\\) does"
  },
  {
    "objectID": "lec3.html#linear-systems",
    "href": "lec3.html#linear-systems",
    "title": "Filtering",
    "section": "Linear Systems",
    "text": "Linear Systems\n\n\n\nLinear systems\n\n\nA function \\(f\\) is linear is it satisfies the following two properties:\n\\[\\begin{aligned}\n& 1. \\; f\\left( \\boldsymbol\\ell_1+\\boldsymbol\\ell_2 \\right) = f(\\boldsymbol\\ell_1)+ f(\\boldsymbol\\ell_2) \\\\ \\nonumber\n& 2. \\; f(a\\boldsymbol\\ell) = af(\\boldsymbol\\ell) ~~\\text{for any scalar} ~ a\n\\end{aligned}\\]"
  },
  {
    "objectID": "lec3.html#systems-3",
    "href": "lec3.html#systems-3",
    "title": "Filtering",
    "section": "Systems",
    "text": "Systems\n\n\n\nWhich operations are linear?\n\n\nWhich ones of the image transformations can be written as linear systems?\n\n\n\n\n\n\nFigure¬†3"
  },
  {
    "objectID": "lec3.html#systems-4",
    "href": "lec3.html#systems-4",
    "title": "Filtering",
    "section": "Systems",
    "text": "Systems\n\n\n\nLinear system: inputs/outputs\n\n\n\ninput: a 1D signal with length \\(N\\) that we will write as \\(\\ell_{\\texttt{in}}\\left[n \\right]\\),\noutput another 1D signal with length \\(M\\) that we will write as \\(\\ell_{\\texttt{out}}\\left[n \\right]\\).\n\nMost of the times we will work with input and output pairs with the same length \\(M=N\\).\n\n\n\n\n\n\nLinear system: definition\n\n\nA linear system, in its most general form, can be written as follows:\n\\[\\ell_{\\texttt{out}}\\left[n \\right] = \\sum_{k=0}^{N-1} h \\left[n,k\\right] \\ell_{\\texttt{in}}\\left[k \\right] ~~~ for ~~~ n \\in \\left[0, M-1 \\right]\n\\qquad(1)\\]"
  },
  {
    "objectID": "lec3.html#systems-5",
    "href": "lec3.html#systems-5",
    "title": "Filtering",
    "section": "Systems",
    "text": "Systems\n\n\n\nLinear system: properties\n\n\n\neach output value \\(\\ell_{\\texttt{out}}\\left[n \\right]\\) is a linear combination of values of the input signal \\(\\ell_{\\texttt{in}}\\left[n \\right]\\) with weights \\(h \\left[n,k\\right]\\)\nthe value \\(h \\left[n,k\\right]\\) is the weight applied to the input sample \\(\\ell_{\\texttt{in}}\\left[k \\right]\\) to compute the output sample \\(\\ell_{\\texttt{out}}\\left[n \\right]\\)."
  },
  {
    "objectID": "lec3.html#systems-6",
    "href": "lec3.html#systems-6",
    "title": "Filtering",
    "section": "Systems",
    "text": "Systems\n\n\n\nMatrix form\n\n\n\\[\\begin{bmatrix}\\ell_{\\texttt{out}}\\left[ 0 \\right] \\\\\n\\ell_{\\texttt{out}}\\left[ 1 \\right] \\\\\n\\vdots \\\\\n\\ell_{\\texttt{out}}\\left[ n \\right] \\\\\n\\vdots \\\\\n\\ell_{\\texttt{out}}\\left[ M-1 \\right]\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n  h\\left[0,0\\right] ~&~ h\\left[0,1\\right] ~&~ ... ~&~ h\\left[0,N-1\\right] \\\\\n  h\\left[1,0\\right] ~&~ h\\left[1,1\\right] ~&~ ... ~&~ h\\left[1,N-1\\right] \\\\\n  \\vdots ~&~  \\vdots ~&~  \\vdots ~&~  \\vdots \\\\\n  \\vdots ~&~  \\vdots ~&~  h \\left[n,k\\right] ~&~  \\vdots \\\\\n\\vdots ~&~  \\vdots ~&~  \\vdots ~&~  \\vdots \\\\\n  h\\left[M-1,0\\right] ~&~ h\\left[M-1,1\\right] ~&~ ... ~&~ h\\left[M-1,N-1\\right]\n\\end{bmatrix}\n~\n\\begin{bmatrix}\n  \\ell_{\\texttt{in}}\\left[0\\right] \\\\\n  \\ell_{\\texttt{in}}\\left[1\\right] \\\\\n  \\vdots \\\\\n  \\ell_{\\texttt{in}}\\left[k\\right] \\\\\n  \\vdots \\\\\n  \\ell_{\\texttt{in}}\\left[N-1\\right]\n\\end{bmatrix}\\]\n\n\n\n\n\n\nShort form\n\n\n\\[\n\\boldsymbol\\ell_{\\texttt{out}}=  \\mathbf{H} \\boldsymbol\\ell_{\\texttt{in}}.\n\\]\nThe matrix \\(\\mathbf{H}\\) has size \\(M \\times N\\)."
  },
  {
    "objectID": "lec3.html#systems-7",
    "href": "lec3.html#systems-7",
    "title": "Filtering",
    "section": "Systems",
    "text": "Systems\n\n\n\nLinear function as a neural network\n\n\nA linear function can also be drawn as a fully connected layer in a neural network, with weights \\(\\mathbf{W} = \\mathbf{H}\\), where the output unit \\(i\\), \\(\\ell_{\\texttt{out}}\\left[i \\right]\\), is a linear combination of the input signal \\(\\boldsymbol\\ell_{\\texttt{in}}\\) with weights given by the row vectors of \\(\\mathbf{H}\\). Graphically it looks like this:\n\n\n\n\n\n\nFigure¬†4: A linear function drawn as a fully connected layer in a neural network."
  },
  {
    "objectID": "lec3.html#systems-8",
    "href": "lec3.html#systems-8",
    "title": "Filtering",
    "section": "Systems",
    "text": "Systems\n\n\n\n2D case\n\n\nIn two dimensions the equations are analogous. Each pixel of the output image, \\(\\ell_{\\texttt{out}}\\left[n,m\\right]\\), is computed as a linear combination of pixels of the input image, \\(\\ell_{\\texttt{in}}\\left[n, m\\right]\\):\n\\[\n\\ell_{\\texttt{out}}\\left[n,m \\right] = \\sum_{k=0}^{M-1} \\sum_{l=0}^{N-1} h \\left[n,m,k,l \\right] \\ell_{\\texttt{in}}\\left[k,l \\right]\n\\qquad(2)\\]\nBy writing the images as column vectors, concatenating all the image columns into a long vector, we can also write the previous equation using matrices and vectors: \\(\\boldsymbol\\ell_{\\texttt{out}}=  \\mathbf{H} \\boldsymbol\\ell_{\\texttt{in}}\\)."
  },
  {
    "objectID": "lec3.html#linear-translation-invariant-systems",
    "href": "lec3.html#linear-translation-invariant-systems",
    "title": "Filtering",
    "section": "Linear Translation Invariant Systems",
    "text": "Linear Translation Invariant Systems\n\n\n\nLinear translation invariant (LTI) systems"
  },
  {
    "objectID": "lec3.html#linear-translation-invariant-systems-1",
    "href": "lec3.html#linear-translation-invariant-systems-1",
    "title": "Filtering",
    "section": "Linear Translation Invariant Systems",
    "text": "Linear Translation Invariant Systems\n\n\n\nMotivation\n\n\nWe don‚Äôt know where within the image we expect to find any given item, so we often want to process the image in a spatially invariant manner, the same processing algorithm at every pixel.\n\n\n\nA fundamental property of images is translation invariance‚Äìthe same image may appear at arbitrary spatial positions within the image. Source: Fredo Durand."
  },
  {
    "objectID": "lec3.html#linear-translation-invariant-systems-2",
    "href": "lec3.html#linear-translation-invariant-systems-2",
    "title": "Filtering",
    "section": "Linear Translation Invariant Systems",
    "text": "Linear Translation Invariant Systems\n\n\n\nExample\n\n\nAn example of a translation invariant system is a function that takes as input an image and computes at each location a local average value of the pixels around in a window of \\(5 \\times 5\\) pixels:\n\\[\\ell_{\\texttt{out}}\\left[n,m \\right] = \\frac{1}{25} \\sum_{k=-2}^{2} \\sum_{l=-2}^{2} \\ell_{\\texttt{in}}\\left[n+k,m+l \\right]\\]"
  },
  {
    "objectID": "lec3.html#linear-translation-invariant-systems-3",
    "href": "lec3.html#linear-translation-invariant-systems-3",
    "title": "Filtering",
    "section": "Linear Translation Invariant Systems",
    "text": "Linear Translation Invariant Systems\n\n\n\nDefinition\n\n\nA system is an LTI system if it is linear and when we translate the input signal by \\(n_0, m_0\\), then output is also translated by \\(n_0, m_0\\): \\[\n\\ell_{\\texttt{out}}\\left[ n - n_0, m-m_0 \\right] = f \\left( \\ell_{\\texttt{in}}\\left[ n-n_0, m-m_0 \\right] \\right)\n\\] for any \\(n_0,m_0\\). This property is called equivariant with respect to translation."
  },
  {
    "objectID": "lec3.html#convolution",
    "href": "lec3.html#convolution",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nDefinition\n\n\nThe convolution, denoted \\(\\circ\\), between a signal \\(\\ell_{\\texttt{in}}\\left[n \\right]\\) and the convolutional kernel \\(h\\left[n \\right]\\) is defined as follows: \\[\n\\ell_{\\texttt{out}}\\left[n\\right] = h \\left[n\\right] \\circ \\ell_{\\texttt{in}}\\left[n\\right] =  \\sum_{k=-\\infty}^{\\infty} h \\left[n-k \\right] \\ell_{\\texttt{in}}\\left[k \\right]\n\\qquad(3)\\]\nIf the signal \\(\\ell_{\\texttt{in}}\\left[n \\right]\\) has a finite length, \\(N\\), then the sum is only done in the interval \\(k \\in \\left[0,N-1\\right]\\).\n\n\n\n\n\n\nWeight\n\n\nThe convolution computes the output values as a linear weighted sum. The weight between the input sample \\(\\ell_{\\texttt{in}}\\left[k \\right]\\) and the output sample \\(\\ell_{\\texttt{out}}\\left[n\\right]\\) is a function \\(h\\left[n, k \\right]\\) that depends only on their relative position \\(n-k\\). Therefore, \\(h\\left[n, k \\right]=h\\left[n-k \\right]\\)."
  },
  {
    "objectID": "lec3.html#convolution-1",
    "href": "lec3.html#convolution-1",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nCorrelation\n\n\nThe convolution is related to the correlation operator. In the correlation, the weights are defined as \\(h\\left[n, k \\right]=h\\left[k-n \\right]\\). The convolution and the correlation use the same kernel but mirrored around the origin. The correlation is also translation invariant as we will discuss at the end of the chapter."
  },
  {
    "objectID": "lec3.html#convolution-2",
    "href": "lec3.html#convolution-2",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nCommutativity\n\n\nOne important property of the convolution is that it is commutative: \\[\nh\\left[n\\right] \\circ \\ell\\left[n\\right] = \\ell\\left[n\\right] \\circ h\\left[n\\right].\n\\]\nThis property is easy to prove by changing the variables, \\(k = n - k'\\), so that:\n\\[h \\left[n\\right] \\circ \\ell_{\\texttt{in}}\\left[n\\right] =\\sum_{k=-\\infty}^{\\infty} h \\left[n-k \\right] \\ell_{\\texttt{in}}\\left[k \\right] = \\sum_{k'=-\\infty}^{\\infty} h \\left[k' \\right] \\ell_{\\texttt{in}}\\left[n-k' \\right]  =\n\\ell_{\\texttt{in}}\\left[n\\right] \\circ h \\left[n\\right]\\]"
  },
  {
    "objectID": "lec3.html#convolution-3",
    "href": "lec3.html#convolution-3",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nTextual description\n\n\n\nTake the kernel \\(h\\left[k\\right]\\) and mirror it \\(h\\left[-k \\right]\\)\nShift the mirrored kernel so that the origin is at location \\(n\\)\nMultiply the input values around location \\(n\\) by the mirrored kernel and sum the result.\nStore the result in the output vector \\(\\ell_{\\texttt{out}}\\left[n\\right]\\).\n\nFor instance, if the convolutional kernel has three non-zero values: \\[h\\left[-1 \\right] =1, ~ h\\left[0 \\right] = 2, ~ h\\left[1 \\right] = 3,\n\\] then the output value at location \\(n\\) is \\[\\ell_{\\texttt{out}}\\left[n\\right] = 3 \\ell_{\\texttt{in}}\\left[n-1\\right]+2 \\ell_{\\texttt{in}}\\left[n\\right] + 1 \\ell_{\\texttt{in}}\\left[n+1\\right].\\]"
  },
  {
    "objectID": "lec3.html#convolution-4",
    "href": "lec3.html#convolution-4",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nMatrix form\n\n\nIf the convolutional kernel has only three non-zero values, then:\n\\[\\begin{bmatrix}\n\\ell_{\\texttt{out}}\\left[ 0 \\right] \\\\\n\\ell_{\\texttt{out}}\\left[ 1 \\right] \\\\\n\\ell_{\\texttt{out}}\\left[ 2 \\right] \\\\\n\\vdots \\\\\n\\ell_{\\texttt{out}}\\left[ N-1 \\right]\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n  h\\left[0\\right] ~&~ h\\left[-1\\right] ~&~ 0 ~&~ ... ~&~ 0 \\\\\n  h\\left[1\\right] ~&~ h\\left[0\\right] ~&~ h\\left[-1\\right] ~&~... ~&~ 0 \\\\\n  0 ~&~ h\\left[1\\right] ~&~ h\\left[0\\right] ~&~... ~&~ 0 \\\\\n  \\vdots ~&~  \\vdots ~&~  \\vdots ~&~  \\vdots \\\\\n  0 ~&~ 0 ~&~ 0 ~&~... ~&~ h\\left[0\\right]\n\\end{bmatrix}\n~\n\\begin{bmatrix}\n  \\ell_{\\texttt{in}}\\left[0\\right] \\\\\n  \\ell_{\\texttt{in}}\\left[1\\right] \\\\\n  \\ell_{\\texttt{in}}\\left[2\\right] \\\\\n  \\vdots \\\\\n  \\ell_{\\texttt{in}}\\left[N-1\\right]\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "lec3.html#convolution-5",
    "href": "lec3.html#convolution-5",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nNeural network representation\n\n\n\n\n\n\n\n\nFigure¬†5: convolution represented as a layer in a neural network. Each output unit \\(i\\), \\(\\ell_{\\text {out }}[i]\\), is a linear combination of the input signal values around location \\(i\\) always using the same set of weights given by \\(\\mathbf{h}\\)."
  },
  {
    "objectID": "lec3.html#convolution-6",
    "href": "lec3.html#convolution-6",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\n2D case\n\n\n\nthe input filter is flipped vertically and horizontally\nthen slid over the image to record the inner product with the image everywhere\n\n\\[\n\\ell_{\\texttt{out}}\\left[n,m\\right] = h \\left[n,m\\right] \\circ \\ell_{\\texttt{in}}\\left[n,m\\right] =  \\sum_{k,l}h \\left[n-k,m-l \\right] \\ell_{\\texttt{in}}\\left[k,l \\right]\n\\qquad(4)\\]"
  },
  {
    "objectID": "lec3.html#convolution-7",
    "href": "lec3.html#convolution-7",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\nFigure¬†6: Illustration of a 2D convolution of an \\(9 \\times 9\\) input image with a kernel of size \\(3 \\times 3\\).For the pixels in the boundary we assumed that input image has zero values outside its boundary. The red and green boxes show the input pixels used to compute the corresponding output pixels."
  },
  {
    "objectID": "lec3.html#convolution-8",
    "href": "lec3.html#convolution-8",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nWhich operations are convolutions?"
  },
  {
    "objectID": "lec3.html#convolution-9",
    "href": "lec3.html#convolution-9",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\nFigure¬†7: Fig (a) Defocusing an image can be written as a convolution. (b) Rotation can‚Äôt be written as a convolution: it‚Äôs not translation invariant."
  },
  {
    "objectID": "lec3.html#convolution-10",
    "href": "lec3.html#convolution-10",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nProperties\n\n\n\nCommutative. As we have already discussed before the convolution is commutative, \\[h\\left[n\\right] \\circ \\ell\\left[n\\right] = \\ell\\left[n\\right] \\circ h\\left[n\\right]\\] which means that the order of convolutions is irrelevant. This is not true for the correlation.\nAssociative. Convolutions can be applied in any order: \\[\\ell_1 \\left[n\\right] \\circ \\ell_2 \\left[n\\right] \\circ \\ell_3 \\left[n\\right] = \\ell_1\\left[n\\right] \\circ (\\ell_2\\left[n\\right] \\circ \\ell_3\\left[n\\right]) = (\\ell_1\\left[n\\right] \\circ \\ell_2\\left[n\\right]) \\circ \\ell_3\\left[n\\right] \\qquad(5)\\] In practice, for finite length signals, the associative property might be affected by how boundary conditions are implemented."
  },
  {
    "objectID": "lec3.html#convolution-11",
    "href": "lec3.html#convolution-11",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nProperties\n\n\n\nDistributive with respect to the sum: \\[\\ell_1\\left[n\\right] \\circ (\\ell_2\\left[n\\right] + \\ell_3\\left[n\\right] ) = \\ell_1\\left[n\\right] \\circ \\ell_2\\left[n\\right] +  \\ell_1 \\left[n\\right] \\circ \\ell_3 \\left[n\\right]\\]\nShift. Another interesting property involves shifting the two convolved functions. Let‚Äôs consider the convolution \\(\\ell_{\\texttt{out}}\\left[n\\right] = h\\left[n\\right] \\circ \\ell_{\\texttt{in}}\\left[n\\right]\\). If the input signal, \\(\\ell_{\\texttt{in}}\\left[n\\right]\\), is translated by a constant \\(n_0\\), i.e. \\(\\ell_{\\texttt{in}}\\left[n - n_0\\right]\\), the result of the convolution with the same kernel, \\(h\\left[n\\right]\\), is the same output as before but translated by the same constant \\(n_0\\): \\[\\ell_{\\texttt{out}}\\left[n-n_0\\right] =  h\\left[n\\right] \\circ \\ell_{\\texttt{in}}\\left[n-n_0\\right]\\] Translating the kernel is equivalent: \\[\\ell_{\\texttt{out}}\\left[n-n_0\\right] =  h\\left[n\\right] \\circ \\ell_{\\texttt{in}}\\left[n-n_0\\right]  = h\\left[n-n_0\\right] \\circ \\ell_{\\texttt{in}}\\left[n\\right]\\]"
  },
  {
    "objectID": "lec3.html#convolution-12",
    "href": "lec3.html#convolution-12",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nProperties\n\n\n\nSupport. The convolution of a discrete signal of length \\(N\\) with another discrete signal of length \\(M\\) results in a discrete signal with length \\(L \\leq M+N-1\\).\nIdentity. The convolution also has an identity function, that is the impulse, \\(\\delta \\left[n\\right]\\), which takes the value 1 for \\(n=0\\) and it is zero everywhere else: \\[\\delta \\left[n\\right] =\n  \\begin{cases}\n    1       & \\quad \\text{if } n=0\\\\\n    0       & \\quad \\text{if } n \\neq 0 \\\\\n  \\end{cases}\\] The convolution of the impulse with any other signal, \\(\\ell\\left[n\\right]\\), returns the same signal: \\[\\delta \\left[n\\right] \\circ \\ell\\left[n\\right] = \\ell\\left[n\\right]\\]"
  },
  {
    "objectID": "lec3.html#convolution-13",
    "href": "lec3.html#convolution-13",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\nGiven the signal:\n\nThe signal \\(\\ell \\left[n - n_0\\right]\\) is a shifted version of \\(\\ell \\left[n\\right]\\). With \\(n_0 = 2\\) this is\n\n\n\nThe impulse function is"
  },
  {
    "objectID": "lec3.html#convolution-14",
    "href": "lec3.html#convolution-14",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\nFigure¬†8: Fig a) An impulse convolved with the input image gives no change, kernel \\(\\delta \\left[n,m\\right]\\). (b) A shifted impulse shifts the image, kernel \\(\\delta \\left[n,m-2\\right]\\). (c) Sum of two shifted copies of the image, kernel \\(0.5 \\delta \\left[n-2,m-2\\right]+ 0.5 \\delta \\left[n+2,m+2\\right]\\). (d) Image defocusing by computing a local average over windows of \\(5 \\times 5\\) pixels, uniform kernel of all \\(1/25\\). All the examples use zero padding for handling boundary conditions."
  },
  {
    "objectID": "lec3.html#convolution-15",
    "href": "lec3.html#convolution-15",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\nHandling Boundaries\n\nOnly the green region contains values that can be computed, the rest will be affected by how we decide to handle the boundaries:"
  },
  {
    "objectID": "lec3.html#convolution-16",
    "href": "lec3.html#convolution-16",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nBoundary handling: extension\n\n\nFor a kernel with support \\[\n\\left[ -N, N \\right] \\times \\left[-M, M\\right],\n\\] one has to add \\(N/2\\) additional pixels left and right of the image and \\(M/2\\) pixels at the top and bottom.\nThen, the output will have the same size as the original input image."
  },
  {
    "objectID": "lec3.html#convolution-17",
    "href": "lec3.html#convolution-17",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nBoundary handling types\n\n\n\nZero padding: Set the pixels outside the boundary to zero (or to some other constant such as the mean image value). This is the default option in most neural networks.\nCircular padding: Extend the image by replicating the pixels from the other side. If the image has size \\(P \\times Q\\), circular padding consists of the following: \\[\\ell_{\\texttt{in}}\\left[n,m\\right] = \\ell_{\\texttt{in}}\\left[(n)_P,(m)_Q\\right]\\] where \\((n)_P\\) denotes the modulo operation and \\((n)_P\\) is the reminder of \\(n/P\\).\nThis padding transform the finite length signal into a periodic infinite length signal. Although this will introduce many artifacts, it is a convenient extension for analytical derivations."
  },
  {
    "objectID": "lec3.html#convolution-18",
    "href": "lec3.html#convolution-18",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nTip\n\n\n\nMirror padding: Reflect the valid image pixels over the boundary of valid output pixels. This is the most common approach and the one that gives the best results.\nRepeat padding: Set the value to that of the nearest output image pixel with valid mask inputs."
  },
  {
    "objectID": "lec3.html#convolution-19",
    "href": "lec3.html#convolution-19",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\nFigure¬†9: (a) Different types of boundary extension. (b) The output of convolving the image with a uniform kernel of size \\(11 \\times 11\\) with all the values equal to \\(1/121\\). (c) The difference between each output and the ground truth output; see last column of (b)."
  },
  {
    "objectID": "lec3.html#convolution-20",
    "href": "lec3.html#convolution-20",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nCircular Convolution\n\n\nSpecial case: the circular convolution uses circular padding.\nThe circular convolution of two signals, \\(h\\) and \\(\\ell_{\\texttt{in}}\\), of length \\(N\\) is defined as follows:\n\\[\n\\ell_{\\texttt{out}}\\left[n\\right] = h \\left[n\\right] \\circ_N \\ell_{\\texttt{in}}\\left[n\\right] =  \\sum_{k=0}^{N-1} h \\left[(n-k)_N \\right] \\ell_{\\texttt{in}}\\left[k \\right]\n\\]\n\n\n\n\n\n\nProperties\n\n\nIn this formulation, the signal \\(h \\left[ (n)_N \\right]\\) is the infinite periodic extension, with period \\(N\\), of the finite length signal \\(h \\left[n \\right]\\). The output of the circular convolution is a periodic signal with period \\(N\\), \\(\\ell_{\\texttt{out}}\\left[n \\right] = \\ell_{\\texttt{out}}\\left[(n)_N \\right]\\).\nThe circular convolution has the same properties than the convolution (e.g., is commutative.)"
  },
  {
    "objectID": "lec3.html#convolution-21",
    "href": "lec3.html#convolution-21",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nCircular Confolution: Matrix Form"
  },
  {
    "objectID": "lec3.html#convolution-22",
    "href": "lec3.html#convolution-22",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nConvolution in the Continuous Domain\n\n\nGiven two continuous signals \\(h(t)\\) and \\(\\ell_{\\texttt{in}}(t)\\), the convolution operator is defined as follows: \\[\n\\ell_{\\texttt{out}}(t) = h(t) \\circ \\ell_{\\texttt{in}}(t) = \\int_{-\\infty}^{\\infty} h(t-\\tau) \\ell_{\\texttt{in}}(\\tau) d\\tau\n\\]\nThe properties of the continuous domain convolution are analogous to the properties of the discrete convolution, with the commutative, associative and distributive relationships still holding."
  },
  {
    "objectID": "lec3.html#convolution-23",
    "href": "lec3.html#convolution-23",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nContinuous impulse\n\n\nWe can also define the impulse, \\(\\delta (t)\\), in the continuous domain such that \\(\\ell(t) \\circ \\delta(t) = \\ell(t)\\).\nThe impulse is defined as being zero everywhere but at the origin where it takes an infinitely high value so that its integral is equal to 1: \\[\n\\int_{-\\infty}^{\\infty} \\delta(t) dt = 1\n\\] The impulse function is also called the impulse distribution (as it is not a function in the strict sense) or the Dirac delta function."
  },
  {
    "objectID": "lec3.html#convolution-24",
    "href": "lec3.html#convolution-24",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\nThe delta distribution is usually represented with an arrow of height 1, indicating that it has an finite value at that point, and a finite area equal to 1:"
  },
  {
    "objectID": "lec3.html#convolution-25",
    "href": "lec3.html#convolution-25",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nImpulse properties\n\n\n\nScaling property: \\(\\delta (at) =  \\delta (t) / |a|\\)\nSymmetry: \\(\\delta (-t) = \\delta (t)\\)\nSampling property: \\(\\ell(t) \\delta (t-a) = \\ell(a) \\delta (t-a)\\) where \\(a\\) is a constant. From this, we have the following: \\[\\int_{-\\infty}^{\\infty} \\ell(t) \\delta (t-a) dt = \\ell(a)\\]\nAs in the discrete case, the continuous impulse is also the identity for the convolution: \\(\\ell(t) \\circ \\delta (t) = \\ell(t)\\)."
  },
  {
    "objectID": "lec3.html#cross-correlation-versus-convolution",
    "href": "lec3.html#cross-correlation-versus-convolution",
    "title": "Filtering",
    "section": "Cross-Correlation Versus Convolution",
    "text": "Cross-Correlation Versus Convolution\n\n\n\nCross-correlation\n\n\nThe cross-correlation (denoted by \\(\\star\\)) between the image \\(\\ell_{\\texttt{in}}\\) and the kernel \\(h\\): \\[\n\\ell_{\\texttt{out}}\\left[n,m\\right] = \\ell_{\\texttt{in}}\\star h =  \\sum_{k,l=-N}^N \\ell_{\\texttt{in}}\\left[n+k,m+l \\right] h \\left[k,l \\right]\n\\] where the sum is done over the support of the filter \\(h \\equiv (-N,N)\\times(-N,N)\\)."
  },
  {
    "objectID": "lec3.html#cross-correlation-versus-convolution-1",
    "href": "lec3.html#cross-correlation-versus-convolution-1",
    "title": "Filtering",
    "section": "Cross-Correlation Versus Convolution",
    "text": "Cross-Correlation Versus Convolution\n\n\n\nCompared to convolution\n\n\nIn the convolution, the kernel is inverted left-right and up-down, while in the cross-correlation is not. Remember that the convolution between image \\(\\ell_{\\texttt{in}}\\) and kernel \\(h\\): \\[\n\\ell_{\\texttt{out}}\\left[n,m\\right] = \\ell_{\\texttt{in}}\\circ h = \\sum_{k,l=-N}^N \\ell_{\\texttt{in}}\\left[n-k,m-l \\right] h \\left[k,l \\right]\n\\]\n\nthe cross-correlation provides a simple technique to locate a template in an image.\nconvolution is commutative and associative while the correlation is not. The correlation breaks the symmetry between the two functions \\(h\\) and \\(\\ell_{\\texttt{in}}\\). For instance, in the correlation, shifting \\(h\\) is not equivalent to shifting \\(\\ell_{\\texttt{in}}\\) (the output will move in opposite directions)."
  },
  {
    "objectID": "lec3.html#cross-correlation-versus-convolution-2",
    "href": "lec3.html#cross-correlation-versus-convolution-2",
    "title": "Filtering",
    "section": "Cross-Correlation Versus Convolution",
    "text": "Cross-Correlation Versus Convolution\n\n\nFigure¬†10: Cross-correlation versus convolution. (a) Kernel. (b) and (e) are two input images. (c) and (f) are the output convolution with the kernel (a). (d) and (g) are the cross-correlation output with the kernel (a)."
  },
  {
    "objectID": "lec3.html#cross-correlation-versus-convolution-3",
    "href": "lec3.html#cross-correlation-versus-convolution-3",
    "title": "Filtering",
    "section": "Cross-Correlation Versus Convolution",
    "text": "Cross-Correlation Versus Convolution\n\n\n\nCorrelation for template matching\n\n\n\n\n\n\n\n\nFigure¬†11: Fig (a) Template. (b) Input image. (c) Correlation between input (b) and template (a). (d) Normalized correlation. e) Locations with cross-correlation above 75 percent of its maximum value."
  },
  {
    "objectID": "lec3.html#cross-correlation-versus-convolution-4",
    "href": "lec3.html#cross-correlation-versus-convolution-4",
    "title": "Filtering",
    "section": "Cross-Correlation Versus Convolution",
    "text": "Cross-Correlation Versus Convolution\n\n\n\nNormalized cross-correlation\n\n\nThe normalized cross-correlation, at location \\((n,m)\\), is the dot product between a template, \\(h\\), (normalized to be zero mean and unit norm) and the local image patch, \\(p\\), centered at that location and with the same size as the kernel, normalized to be unit norm. This can be implemented with the following steps:\n\nNormalize the kernel \\(h\\). The normalized kernel \\(\\hat h\\) is the result of making the kernel \\(h\\) zero mean and unit norm.\nThe normalized cross-correlation is as follows (Figure¬†11 (d)):\n\n\\[\n\\ell_{\\texttt{out}}\\left[n,m\\right]\n                =\n                \\frac{1}{\\sigma \\left[n,m \\right]}\n                \\sum_{k,l=-N}^N\n                \\ell_{\\texttt{in}}\\left[n+k,m+l \\right]\n                \\hat{h} \\left[k,l \\right]\n\\]"
  },
  {
    "objectID": "lec3.html#cross-correlation-versus-convolution-5",
    "href": "lec3.html#cross-correlation-versus-convolution-5",
    "title": "Filtering",
    "section": "Cross-Correlation Versus Convolution",
    "text": "Cross-Correlation Versus Convolution\n\n\n\nNormalized cross-correlation: computation\n\n\nNote that this equation is similar to the correlation function, but the output is scaled by the local standard deviation, \\(\\sigma \\left[m,n \\right]\\), of the image patch centered in \\((n,m)\\) and with support \\((-N,N)\\times(-N,N)\\), where \\((-N,N)\\times(-N,N)\\) is the size of the kernel \\(h\\). To compute the local standard deviation, we first compute the local mean, \\(\\mu \\left[n,m \\right]\\), as follows: \\[\n\\mu \\left[n,m \\right] = \\frac{1}{(2N+1)^2} \\sum_{k,l=-N}^N \\ell_{\\texttt{in}}\\left[n+k,m+l \\right]\n\\] And then we compute: \\[\n\\sigma^2 \\left[n,m \\right] = \\frac{1}{(2N+1)^2} \\sum_{k,l=-N}^N \\left( \\ell_{\\texttt{in}}\\left[n+k,m+l \\right] - \\mu \\left[n,m \\right] \\right) ^2\n\\]"
  },
  {
    "objectID": "lec3.html#cross-correlation-versus-convolution-6",
    "href": "lec3.html#cross-correlation-versus-convolution-6",
    "title": "Filtering",
    "section": "Cross-Correlation Versus Convolution",
    "text": "Cross-Correlation Versus Convolution\n\n\n\nNormalized cross-correlation: features\n\n\n\nwill detect the object independently of location,\nnot robust to changes such as:\n\nrotation\nscaling\nappearance"
  },
  {
    "objectID": "lec3.html#system-identification",
    "href": "lec3.html#system-identification",
    "title": "Filtering",
    "section": "System Identification",
    "text": "System Identification\nOne important task that we will study is that of explaining the behavior of complex systems such as neural networks. For an LTI filter with kernel \\(h\\left[n\\right]\\), the output from an impulse is \\(y\\left[n\\right] = h\\left[n\\right]\\). The output of a translated impulse \\(\\delta \\left[n -n_0 \\right]\\) is \\(h \\left[n -n_0 \\right]\\). This is why the kernel that defines an LTI system, \\(h\\left[n\\right]\\), is also called the impulse response of the system."
  },
  {
    "objectID": "lec1.html#euclid",
    "href": "lec1.html#euclid",
    "title": "Computer vision: intro",
    "section": "Euclid",
    "text": "Euclid\nEuclid (ca. 300 BCE): natural perspective: ray OP joining the center of projection O to the point P."
  },
  {
    "objectID": "lec1.html#aristotle",
    "href": "lec1.html#aristotle",
    "title": "Computer vision: intro",
    "section": "Aristotle",
    "text": "Aristotle\nThought that eyes are emitting vision (emission theory)."
  },
  {
    "objectID": "lec1.html#medieval",
    "href": "lec1.html#medieval",
    "title": "Computer vision: intro",
    "section": "Medieval",
    "text": "Medieval\n·∏§asan Ibn al-Haytham (Alhazen) - father of modern optics"
  },
  {
    "objectID": "lec1.html#renaissance",
    "href": "lec1.html#renaissance",
    "title": "Computer vision: intro",
    "section": "Renaissance",
    "text": "Renaissance\nLeon Battista Alberti\n\n\n\nA truly universal genius (Jacob Burckhardt, The Civilization of the Renaissance in Italy)"
  },
  {
    "objectID": "lec1.html#descartes",
    "href": "lec1.html#descartes",
    "title": "Computer vision: intro",
    "section": "Descartes",
    "text": "Descartes\nCamera eye"
  },
  {
    "objectID": "lec1.html#earlier-technologies",
    "href": "lec1.html#earlier-technologies",
    "title": "Computer vision: intro",
    "section": "Earlier technologies",
    "text": "Earlier technologies\n\n\n\ncamera obscura\nthe stereoscope\nfilm photography"
  },
  {
    "objectID": "lec1.html#tank-detector",
    "href": "lec1.html#tank-detector",
    "title": "Computer vision: intro",
    "section": "Tank Detector",
    "text": "Tank Detector\n\n\n\nRecognition system design by statistical analysis (https://dl.acm.org/doi/pdf/10.1145/800257.808903)"
  },
  {
    "objectID": "lec1.html#facial-recognition",
    "href": "lec1.html#facial-recognition",
    "title": "Computer vision: intro",
    "section": "Facial recognition",
    "text": "Facial recognition\nWoody Bledsoe, Charles Bisson and Helen Chan: facial recognition for military (1964)"
  },
  {
    "objectID": "lec1.html#summer-vision-project",
    "href": "lec1.html#summer-vision-project",
    "title": "Computer vision: intro",
    "section": "Summer Vision project",
    "text": "Summer Vision project\n\n\n\n\n\nSeymour Papert: https://dspace.mit.edu/bitstream/handle/1721.1/6125/AIM-100.pdf"
  },
  {
    "objectID": "lec1.html#summer-vision-project-1",
    "href": "lec1.html#summer-vision-project-1",
    "title": "Computer vision: intro",
    "section": "Summer Vision project",
    "text": "Summer Vision project\nWhat was the plan? Divide and conquer\nSplit teams doing different tasks:\n\n\nwriting a program to detect edges, corners, and other pixel-level information\nforming continous shapes out of these low-level features\narranging the shapes in three-dimensional space\netc."
  },
  {
    "objectID": "lec1.html#perceptron",
    "href": "lec1.html#perceptron",
    "title": "Computer vision: intro",
    "section": "Perceptron",
    "text": "Perceptron\nFrank Rosenblatt"
  },
  {
    "objectID": "lec1.html#generalized-cylinders",
    "href": "lec1.html#generalized-cylinders",
    "title": "Computer vision: intro",
    "section": "Generalized cylinders",
    "text": "Generalized cylinders\nT.O. Binford (1970)\n\n\n\nApplied in Rodney Brooks‚Äô ACRONYM (1981) - CIA aircraft detection"
  },
  {
    "objectID": "lec1.html#deformable-templates",
    "href": "lec1.html#deformable-templates",
    "title": "Computer vision: intro",
    "section": "Deformable templates",
    "text": "Deformable templates\nMartin Fischler and Robert Elschlager (1972)"
  },
  {
    "objectID": "lec1.html#mobots",
    "href": "lec1.html#mobots",
    "title": "Computer vision: intro",
    "section": "Mobots",
    "text": "Mobots\nRodney Brooks (1987): perception as action"
  },
  {
    "objectID": "lec1.html#neocognitron",
    "href": "lec1.html#neocognitron",
    "title": "Computer vision: intro",
    "section": "Neocognitron",
    "text": "Neocognitron\n\n\n\nNeocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position (https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf)"
  },
  {
    "objectID": "lec1.html#lecun-cnn-paper",
    "href": "lec1.html#lecun-cnn-paper",
    "title": "Computer vision: intro",
    "section": "LeCun CNN paper",
    "text": "LeCun CNN paper\n\n\n\n\n\n\n\n\nHandwritten Digit Recognition with a Back-Propagation Network (https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf)"
  },
  {
    "objectID": "lec1.html#what-is-vision",
    "href": "lec1.html#what-is-vision",
    "title": "Computer vision: intro",
    "section": "What is vision?",
    "text": "What is vision?\nVision is a perceptual channel that accepts a stimulus and reports some representation of the world."
  },
  {
    "objectID": "lec1.html#problem",
    "href": "lec1.html#problem",
    "title": "Computer vision: intro",
    "section": "Problem",
    "text": "Problem\nAzriel Rosenfeld, Picture Processing by Computer (1969)\n\nIf we want to give our computers eyes, we must first give them an education in the facts of life."
  },
  {
    "objectID": "lec1.html#sensing-types",
    "href": "lec1.html#sensing-types",
    "title": "Computer vision: intro",
    "section": "Sensing types",
    "text": "Sensing types\n\n\nPassive\nNot sending out light to see.\n\nActive\nSending out a signal and sensing a reflection"
  },
  {
    "objectID": "lec1.html#active-sensing-examples",
    "href": "lec1.html#active-sensing-examples",
    "title": "Computer vision: intro",
    "section": "Active sensing examples",
    "text": "Active sensing examples\n\nbats (ultrasound),\ndolphins (sound)\nabyssal fishes (light)\nsome robots (light, sound, radar)"
  },
  {
    "objectID": "lec1.html#features",
    "href": "lec1.html#features",
    "title": "Computer vision: intro",
    "section": "Features",
    "text": "Features\nA feature is a number obtained by applying simple computations to an image. Very useful information can be obtained directly from features.\nFeature extraction: simple, direct computations applied to sensor responses."
  },
  {
    "objectID": "lec1.html#model-based-approach",
    "href": "lec1.html#model-based-approach",
    "title": "Computer vision: intro",
    "section": "Model-based approach",
    "text": "Model-based approach\nTwo kinds of models:\n\nobject model: precise and geometric\nrendering model: describes the physical, geometric, and statistical processes that produce the stimulus"
  },
  {
    "objectID": "lec1.html#core-problems",
    "href": "lec1.html#core-problems",
    "title": "Computer vision: intro",
    "section": "Core problems",
    "text": "Core problems\nThe two core problems of computer vision are\n\nreconstruction\nrecognition"
  },
  {
    "objectID": "lec1.html#reconstruction",
    "href": "lec1.html#reconstruction",
    "title": "Computer vision: intro",
    "section": "Reconstruction",
    "text": "Reconstruction\nAn agent builds a model of the world from an image(s)"
  },
  {
    "objectID": "lec1.html#recognition",
    "href": "lec1.html#recognition",
    "title": "Computer vision: intro",
    "section": "Recognition",
    "text": "Recognition\nAgent draws distinctions among the objects it encounters based on visual and other information"
  },
  {
    "objectID": "lec1.html#goals",
    "href": "lec1.html#goals",
    "title": "Computer vision: intro",
    "section": "Goals",
    "text": "Goals\nThe goal of vision is to extract information needed for tasks such as:\n\nmanipulation\nnavigation\nobject recognition"
  },
  {
    "objectID": "lec1.html#computer-vision-vs-graphics",
    "href": "lec1.html#computer-vision-vs-graphics",
    "title": "Computer vision: intro",
    "section": "Computer Vision vs Graphics",
    "text": "Computer Vision vs Graphics\n\n\nVision\nEmphasis on analyzing images \n\nGraphics\nEmphasis on creating images"
  },
  {
    "objectID": "lec1.html#challenge",
    "href": "lec1.html#challenge",
    "title": "Computer vision: intro",
    "section": "Challenge",
    "text": "Challenge\nGeometry distortion"
  },
  {
    "objectID": "lec1.html#challenge-1",
    "href": "lec1.html#challenge-1",
    "title": "Computer vision: intro",
    "section": "Challenge",
    "text": "Challenge\nIllumination effects"
  },
  {
    "objectID": "lec1.html#challenge-2",
    "href": "lec1.html#challenge-2",
    "title": "Computer vision: intro",
    "section": "Challenge",
    "text": "Challenge\nAppearance variation"
  },
  {
    "objectID": "lec1.html#aside-on-cameras",
    "href": "lec1.html#aside-on-cameras",
    "title": "Computer vision: intro",
    "section": "Aside on cameras",
    "text": "Aside on cameras\nPinhole camera"
  },
  {
    "objectID": "lec1.html#aside-on-cameras-1",
    "href": "lec1.html#aside-on-cameras-1",
    "title": "Computer vision: intro",
    "section": "Aside on cameras",
    "text": "Aside on cameras\nLens camera"
  },
  {
    "objectID": "lec1.html#aside-on-cameras-2",
    "href": "lec1.html#aside-on-cameras-2",
    "title": "Computer vision: intro",
    "section": "Aside on cameras",
    "text": "Aside on cameras\nPhone camera"
  },
  {
    "objectID": "lec1.html#image-properties",
    "href": "lec1.html#image-properties",
    "title": "Computer vision: intro",
    "section": "Image properties",
    "text": "Image properties\nFour general properties of images and video\n\nedges\ntexture\noptical flow\nsegmentation into regions"
  },
  {
    "objectID": "lec1.html#edges",
    "href": "lec1.html#edges",
    "title": "Computer vision: intro",
    "section": "Edges",
    "text": "Edges\n\n\ndepth discontinuities\nsurface orientation discontinuities\nreflectance discontinuities\nillumination discontinuities (shadows)"
  },
  {
    "objectID": "lec1.html#texture",
    "href": "lec1.html#texture",
    "title": "Computer vision: intro",
    "section": "Texture",
    "text": "Texture"
  },
  {
    "objectID": "lec1.html#optical-flow",
    "href": "lec1.html#optical-flow",
    "title": "Computer vision: intro",
    "section": "Optical flow",
    "text": "Optical flow"
  },
  {
    "objectID": "lec1.html#segmentation",
    "href": "lec1.html#segmentation",
    "title": "Computer vision: intro",
    "section": "Segmentation",
    "text": "Segmentation"
  },
  {
    "objectID": "lec1.html#applications",
    "href": "lec1.html#applications",
    "title": "Computer vision: intro",
    "section": "Applications",
    "text": "Applications\n\n\nunderstanding human actions\ncaptioning\ngeometry reconstruction\nimage transformation\nmovement control"
  },
  {
    "objectID": "lec1.html#augmented-reality",
    "href": "lec1.html#augmented-reality",
    "title": "Computer vision: intro",
    "section": "Augmented Reality",
    "text": "Augmented Reality"
  },
  {
    "objectID": "lec1.html#augmented-reality-1",
    "href": "lec1.html#augmented-reality-1",
    "title": "Computer vision: intro",
    "section": "Augmented Reality",
    "text": "Augmented Reality"
  },
  {
    "objectID": "lec1.html#augmented-reality-2",
    "href": "lec1.html#augmented-reality-2",
    "title": "Computer vision: intro",
    "section": "Augmented Reality",
    "text": "Augmented Reality"
  },
  {
    "objectID": "lec1.html#autonomous-driving",
    "href": "lec1.html#autonomous-driving",
    "title": "Computer vision: intro",
    "section": "Autonomous driving",
    "text": "Autonomous driving\n\n\nobject detection and lane recognition\nadaptive cruise control\nreal-time environmental perception and decision-making"
  },
  {
    "objectID": "lec1.html#opencv",
    "href": "lec1.html#opencv",
    "title": "Computer vision: intro",
    "section": "OpenCV",
    "text": "OpenCV\n\nwhen?: at Intel in 1999.\ngoal: democratize computer vision"
  },
  {
    "objectID": "lec1.html#architecture",
    "href": "lec1.html#architecture",
    "title": "Computer vision: intro",
    "section": "Architecture",
    "text": "Architecture"
  },
  {
    "objectID": "lec1.html#opencv-modules",
    "href": "lec1.html#opencv-modules",
    "title": "Computer vision: intro",
    "section": "OpenCV modules",
    "text": "OpenCV modules"
  },
  {
    "objectID": "lec1.html#features-1",
    "href": "lec1.html#features-1",
    "title": "Computer vision: intro",
    "section": "Features",
    "text": "Features\nThe library has more than 2500 optimized algorithms for:\n\nface recognition\nobject identification\nhuman action classification\nobject tracking\n3D model extraction\naugmented reality"
  },
  {
    "objectID": "lec1.html#where-is-opencv-5",
    "href": "lec1.html#where-is-opencv-5",
    "title": "Computer vision: intro",
    "section": "Where is OpenCV 5?",
    "text": "Where is OpenCV 5?\n\nWill the future be open? Or will our algorithms be lost in time, like tears in rain?\n\n\nhttps://opencv.org/blog/where-is-opencv-5"
  },
  {
    "objectID": "lec1.html#yolo",
    "href": "lec1.html#yolo",
    "title": "Computer vision: intro",
    "section": "YOLO",
    "text": "YOLO\nA single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes \n\n\n\nYou Only Look Once: Unified, Real-Time Object Detection (https://arxiv.org/pdf/1506.02640)"
  },
  {
    "objectID": "lec1.html#yolo-model",
    "href": "lec1.html#yolo-model",
    "title": "Computer vision: intro",
    "section": "YOLO model",
    "text": "YOLO model\nDetection as a regression problem"
  },
  {
    "objectID": "lec1.html#yolo-architecture",
    "href": "lec1.html#yolo-architecture",
    "title": "Computer vision: intro",
    "section": "YOLO architecture",
    "text": "YOLO architecture"
  },
  {
    "objectID": "lec1.html#detrs",
    "href": "lec1.html#detrs",
    "title": "Computer vision: intro",
    "section": "DETRs",
    "text": "DETRs\nEnd-to-end Transformer-based detectors (DETRs)\n\n\n\nEnd-to-End Object Detection with Transformers (https://arxiv.org/pdf/2005.12872)"
  },
  {
    "objectID": "lec1.html#rt-detr",
    "href": "lec1.html#rt-detr",
    "title": "Computer vision: intro",
    "section": "RT-DETR",
    "text": "RT-DETR"
  },
  {
    "objectID": "lec1.html#segment-anything",
    "href": "lec1.html#segment-anything",
    "title": "Computer vision: intro",
    "section": "Segment Anything",
    "text": "Segment Anything\nFoundation model for image segmentation  \n\n\nSegment Anything (https://arxiv.org/pdf/2304.02643)"
  },
  {
    "objectID": "lec1.html#segment-anything-1",
    "href": "lec1.html#segment-anything-1",
    "title": "Computer vision: intro",
    "section": "Segment Anything",
    "text": "Segment Anything"
  },
  {
    "objectID": "lec1.html#natural-language-supervision",
    "href": "lec1.html#natural-language-supervision",
    "title": "Computer vision: intro",
    "section": "Natural Language Supervision",
    "text": "Natural Language Supervision\n\n\n\nLearning Transferable Visual Models From Natural Language Supervision (https://arxiv.org/pdf/2103.00020)"
  },
  {
    "objectID": "lec1.html#basic-image-operations",
    "href": "lec1.html#basic-image-operations",
    "title": "Computer vision: intro",
    "section": "Basic image operations",
    "text": "Basic image operations\nReading\n\nPython codeExample\n\n\n\nretval = cv2.imread( filename[, flags] )\n\n\n\n\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimg=cv2.imread(\"img/yolo_detection_system.png\", cv2.IMREAD_GRAYSCALE)\n\n#Displaying image using plt.imshow() method\nplt.imshow(img)\n\n\n\n\n\n\n\nFigure¬†1: Image reading example\n\n\n\n\n\n\n\n\nWriting\n\nPython codeExample\n\n\n\ncv2.imwrite( filename, img[, params] )\n\n\n\n\ncv2.imwrite(\"new_file.jpg\", img)\n\nTrue\n\n\n\n\n\nDimensions\n\n\nprint(\"Image size (H, W, C) is:\", img.shape)\n\nImage size (H, W, C) is: (466, 2140)"
  },
  {
    "objectID": "lec1.html#image-cropping",
    "href": "lec1.html#image-cropping",
    "title": "Computer vision: intro",
    "section": "Image cropping",
    "text": "Image cropping\n\ncropped = img[100:300, 500:800]\nplt.imshow(cropped)"
  },
  {
    "objectID": "lec1.html#image-resizing-rotation",
    "href": "lec1.html#image-resizing-rotation",
    "title": "Computer vision: intro",
    "section": "Image resizing / rotation",
    "text": "Image resizing / rotation\nResizing\n\nPython codeExample\n\n\n\ndst = resize( src, dsize[, dst[, fx[, fy[, interpolation]]]] )\n\n\n\n\nresized = cv2.resize(img, None, fx=0.1, fy=0.1)\nplt.imshow(resized)\n\n\n\n\n\n\n\n\n\n\n\nRotation\n\nPython codeExample\n\n\n\ndst = cv.flip( src, flipCode )\n\n\n\n\nrotated = cv2.flip(img, 0)\nplt.imshow(rotated)\n\n\n\n\n\n\n\n\n\nrotated = cv2.flip(img, 1)\nplt.imshow(rotated)\n\n\n\n\n\n\n\n\n\nrotated = cv2.flip(img, -1)\nplt.imshow(rotated)"
  },
  {
    "objectID": "lec1.html#image-annotation",
    "href": "lec1.html#image-annotation",
    "title": "Computer vision: intro",
    "section": "Image annotation",
    "text": "Image annotation\n\nPython codeExample\n\n\n\nimg = cv2.line(img, pt1, pt2, color[, thickness[, lineType[, shift]]])\nimg = cv2.circle(img, center, radius, color[, thickness[, lineType[, shift]]])\nimg = cv2.rectangle(img, pt1, pt2, color[, thickness[, lineType[, shift]]])\n\n\n\n\nannotated = cv2.line(img, (200, 100), (800, 100), (0, 255, 255), thickness=50, lineType=cv2.LINE_AA);\n\nplt.imshow(annotated)\n\n\n\n\n\n\n\n\n\nannotated2 = cv2.circle(annotated, (200,200), 100, (0, 0, 255), thickness=20, lineType=cv2.LINE_AA);\nplt.imshow(annotated2)"
  },
  {
    "objectID": "lec1.html#adding-text",
    "href": "lec1.html#adding-text",
    "title": "Computer vision: intro",
    "section": "Adding text",
    "text": "Adding text\n\nPython codeExample\n\n\n\nimg = cv2.putText(img, text, org, fontFace, fontScale, color[, thickness[, lineType[, bottomLeftOrigin]]])\n\n\n\n\nimageText = img.copy()\ntext = \"Random text\"\nfontScale = 5.3\nfontFace = cv2.FONT_HERSHEY_PLAIN\nfontColor = (0, 255, 0)\nfontThickness = 5\n\ncv2.putText(imageText, text, (100, 300), fontFace, fontScale, fontColor, fontThickness, cv2.LINE_AA);\n\n# Display the image\nplt.imshow(imageText)"
  },
  {
    "objectID": "lec1.html#image-thresholding",
    "href": "lec1.html#image-thresholding",
    "title": "Computer vision: intro",
    "section": "Image Thresholding",
    "text": "Image Thresholding\n\nPython codeExample\n\n\n\nretval, dst = cv2.threshold( src, thresh, maxval, type[, dst] )\n\n\n\n\nretval, img_thresh = cv2.threshold(img, 100, 255, cv2.THRESH_BINARY)\n\n# Show the images\nplt.figure(figsize=[18, 5])\n\nplt.subplot(121);plt.imshow(img, cmap=\"gray\");  plt.title(\"Original\")\nplt.subplot(122);plt.imshow(img_thresh, cmap=\"gray\");plt.title(\"Thresholded\")\n\nprint(img_thresh.shape)\n\n(466, 2140)"
  },
  {
    "objectID": "lec1.html#haar-cascade-classifiers",
    "href": "lec1.html#haar-cascade-classifiers",
    "title": "Computer vision: intro",
    "section": "Haar cascade classifiers",
    "text": "Haar cascade classifiers\nA Haar feature is essentially calculations that are performed on adjacent rectangular regions at a specific location in a detection window."
  },
  {
    "objectID": "lec1.html#code-example",
    "href": "lec1.html#code-example",
    "title": "Computer vision: intro",
    "section": "Code example",
    "text": "Code example\n\nPreparationExecution\n\n\n\n# Load the Haar Cascade Classifier\nface_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n\n# Read the image\nimg = cv2.imread('diverse_faces.jpg')\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# Detect faces\nfaces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n\n# Draw rectangles around faces\nfor (x, y, w, h) in faces:\n    cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\nplt.imshow(img)"
  },
  {
    "objectID": "lec1.html#pose-estimation",
    "href": "lec1.html#pose-estimation",
    "title": "Computer vision: intro",
    "section": "Pose estimation",
    "text": "Pose estimation"
  },
  {
    "objectID": "lec1.html#pose-estimation-1",
    "href": "lec1.html#pose-estimation-1",
    "title": "Computer vision: intro",
    "section": "Pose estimation",
    "text": "Pose estimation\n\nimport os\nfrom IPython.display import YouTubeVideo, display, Image\n\nprotoFile   = \"pose_deploy_linevec_faster_4_stages.prototxt\"\nweightsFile = os.path.join(\"model\", \"pose_iter_160000.caffemodel\")\nnet = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\nim = cv2.imread(\"jump.png\") #\"Tiger_Woods_crop.png\")\nim = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n\ninWidth  = im.shape[1]\ninHeight = im.shape[0]\n\nnPoints = 15\nPOSE_PAIRS = [\n    [0, 1],\n    [1, 2],\n    [2, 3],\n    [3, 4],\n    [1, 5],\n    [5, 6],\n    [6, 7],\n    [1, 14],\n    [14, 8],\n    [8, 9],\n    [9, 10],\n    [14, 11],\n    [11, 12],\n    [12, 13],\n]\n\nnetInputSize = (368, 368)\ninpBlob = cv2.dnn.blobFromImage(im, 1.0 / 255, netInputSize, (0, 0, 0), swapRB=True, crop=False)\nnet.setInput(inpBlob)\n\n# Forward Pass\noutput = net.forward()\n\n# Display probability maps\nplt.figure(figsize=(20, 5))\nfor i in range(nPoints):\n    probMap = output[0, i, :, :]\n    displayMap = cv2.resize(probMap, (inWidth, inHeight), cv2.INTER_LINEAR)\n\n    plt.subplot(2, 8, i + 1)\n    plt.axis(\"off\")\n    plt.imshow(displayMap, cmap=\"jet\")"
  },
  {
    "objectID": "lec1.html#pose-estimation-2",
    "href": "lec1.html#pose-estimation-2",
    "title": "Computer vision: intro",
    "section": "Pose estimation",
    "text": "Pose estimation\n\n# X and Y Scale\nscaleX = inWidth  / output.shape[3]\nscaleY = inHeight / output.shape[2]\n\n# Empty list to store the detected keypoints\npoints = []\n\n# Treshold\nthreshold = 0.1\n\nfor i in range(nPoints):\n    # Obtain probability map\n    probMap = output[0, i, :, :]\n\n    # Find global maxima of the probMap.\n    minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n\n    # Scale the point to fit on the original image\n    x = scaleX * point[0]\n    y = scaleY * point[1]\n\n    if prob &gt; threshold:\n        # Add the point to the list if the probability is greater than the threshold\n        points.append((int(x), int(y)))\n    else:\n        points.append(None)\n\nimPoints = im.copy()\nimSkeleton = im.copy()\n\n# Draw points\nfor i, p in enumerate(points):\n    cv2.circle(imPoints, p, 8, (255, 255, 0), thickness=-1, lineType=cv2.FILLED)\n    cv2.putText(imPoints, \"{}\".format(i), p, cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, lineType=cv2.LINE_AA)\n\n# Draw skeleton\nfor pair in POSE_PAIRS:\n    partA = pair[0]\n    partB = pair[1]\n\n    if points[partA] and points[partB]:\n        cv2.line(imSkeleton, points[partA], points[partB], (255, 255, 0), 2)\n        cv2.circle(imSkeleton, points[partA], 8, (255, 0, 0), thickness=-1, lineType=cv2.FILLED)\n\nplt.figure() #figsize=(50, 50))\n\nplt.subplot(121)\nplt.axis(\"off\")\nplt.imshow(imPoints)\n\nplt.subplot(122)\nplt.axis(\"off\")\nplt.imshow(imSkeleton)"
  },
  {
    "objectID": "lec1.html#east",
    "href": "lec1.html#east",
    "title": "Computer vision: intro",
    "section": "EAST",
    "text": "EAST\n\n\n\nEAST: An Efficient and Accurate Scene Text Detector"
  },
  {
    "objectID": "lec1.html#east-1",
    "href": "lec1.html#east-1",
    "title": "Computer vision: intro",
    "section": "EAST",
    "text": "EAST"
  },
  {
    "objectID": "lec1.html#east-2",
    "href": "lec1.html#east-2",
    "title": "Computer vision: intro",
    "section": "EAST",
    "text": "EAST\n\n# load the input image and grab the image dimensions\nimage = cv2.imread(\"img/chocolate.png\")\norig = image.copy()\n(H, W) = image.shape[:2]\n\nwidth = 320\nheight = 320\n# set the new width and height and then determine the ratio in change\n# for both the width and height\n(newW, newH) = (width, height)\nrW = W / float(newW)\nrH = H / float(newH)\n\n# resize the image and grab the new image dimensions\nimage = cv2.resize(image, (newW, newH))\n(H, W) = image.shape[:2]"
  },
  {
    "objectID": "lec1.html#east-3",
    "href": "lec1.html#east-3",
    "title": "Computer vision: intro",
    "section": "EAST",
    "text": "EAST\n\n# define the two output layer names for the EAST detector model that\n# we are interested -- the first is the output probabilities and the\n# second can be used to derive the bounding box coordinates of text\nlayerNames = [\n    \"feature_fusion/Conv_7/Sigmoid\",\n    \"feature_fusion/concat_3\"]\n\n# load the pre-trained EAST text detector\nprint(\"[INFO] loading EAST text detector...\")\nnet = cv2.dnn.readNet(\"frozen_east_text_detection.pb\")\n\n# construct a blob from the image and then perform a forward pass of\n# the model to obtain the two output layer sets\nblob = cv2.dnn.blobFromImage(image, 1.0, (W, H),\n                             (123.68, 116.78, 103.94), swapRB=True, crop=False)\n\n[INFO] loading EAST text detector..."
  },
  {
    "objectID": "lec1.html#east-4",
    "href": "lec1.html#east-4",
    "title": "Computer vision: intro",
    "section": "EAST",
    "text": "EAST\n\nimport time\nfrom imutils.object_detection import non_max_suppression\n\n\nconfidence = 0.5\n\nstart = time.time()\nnet.setInput(blob)\n(scores, geometry) = net.forward(layerNames)\nend = time.time()\n\n# show timing information on text prediction\nprint(\"[INFO] text detection took {:.6f} seconds\".format(end - start))\n\n# grab the number of rows and columns from the scores volume, then\n# initialize our set of bounding box rectangles and corresponding\n# confidence scores\n(numRows, numCols) = scores.shape[2:4]\nrects = []\nconfidences = []\n\n[INFO] text detection took 0.099429 seconds"
  },
  {
    "objectID": "lec1.html#east-5",
    "href": "lec1.html#east-5",
    "title": "Computer vision: intro",
    "section": "EAST",
    "text": "EAST\n\n# loop over the number of rows\nfor y in range(0, numRows):\n    # extract the scores (probabilities), followed by the geometrical\n    # data used to derive potential bounding box coordinates that\n    # surround text\n    scoresData = scores[0, 0, y]\n    xData0 = geometry[0, 0, y]\n    xData1 = geometry[0, 1, y]\n    xData2 = geometry[0, 2, y]\n    xData3 = geometry[0, 3, y]\n    anglesData = geometry[0, 4, y]\n\n    # loop over the number of columns\n    for x in range(0, numCols):\n        # if our score does not have sufficient probability, ignore it\n        if scoresData[x] &lt; confidence:\n            continue\n\n        # compute the offset factor as our resulting feature maps will\n        # be 4x smaller than the input image\n        (offsetX, offsetY) = (x * 4.0, y * 4.0)\n\n        # extract the rotation angle for the prediction and then\n        # compute the sin and cosine\n        angle = anglesData[x]\n        cos = np.cos(angle)\n        sin = np.sin(angle)\n\n        # use the geometry volume to derive the width and height of\n        # the bounding box\n        h = xData0[x] + xData2[x]\n        w = xData1[x] + xData3[x]\n\n        # compute both the starting and ending (x, y)-coordinates for\n        # the text prediction bounding box\n        endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n        endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n        startX = int(endX - w)\n        startY = int(endY - h)\n\n        # add the bounding box coordinates and probability score to\n        # our respective lists\n        rects.append((startX, startY, endX, endY))\n        confidences.append(scoresData[x])"
  },
  {
    "objectID": "lec1.html#east-6",
    "href": "lec1.html#east-6",
    "title": "Computer vision: intro",
    "section": "EAST",
    "text": "EAST\n\n\n# apply non-maxima suppression to suppress weak, overlapping bounding\n# boxes\nboxes = non_max_suppression(np.array(rects), probs=confidences)\n\n# loop over the bounding boxes\nfor (startX, startY, endX, endY) in boxes:\n    # scale the bounding box coordinates based on the respective\n    # ratios\n    startX = int(startX * rW)\n    startY = int(startY * rH)\n    endX = int(endX * rW)\n    endY = int(endY * rH)\n\n    # draw the bounding box on the image\n    cv2.rectangle(orig, (startX, startY), (endX, endY), (0, 255, 0), 2)\n\n# show the output image\nplt.imshow(orig)"
  },
  {
    "objectID": "lec1.html#image-segmentation",
    "href": "lec1.html#image-segmentation",
    "title": "Computer vision: intro",
    "section": "Image segmentation",
    "text": "Image segmentation\n\n\nPreparationGrayscaleOtsuNoise removal\n\n\n\nimport cv2\nimport numpy as np\nfrom IPython.display import Image, display\nfrom matplotlib import pyplot as plt\n\n# Plot the image\ndef imshow(img, ax=None):\n    if ax is None:\n        plt.imshow(img)\n    else:\n        ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n        ax.axis('off')\n\n#Image loading\nimg = cv2.imread(\"img/coins.png\")\n# Show image\nimshow(img)\n\n\n\n\n\n\n\n\n\n\n\n#image grayscale conversion\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nimshow(gray)\n\n\n\n\n\n\n\n\n\n\n\n#Threshold Processing\nret, bin_img = cv2.threshold(gray,\n                            0, 255, \n                            cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\nimshow(bin_img)\n\n\n\n\n\n\n\n\n\n\n\nkernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\nbin_img = cv2.morphologyEx(bin_img, \n                        cv2.MORPH_OPEN,\n                        kernel,\n                        iterations=2)\nimshow(bin_img)"
  },
  {
    "objectID": "lec1.html#image-segmentation-1",
    "href": "lec1.html#image-segmentation-1",
    "title": "Computer vision: intro",
    "section": "Image segmentation",
    "text": "Image segmentation\n\nBg/fg/unknownMarkersFinal\n\n\n\n# Create subplots with 1 row and 2 columns\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(8, 8))\n# sure background area\nsure_bg = cv2.dilate(bin_img, kernel, iterations=3)\nimshow(sure_bg, axes[0,0])\naxes[0, 0].set_title('Sure Background')\n\n# Distance transform\ndist = cv2.distanceTransform(bin_img, cv2.DIST_L2, 5)\nimshow(dist, axes[0,1])\naxes[0, 1].set_title('Distance Transform')\n\n#foreground area\nret, sure_fg = cv2.threshold(dist, 0.5 * dist.max(), 255, cv2.THRESH_BINARY)\nsure_fg = sure_fg.astype(np.uint8) \nimshow(sure_fg, axes[1,0])\naxes[1, 0].set_title('Sure Foreground')\n\n# unknown area\nunknown = cv2.subtract(sure_bg, sure_fg)\nimshow(unknown, axes[1,1])\naxes[1, 1].set_title('Unknown')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n# Marker labelling\n# sure foreground \nret, markers = cv2.connectedComponents(sure_fg)\n\n# Add one to all labels so that background is not 0, but 1\nmarkers += 1\n# mark the region of unknown with zero\nmarkers[unknown == 255] = 0\n\nfig, ax = plt.subplots(figsize=(6, 6))\nax.imshow(markers, cmap=\"tab20b\")\nax.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n# watershed Algorithm\nmarkers = cv2.watershed(img, markers)\n\nfig, ax = plt.subplots(figsize=(5, 5))\nax.imshow(markers, cmap=\"tab20b\")\nax.axis('off')\nplt.show()\n\n\nlabels = np.unique(markers)\n\ncoins = []\nfor label in labels[2:]: \n\n# Create a binary image in which only the area of the label is in the foreground \n#and the rest of the image is in the background \n    target = np.where(markers == label, 255, 0).astype(np.uint8)\n\n# Perform contour extraction on the created binary image\n    contours, hierarchy = cv2.findContours(\n        target, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n    )\n    coins.append(contours[0])\n\n# Draw the outline\nimg = cv2.drawContours(img, coins, -1, color=(0, 23, 223), thickness=2)\nimshow(img)"
  },
  {
    "objectID": "lec1.html#overlays",
    "href": "lec1.html#overlays",
    "title": "Computer vision: intro",
    "section": "Overlays",
    "text": "Overlays\n\n\n\nPython codeOutputOutput 2\n\n\n\nimport cv2\nimport numpy as np\n\n# Load the Haar Cascade XML file for face detection\nface_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\neye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n\n# Load the accessory image\naccessory_image = cv2.imread('img/ar_overlay.png', cv2.IMREAD_UNCHANGED)\n\n# Initialize the video capture\nvideo_capture = cv2.VideoCapture(0)\n\nwhile True:\n    # Read the video frame\n    ret, frame = video_capture.read()\n\n    # Convert the frame to grayscale\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n    # Perform face detection\n    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n\n    # Iterate over detected faces\n    for (x, y, w, h) in faces:\n        # Resize the accessory image to fit the face\n        resized_accessory = cv2.resize(accessory_image, (w, h))\n        \n        # Calculate the region of interest (ROI) for the accessory\n        roi = frame[y:y+h, x:x+w]\n\n        # Create a mask for the accessory\n        accessory_mask = resized_accessory[:, :, 3] / 255.0\n        bg_mask = 1.0 - accessory_mask\n\n        # Blend the accessory and the frame\n        accessory_pixels = resized_accessory[:, :, 0:3]\n        bg_pixels = roi[:, :, 0:3]\n\n        blended_pixels = (accessory_pixels * accessory_mask[:, :, np.newaxis]) + (bg_pixels * bg_mask[:, :, np.newaxis])\n\n        # Replace the ROI with the blended image\n        frame[y:y+h, x:x+w] = blended_pixels\n\n    # Display the resulting frame\n    cv2.imshow('Face Detection with Accessories', frame)\n\n    # Break the loop if 'q' is pressed\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# Release the video capture\nvideo_capture.release()\n\n# Close all OpenCV windows\ncv2.destroyAllWindows()"
  },
  {
    "objectID": "lec1.html#yolo-object-recognition",
    "href": "lec1.html#yolo-object-recognition",
    "title": "Computer vision: intro",
    "section": "YOLO object recognition",
    "text": "YOLO object recognition\n\n\nInitializationDetectionResult\n\n\n\nimport matplotlib.pyplot as plt\n\nimport datetime\nfrom ultralytics import YOLO\nimport cv2\nfrom imutils.video import VideoStream\n\n# define some constants\nCONFIDENCE_THRESHOLD = 0.8\nGREEN = (0, 255, 0)\n\n# load the pre-trained YOLOv8n model\nmodel = YOLO(\"yolov8n.pt\")\n\n\n\n\nframe = cv2.imread('./img/yolo_test.png')\ndetections = model(frame)[0]\nfor box in detections.boxes:\n    #extract the label name\n    label=model.names.get(box.cls.item())\n        \n    # extract the confidence (i.e., probability) associated with the detection\n    data=box.data.tolist()[0]\n    confidence = data[4]\n\n    # filter out weak detections by ensuring the\n    # confidence is greater than the minimum confidence\n    if float(confidence) &lt; CONFIDENCE_THRESHOLD:\n        continue\n\n    # if the confidence is greater than the minimum confidence,\n    # draw the bounding box on the frame\n    xmin, ymin, xmax, ymax = int(data[0]), int(data[1]), int(data[2]), int(data[3])\n    cv2.rectangle(frame, (xmin, ymin) , (xmax, ymax), GREEN, 2)\n\n    #draw confidence and label\n    y = ymin - 15 if ymin - 15 &gt; 15 else ymin + 15\n    cv2.putText(frame, \"{} {:.1f}%\".format(label,float(confidence*100)), (xmin, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, GREEN, 2)\n\n\n0: 576x640 4 persons, 12 cars, 41.2ms\nSpeed: 2.0ms preprocess, 41.2ms inference, 1.0ms postprocess per image at shape (1, 3, 576, 640)\n\n\n\n\n\nplt.imshow(frame)"
  },
  {
    "objectID": "lec1.html#thank-you",
    "href": "lec1.html#thank-you",
    "title": "Computer vision: intro",
    "section": "Thank you",
    "text": "Thank you"
  },
  {
    "objectID": "lab1.html",
    "href": "lab1.html",
    "title": "Lab 1",
    "section": "",
    "text": "Please enroll at OpenCV intro course.\nComplete the following items in the course:\n2.1. Getting Started with Images.\n2.2. Basic Image Manipulation.\nPractice cropping/resizing on a couple of images downloaded from the net."
  },
  {
    "objectID": "lec2.html#goal",
    "href": "lec2.html#goal",
    "title": "Basic concepts",
    "section": "Goal",
    "text": "Goal\n\nsimulate original research from 1960s\nhand-design an end-to-end visual system.\ncover some of the main concepts"
  },
  {
    "objectID": "lec2.html#history",
    "href": "lec2.html#history",
    "title": "Basic concepts",
    "section": "History",
    "text": "History\n\n\n\nSeymour Papert (1966)\n\n\n\n‚ÄúThe summer vision project is an attempt to use our summer workers effectively in the construction of a significant part of a visual system.‚Äù\n\n\n\n\n\n\n\nWarning\n\n\nToo optimistic!"
  },
  {
    "objectID": "lec2.html#problem-setup",
    "href": "lec2.html#problem-setup",
    "title": "Basic concepts",
    "section": "Problem setup",
    "text": "Problem setup\nVision has many different goals:\n\nobject recognition\nscene interpretation\nthree-dimensional [3D] interpretation"
  },
  {
    "objectID": "lec2.html#simple-world",
    "href": "lec2.html#simple-world",
    "title": "Basic concepts",
    "section": "Simple world",
    "text": "Simple world\n\n\n\nBlock world\n\n\nIntroduced by Larry G. Roberts in 1963."
  },
  {
    "objectID": "lec2.html#simple-world-1",
    "href": "lec2.html#simple-world-1",
    "title": "Basic concepts",
    "section": "Simple world",
    "text": "Simple world\n\n\n\nDescription\n\n\n\nworld is composed of a very simple (yet varied) set of objects.\nthese simple objects are composed of flat surfaces that can be horizontal or vertical\nthese objects will be resting on a white horizontal ground plane\nwe can build these objects by cutting, folding, and gluing together some pieces of colored paper\n\n\n\n\n\n\n\nFigure¬†1: A world of simple objects. Print, cut, and build your own blocks world!"
  },
  {
    "objectID": "lec2.html#a-simple-image-formation-model-1",
    "href": "lec2.html#a-simple-image-formation-model-1",
    "title": "Basic concepts",
    "section": "A Simple Image Formation Model",
    "text": "A Simple Image Formation Model\n\n\n\nParallel of orthographic projection\n\n\n\nlight rays travel parallel to each other and perpendicular to the camera plane\nthis type of projection produces images in which objects do not change size as they move closer or farther from the camera\nparallel lines in 3D remain parallel in the 2D image."
  },
  {
    "objectID": "lec2.html#a-simple-image-formation-model-2",
    "href": "lec2.html#a-simple-image-formation-model-2",
    "title": "Basic concepts",
    "section": "A Simple Image Formation Model",
    "text": "A Simple Image Formation Model\n\n\n\nPerspective projection\n\n\n\nthe image is formed by the convergence of the light rays into a single point (focal point).\nmost pictures taken with a camera will be better described by perspective projection"
  },
  {
    "objectID": "lec2.html#a-simple-image-formation-model-3",
    "href": "lec2.html#a-simple-image-formation-model-3",
    "title": "Basic concepts",
    "section": "A Simple Image Formation Model",
    "text": "A Simple Image Formation Model\n\n\n\n\n\n\nFigure¬†2\n\n\n\n\n\n\nNote that near edges are larger than far edges, and parallel lines in 3D are not parallel.\nPicture taken from far away using zoom resulting in an image that can be described by parallel projection."
  },
  {
    "objectID": "lec2.html#a-simple-image-formation-model-4",
    "href": "lec2.html#a-simple-image-formation-model-4",
    "title": "Basic concepts",
    "section": "A Simple Image Formation Model",
    "text": "A Simple Image Formation Model"
  },
  {
    "objectID": "lec2.html#a-simple-image-formation-model-5",
    "href": "lec2.html#a-simple-image-formation-model-5",
    "title": "Basic concepts",
    "section": "A Simple Image Formation Model",
    "text": "A Simple Image Formation Model\n\nhow a point in world coordinates \\((X,Y,Z)\\) projects into the image plane\ncamera center is inside the 3D plane \\(X=0\\)\nthe horizontal axis of the camera (\\(x\\)) is parallel to the ground plane (\\(Y=0\\))\nthe camera is tilted so that the line connecting the origin of the world coordinates system and the image center is perpendicular to the image plane\nthe angle \\(\\theta\\) is the angle between this line and the \\(Z\\)-axis\nthe image is parameterized by coordinates \\((x,y)\\)."
  },
  {
    "objectID": "lec2.html#a-simple-image-formation-model-6",
    "href": "lec2.html#a-simple-image-formation-model-6",
    "title": "Basic concepts",
    "section": "A Simple Image Formation Model",
    "text": "A Simple Image Formation Model\n\n\n\n\n\n\nFigure¬†3\n\n\n\n\n\nThe \\(Z\\)-axis is identical to the \\(Y\\)-axis up to a sign change and a scaling."
  },
  {
    "objectID": "lec2.html#a-simple-image-formation-model-7",
    "href": "lec2.html#a-simple-image-formation-model-7",
    "title": "Basic concepts",
    "section": "A Simple Image Formation Model",
    "text": "A Simple Image Formation Model"
  },
  {
    "objectID": "lec2.html#a-simple-image-formation-model-8",
    "href": "lec2.html#a-simple-image-formation-model-8",
    "title": "Basic concepts",
    "section": "A Simple Image Formation Model",
    "text": "A Simple Image Formation Model\n\n\n\nFeatures\n\n\n\nthe origin of the world coordinates projects on the origin of the image coordinates\ntherefore, the world point \\((0,0,0)\\) projects into \\((0,0)\\)\nthe resolution of the image will also affect the transformation from world coordinates to image coordinates via a constant factor \\(\\alpha\\)\nfor now we assume that pixels are square and we will see a more general form in ) and that this constant is \\(\\alpha=1\\)."
  },
  {
    "objectID": "lec2.html#a-simple-image-formation-model-9",
    "href": "lec2.html#a-simple-image-formation-model-9",
    "title": "Basic concepts",
    "section": "A Simple Image Formation Model",
    "text": "A Simple Image Formation Model"
  },
  {
    "objectID": "lec2.html#a-simple-image-formation-model-10",
    "href": "lec2.html#a-simple-image-formation-model-10",
    "title": "Basic concepts",
    "section": "A Simple Image Formation Model",
    "text": "A Simple Image Formation Model\n\n\n\nCoordinate transformation\n\n\nThe transformation between world coordinates and image coordinates can be written as follows:\n\\[\\begin{aligned}\nx &=& X \\\\\ny &=&  \\cos(\\theta) Y - \\sin(\\theta) Z\n\\end{aligned} \\qquad(1)\\]\n\n\n\n\n\nWith this particular parametrization of the world and camera coordinate systems, the world coordinates \\(Y\\) and \\(Z\\) are mixed after projection. From the camera, a point moving parallel to the \\(Z\\)-axis will be indistinguishable from a point moving parallel to the \\(Y\\)-axis."
  },
  {
    "objectID": "lec2.html#a-simple-goal",
    "href": "lec2.html#a-simple-goal",
    "title": "Basic concepts",
    "section": "A Simple Goal",
    "text": "A Simple Goal\n\n\n\nOur goal\n\n\n\nrecovering the world coordinates of all the pixels seen by the camera.\n\n\n\n\n\n\n\nNon-goal\n\n\n\nrecover the actual color of the surface seen by each pixel \\((x,y)\\)\n\n(requires discounting for illumination effects as the color of the pixel is a combination of the surface albedo and illumination (color of the light sources and interreflections)."
  },
  {
    "objectID": "lec2.html#from-images-to-edges",
    "href": "lec2.html#from-images-to-edges",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\nWe want to recover \\(X(x,y),Y(x,y), Z(x,y)\\) from \\(l(x,y)\\)"
  },
  {
    "objectID": "lec2.html#from-images-to-edges-1",
    "href": "lec2.html#from-images-to-edges-1",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\nThe observed image is a function: \\[\\ell(x,y)\\]\n\n\n\nDescription\n\n\n\ninput: location, \\((x,y)\\)\noutput: the intensity at that location.\n\n\n\n\n\n\n\nWhat is an image?\n\n\nIn this representation, the image is an array of intensity values (color values) indexed by location."
  },
  {
    "objectID": "lec2.html#from-images-to-edges-2",
    "href": "lec2.html#from-images-to-edges-2",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\n\nAny planar line-drawing is geometrically consistent with infinitely many 3-D structures (Sinha-Adelson ‚Äô93 paper)"
  },
  {
    "objectID": "lec2.html#from-images-to-edges-3",
    "href": "lec2.html#from-images-to-edges-3",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\nFigure¬†4: Image as a surface. The vertical axis corresponds to image intensity. For clarity here, we have reversed the vertical axis. Dark values are shown higher than lighter values."
  },
  {
    "objectID": "lec2.html#from-images-to-edges-4",
    "href": "lec2.html#from-images-to-edges-4",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\nBenefits\n\n\n\nthis representation is ideal for determining the light intensity originating from different directions in space and striking the camera plane, as it provides explicit representation of this information\nthe array of pixel intensities, \\(\\ell(x,y)\\), is a reasonable representation as input to the early stages of visual processing because, although we do not know the distance of surfaces in the world, the direction of each light ray in the world is well defined.\n\n\n\n\n\n\n\nAlternative options\n\n\nOther initial representations:\n\nimages could be coded in the Fourier domain\npixels could combine light coming in different directions."
  },
  {
    "objectID": "lec2.html#from-images-to-edges-5",
    "href": "lec2.html#from-images-to-edges-5",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\n\n\n\nNote\n\n\nWe are interested in interpreting the 3D structure of the scene and the objects within.\n\n\n\n\n\n\nChallenges\n\n\nNeed to represent:\n\nboundaries between objects\nchanges in the surface orientation"
  },
  {
    "objectID": "lec2.html#from-images-to-edges-6",
    "href": "lec2.html#from-images-to-edges-6",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\nAlternative representation\n\n\nFor scene interpretation:\n\ncollections of small image patches\nregions of uniform properties\nedges"
  },
  {
    "objectID": "lec2.html#from-images-to-edges-7",
    "href": "lec2.html#from-images-to-edges-7",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\nAlternative initial representations\n\n\nGekko‚Äôs eye. Their pupil has a four-diamond-shaped pinhole aperture that could allow them to encode distance to a target in the retinal image."
  },
  {
    "objectID": "lec2.html#from-images-to-edges-8",
    "href": "lec2.html#from-images-to-edges-8",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\n\n\n\nWhat is an edge\n\n\nAn image regions where there are strong changes of the image with respect to location.\n\n\n\n\n\n\nEdge factors\n\n\n\nocclusion boundaries\nchanges in surface orientation\nchanges in surface albedo\nshadows"
  },
  {
    "objectID": "lec2.html#from-images-to-edges-9",
    "href": "lec2.html#from-images-to-edges-9",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\nFigure¬†5"
  },
  {
    "objectID": "lec2.html#from-images-to-edges-10",
    "href": "lec2.html#from-images-to-edges-10",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\nImage edge classification\n\n\n\nObject boundaries: These indicate pixels that delineate the boundaries of any object. Boundaries between objects generally correspond to changes in surface color, texture, and orientation.\nChanges in surface orientation: These indicate locations where there are strong image variations due to changes in the surface orientations. A change in surface orientation produces changes in the image intensity because intensity is a function of the angle between the surface and the incident light.\nShadow edges: This can be harder than it seems. In this simple world, shadows are soft, creating slow transitions between dark and light."
  },
  {
    "objectID": "lec2.html#from-images-to-edges-11",
    "href": "lec2.html#from-images-to-edges-11",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\nObject boundary classification\n\n\n\nContact edges: This is a boundary between two objects that are in physical contact. Therefore, there is no depth discontinuity.\nOcclusion boundaries: Occlusion boundaries happen when an object is partially in front of another. Occlusion boundaries generally produce depth discontinuities. In this simple world, we will position the objects in such a way that objects do not occlude each other but they will occlude the background."
  },
  {
    "objectID": "lec2.html#from-images-to-edges-12",
    "href": "lec2.html#from-images-to-edges-12",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\n\n\n\nChallenges\n\n\n\nin most natural scenes, this classification is very hard\nrequires the interpretation of the scene at different levels."
  },
  {
    "objectID": "lec2.html#from-images-to-edges-13",
    "href": "lec2.html#from-images-to-edges-13",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\nFirst step: detecting candidate edges in the image.\nIf we think of the image as a function \\(\\ell (x,y) \\in C(\\mathbb{R}^2)\\), we can measure the degree of variation using the gradient:\n\\[\\begin{aligned}\n\\nabla \\ell = \\left( \\frac{\\partial \\ell}{\\partial x}, \\frac{\\partial \\ell}{\\partial y} \\right)\n\\end{aligned}\\]\n\n\n\nThe direction of the gradient indicates the direction in which the variation of intensities is larger. If we are on top of an edge, the direction of larger variation will be in the direction perpendicular to the edge."
  },
  {
    "objectID": "lec2.html#from-images-to-edges-14",
    "href": "lec2.html#from-images-to-edges-14",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\nImportant\n\n\nThe image is not a continuous function as we only know the values of the \\(\\ell (x,y)\\) at discrete locations (pixels).\n\n\n\n\n\n\nApproximation\n\n\nWe approximate the partial derivatives by:\n\\[\\begin{aligned}\n\\frac{\\partial \\ell}{\\partial x} &\\simeq \\ell(x,y) - \\ell(x-1,y) \\\\\n\\frac{\\partial \\ell}{\\partial y} &\\simeq \\ell(x,y) - \\ell(x,y-1)\n\\end{aligned} \\qquad(2)\\]"
  },
  {
    "objectID": "lec2.html#from-images-to-edges-15",
    "href": "lec2.html#from-images-to-edges-15",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\nBetter approximation (TBD)\n\n\nCombine the image pixels around \\((x,y)\\) with the weights: \\[\\begin{aligned}\n\\frac{1}{4} \\times\n\\left [\n\\begin{matrix}\n-1 & 0 & 1 \\\\\n-2 & 0 & 2 \\\\\n-1 & 0 & 1\n\\end{matrix}\n\\right ] \\nonumber\n\\end{aligned}\\]"
  },
  {
    "objectID": "lec2.html#from-images-to-edges-16",
    "href": "lec2.html#from-images-to-edges-16",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\nEdge properties\n\n\nFrom the image gradient, we can extract a number of interesting quantities: \\[\\begin{aligned}\n    e(x,y) &= \\lVert \\nabla \\ell(x,y) \\rVert   & \\quad\\quad \\triangleleft \\quad \\texttt{edge strength}\\\\\n    \\theta(x,y) &= \\angle \\nabla \\ell =  \\arctan \\left( \\frac{\\partial \\ell / \\partial y}{\\partial \\ell / \\partial x} \\right) & \\quad\\quad \\triangleleft \\quad \\texttt{edge orientation}\n\\end{aligned}\\]\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nedge strength is the gradient magnitude\nedge orientation is perpendicular to the gradient direction"
  },
  {
    "objectID": "lec2.html#from-images-to-edges-17",
    "href": "lec2.html#from-images-to-edges-17",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\nThe unit norm vector perpendicular to an edge is:\n\\[\\begin{aligned}\n{\\bf n} = \\frac{\\nabla \\ell}{\\lVert \\nabla \\ell \\rVert}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lec2.html#from-images-to-edges-18",
    "href": "lec2.html#from-images-to-edges-18",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\nDecision\n\n\nWhich pixels belong to:\n\nedges (regions of the image with sharp intensity variations)\nuniform regions (flat surfaces)\n\n\n\n\n\n\n\nHow?\n\n\nBy thresholding the edge strength \\(e(x,y)\\).\nIn the pixels with edges, we can also measure the edge orientation \\(\\theta(x,y)\\)."
  },
  {
    "objectID": "lec2.html#from-images-to-edges-19",
    "href": "lec2.html#from-images-to-edges-19",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\n\n\n\nFigure¬†6: Gradient and edge types.\n\n\n\n\n\n\n\n\n\nFigure¬†7\n\n\n\n\n\nFigure¬†6 and Figure¬†7 visualize the edges and the normal vector on each edge."
  },
  {
    "objectID": "lec2.html#from-images-to-edges-20",
    "href": "lec2.html#from-images-to-edges-20",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges"
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-1",
    "href": "lec2.html#from-edges-to-surfaces-1",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nObjective\n\n\nWe want to recover world coordinates \\(X(x,y)\\), \\(Y(x,y)\\), and \\(Z(x,y)\\) for each image location \\((x,y)\\)\n\n\n\n\n\n\nSteps\n\n\n\nrecovering the \\(X\\) world coordinates is trivial as they are directly observed: for each pixel with image coordinates \\((x,y)\\) the world coordinate is \\(X(x,y) = x\\)\nrecovering \\(Y\\) and \\(Z\\) will be harder as we only observe a mixture of the two world coordinates.\n\n\n\n\n\n\nHere we have written the world coordinates as functions of image location \\((x,y)\\) to make explicit that we want to recover the 3D locations of the visible points.\nIn this simple world, we will formulate this problem as a set of linear equations."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-2",
    "href": "lec2.html#from-edges-to-surfaces-2",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nFigure/Ground Segmentation\n\n\nSegmentation of an image into figure and ground is a classical problem in human perception and computer vision that was introduced by Gestalt psychology.\n\n\n\n\n\n\nThe classical visual illusion ‚Äútwo faces or a vase‚Äù is an example of figure-ground segmentation problem."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-3",
    "href": "lec2.html#from-edges-to-surfaces-3",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\n\n\n\nSegmentation: goal\n\n\nDecide whether a pixel belongs to one of the foreground objects or to the background.\n\n\n\n\n\n\n\n\n\nSegmentation: basic approach\n\n\nWe can simply look at the color values of each pixel\n\nbright pixels that have low saturation (similar values of the red-blue-green [RBG] components) correspond to the white ground plane\nand the rest of the pixels are likely to belong to the colored blocks that compose our simple world\n\n\n\n\n\n\nIn general, the problem of image segmentation into distinct objects is a very challenging task."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-4",
    "href": "lec2.html#from-edges-to-surfaces-4",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nBackground\n\n\nIf we assume that the background corresponds to a horizontal ground plane, then for all pixels that belong to the ground we can set \\(Y(x,y)=0\\).\n\n\n\n\n\n\nObjects\n\n\nFor pixels that belong to objects we will have to measure additional image properties before we can deduce any geometric scene constraints."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-5",
    "href": "lec2.html#from-edges-to-surfaces-5",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nOcclusion Edges\n\n\nAn occlusion boundary separates two different surfaces at different distances from the observer.\nThe object in front is the one owning the boundary.\n\n\n\n\n\n\nKnowing who owns the boundary is important as an edge provides cues about the 3D geometry, but those cues only apply to the surface that owns the boundary."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-6",
    "href": "lec2.html#from-edges-to-surfaces-6",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nAssumption\n\n\nIn this simple world, we will assume that objects do not occlude each other (this can be relaxed) and that the only occlusion boundaries are the boundaries between the objects and the ground.\n\n\n\n\n\n\n\n\n\nNote\n\n\nNot all boundaries between the objects and the ground correspond to depth gradients."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-7",
    "href": "lec2.html#from-edges-to-surfaces-7",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nContact Edges\n\n\nContact edges are boundaries between two distinct objects but where there exists no depth discontinuity.\nDespite that there is not a depth discontinuity, there is an occlusion here (as one surface is hidden behind another), and the edge shape is only owned by one of the two surfaces.\n\n\n\n\n\n\nCalculation\n\n\nIf we assume that all the objects rest on the ground plane, then we can set \\(Y(x,y)=0\\) on the contact edges.\nContact edges can be detected as transitions between the object (above) and ground (below). In our simple world only horizontal edges can be contact edges."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-8",
    "href": "lec2.html#from-edges-to-surfaces-8",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\nFigure¬†8: For each vertical line (shown in red), scanning from top to bottom, transitions from ground to figure are occlusion boundaries, and transitions from figure to ground are contact edges. This heuristic will fails when an object occludes another."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-9",
    "href": "lec2.html#from-edges-to-surfaces-9",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\n\n\n\nInvariant Scene Properties (world \\(\\rightarrow\\) image)\n\n\n\nCollinearity: a straight 3D line will project into a straight line in the image.\nCotermination: if two or more 3D lines terminate at the same point, the corresponding projections will also terminate at a common point.\nSmoothness: a smooth 3D curve will project into a smooth 2D curve."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-10",
    "href": "lec2.html#from-edges-to-surfaces-10",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nReverse invariants (image \\(\\rightarrow\\) world)\n\n\n\na straight line in the image could correspond to a curved line in the 3D world but that happens to be precisely aligned with respect to the viewers point of view to appear as a straight line\ntwo lines that intersect in the image plane could be disjointed in the 3D space."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-11",
    "href": "lec2.html#from-edges-to-surfaces-11",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nReverse invariants (image \\(\\rightarrow\\) world)\n\n\n\nif two lines coterminate in the image, then, one can conclude that it is very likely that they also touch each other in 3D\nif the 3D lines do not touch each other, then it will require a very specific alignment between the observer and the lines for them to appear to coterminate in the image. Therefore, one can safely conclude that the lines might also touch in 3D."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-12",
    "href": "lec2.html#from-edges-to-surfaces-12",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nNonaccidental properties\n\n\nThese properties are called nonaccidental properties because they will only be observed in the image if they also exist in the world or by accidental alignments between the observer and scene structures.\n\n\n\n\n\n\nGeneric view\n\n\nUnder a generic view, nonaccidental properties will be shared by the image and the 3D world."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-13",
    "href": "lec2.html#from-edges-to-surfaces-13",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nGeneric view assumption: the observer should not assume that he has a special position in the world‚Ä¶ The most generic interpretation is to see a vertical line as a vertical line in 3D."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-14",
    "href": "lec2.html#from-edges-to-surfaces-14",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nApplication to simple world\n\n\n\nin the simple world all 3D edges are either vertical or horizontal\nunder parallel projection and with the camera having its horizontal axis parallel to the ground, we know that vertical 3D lines will project into vertical 2D lines in the image\nhorizontal lines will, in general, project into oblique lines\ntherefore, we can assume than any vertical line in the image is also a vertical line in the world."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-15",
    "href": "lec2.html#from-edges-to-surfaces-15",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\n\n\n\nChallenge\n\n\nThe assumption that vertical 2D lines are also 3D vertical lines will not always work!\n\n\n\n\n\n\n\n\n\nFigure¬†9\n\n\n\n\n\nIn the case of the cube, there is a particular viewpoint that will make an horizontal line project into a vertical line, but this will require an accidental alignment between the cube and the line of sight of the observer."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-16",
    "href": "lec2.html#from-edges-to-surfaces-16",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nFormulation\n\n\nWe can now translate the inferred 3D edge orientation into linear constraints on the global 3D structure. We will formulate these constraints in terms of \\(Y(x,y)\\). Once \\(Y(x,y)\\) is recovered we can also recover \\(Z(x,y)\\) from Equation¬†1.\nIn a 3D vertical edge, using the projection equations, the derivative of \\(Y\\) along the edge will be\n\\[\\begin{aligned}\n\\partial Y / \\partial y &= 1/ \\cos(\\theta)\\end{aligned}\n\\qquad(3)\\]"
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-17",
    "href": "lec2.html#from-edges-to-surfaces-17",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nFormulation\n\n\nIn a 3D horizontal edge, the coordinate \\(Y\\) will not change. Therefore, the derivative along the edge should be zero:\n\\[\\begin{aligned}\n\\partial Y / \\partial {\\bf t} &= 0\n\\end{aligned}\n\\qquad(4)\\]\nwhere the vector \\(\\bf t\\) denotes direction tangent to the edge, \\({\\bf t}=(-n_y, n_x)\\)."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-18",
    "href": "lec2.html#from-edges-to-surfaces-18",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nFormulation\n\n\nWe can write this derivative as a function of derivatives along the \\(x\\) and \\(y\\) image coordinates: \\[\\begin{aligned}\n\\partial Y / \\partial {\\bf t} =  \\nabla Y \\cdot {\\bf t} = -n_y \\partial Y / \\partial x + n_x \\partial Y / \\partial y\n\\end{aligned} \\qquad(5)\\]\nWhen the edges coincide with occlusion edges, special care should be taken so that these constraints are only applied to the surface that owns the boundary."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-19",
    "href": "lec2.html#from-edges-to-surfaces-19",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\n\n\n\nApproximation\n\n\nWe represent the world coordinates \\(X(x,y)\\), \\(Y(x,y)\\), and \\(Z(x,y)\\) as images where the coordinates \\(x,y\\) correspond to pixel locations.\nTherefore, it is useful to approximate the partial derivatives in the same way that we approximated the image partial derivatives in equations (Equation¬†2). Using this approximation, Equation¬†3 can be written as follows: \\[\\begin{aligned}\nY(x,y)-Y(x,y-1) &= 1/ \\cos(\\theta)\n\\end{aligned}\\]\nSimilar relationships can be obtained from equations Equation¬†4 and Equation¬†5."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-20",
    "href": "lec2.html#from-edges-to-surfaces-20",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\nNo edges\n\n\n\nThe ‚ÄúRule of Nothing‚Äù (Ted Adelson): where you see nothing, assume nothing happens, and just propagate information from where something happened."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-21",
    "href": "lec2.html#from-edges-to-surfaces-21",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nConstraint Propagation\n\n\nMost of the image consists of flat regions where we do not have such edge constraints and we thus don‚Äôt have enough local information to infer the surface orientation.\nTherefore, we need some criteria in order to propagate information from the boundaries, where we do have information about the 3D structure, into the interior of flat image regions.\n\n\n\n\n\n\nNote\n\n\nThis problem is common in many visual domains."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-22",
    "href": "lec2.html#from-edges-to-surfaces-22",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nAn image patch without context is not enough to infer its 3D shape.\n\n\nThe same patch shown in the original image. Information about its 3D orientation its propagated from the surrounding edges."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-23",
    "href": "lec2.html#from-edges-to-surfaces-23",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nAssumption\n\n\nIn this case we will assume that the object faces are planar. Thus, flat image regions impose the following constraints on the local 3D structure: \\[\\begin{aligned}\n\\partial^2 Y / \\partial x^2 &= 0  \\\\\n\\partial^2 Y / \\partial y^2 &= 0 \\\\  \n\\partial^2 Y / \\partial y \\partial x &= 0\n\\end{aligned}\\]\nThat is, the second order derivative of \\(Y\\) should be zero. As before, we want to approximate the continuous partial derivatives."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-24",
    "href": "lec2.html#from-edges-to-surfaces-24",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nApproximation\n\n\nThe approximation to the second derivative can be obtained by applying twice the first order derivative approximated by equations (Equation¬†2). The result is\n\\[\\partial^2 Y / \\partial x^2 \\simeq 2Y(x,y)-Y(x+1,y)-Y(x-1,y),\n\\] and similarly for \\(\\partial^2 X / \\partial x^2\\)."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-25",
    "href": "lec2.html#from-edges-to-surfaces-25",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\nA Simple Inference Scheme\n\n\n\nEquation system\n\n\nAll the different constraints described previously can be written as an overdetermined system of linear equations. Each equation will have the form: \\[\\begin{aligned}\n\\mathbf{a}_i \\mathbf{Y} = b_i\n\\end{aligned}\\]\nwhere \\(\\mathbf{Y}\\) is a vectorized version of the image \\(Y\\) (i.e., all rows of pixels have been concatenated into a flat vector).\n\n\n\n\n\n\n\n\n\nNote\n\n\nThere might be many more equations than there are image pixels."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-26",
    "href": "lec2.html#from-edges-to-surfaces-26",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nTranslation\n\n\nWe can translate all the constraints described in the previous sections into this form:\n\nfor instance, if the index \\(i\\) corresponds to one of the pixels inside one of the planar faces of a foreground object, then there will be three equations\none of the planarity constraint can be written as \\[\\mathbf{a}_i = [0, \\dots, 0, -1, 2, -1, 0, \\dots, 0], \\, b_i=0,\\] and analogous equations can be written for the other two."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-27",
    "href": "lec2.html#from-edges-to-surfaces-27",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\n\n\n\nHow to solve?\n\n\nWe can solve the system of equations by minimizing the following cost function: \\[\\begin{aligned}\nJ = \\sum_i (\\mathbf{a}_i\\mathbf{Y} - b_i)^2\n\\end{aligned}\\] where the sum is over all the constraints.\n\n\n\n\n\n\nWeights\n\n\nIf some constraints are more important than others, we can use weights \\(w_i\\). \\[\\begin{aligned}\nJ = \\sum_i w_i (\\mathbf{a}_i \\mathbf{Y} - b_i)^2\n\\end{aligned}\\]"
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-28",
    "href": "lec2.html#from-edges-to-surfaces-28",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\n\n\n\nNote\n\n\nOur formulation has resulted on a big system of linear constraints (there are more equations than there are pixels in the image).\n\n\n\n\n\n\nSystem of equations\n\n\nIt is convenient to write the system of equations in matrix form: \\[\\begin{aligned}\n\\mathbf{A} \\mathbf{Y}  = \\mathbf{b}\n\\end{aligned}\\]\nwhere row \\(i\\) of the matrix \\({\\bf A}\\) contains the constraint coefficients \\(\\mathbf{a}_i\\).\nThe system of equations is overdetermined (\\(\\mathbf{A}\\) has more rows than columns)."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-29",
    "href": "lec2.html#from-edges-to-surfaces-29",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nSolution\n\n\nWe can use the pseudoinverse to compute the solution:\n\\[\\begin{aligned}\n\\bf Y = (\\mathbf{A}^T \\mathbf{A})^{-1} \\mathbf{A}^T \\mathbf{b}\n\\end{aligned}\\]\nThis problem can be solved efficiently as the matrix \\(\\mathbf{A}\\) is very sparse (most of the elements are zero)."
  },
  {
    "objectID": "lec2.html#results",
    "href": "lec2.html#results",
    "title": "Basic concepts",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\nFigure¬†10\n\n\n\n\n\nCoordinates shows the resulting world coordinates \\(X(x,y)\\), \\(Y(x,y)\\), \\(Z(x,y)\\) for each pixel.\nWorld coordinates \\(X\\), \\(Y\\), and \\(Z\\) are shown as images with the gray level coding the value of each coordinate (black corresponds to the value 0)."
  },
  {
    "objectID": "lec2.html#results-1",
    "href": "lec2.html#results-1",
    "title": "Basic concepts",
    "section": "Results",
    "text": "Results\n\n\n\nWhat do we get?\n\n\nThere are a few things to reflect on:\n\nIt works. At least it seems to work pretty well. Knowing how well it works will require having some way of evaluating performance. This will be important.\nBut it cannot possibly work all the time. We have made lots of assumptions that will work only in this simple world. The rest of the book will involve upgrading this approach to apply to more general input images.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nDespite that this approach will not work on general images, many of the general ideas will carry over to more sophisticated solutions (e.g., gather and propagate local evidence)."
  },
  {
    "objectID": "lec2.html#results-2",
    "href": "lec2.html#results-2",
    "title": "Basic concepts",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\nEvaluation\n\n\nEvaluationof performance is a very important topic. Here, one simple way to visually verify that the solution is correct is to render the objects under new view points.\n\n\n\n\n\n\n\n\n\nFigure¬†11\n\n\n\n\n\nTo show that the algorithm for 3D interpretation gives reasonable results we can re-render the inferred 3D structure from different viewpoints."
  },
  {
    "objectID": "lec2.html#results-3",
    "href": "lec2.html#results-3",
    "title": "Basic concepts",
    "section": "Results",
    "text": "Results\nYou may also try the interactive demo below to see the 3D structure. (The demo supports mouse zoom in and out, pan, and rotate.)"
  },
  {
    "objectID": "lec2.html#generalization",
    "href": "lec2.html#generalization",
    "title": "Basic concepts",
    "section": "Generalization",
    "text": "Generalization\n\n\n\nNote\n\n\nOne desired property of any vision system is it ability to generalize outside of the domain for which it was designed to operate.\nOut of domain generalization refers to the ability of a system to operate outside the domain for which it was designed.\nWe have listed several assumptions earlier.\nIn learning-based approaches the training dataset specifies the domain."
  },
  {
    "objectID": "lec2.html#generalization-1",
    "href": "lec2.html#generalization-1",
    "title": "Basic concepts",
    "section": "Generalization",
    "text": "Generalization\n\n\n\n\n\n\nFigure¬†12\n\n\n\n\n\n\nshadows are not soft\nthe green cube occludes the red one\none object on top of the other"
  },
  {
    "objectID": "lec2.html#generalization-2",
    "href": "lec2.html#generalization-2",
    "title": "Basic concepts",
    "section": "Generalization",
    "text": "Generalization\n\n\n\n\n\n\nFigure¬†13: Impossible steps\n\n\n\n\n\n\nleft: this shape looks rectangular and the stripes appear to be painted on the surface\nright: the shape looks as though it has steps, with the stripes corresponding to shading due to the surface orientation\nmiddle: the shape is ambiguous."
  },
  {
    "objectID": "lec2.html#concluding-remarks",
    "href": "lec2.html#concluding-remarks",
    "title": "Basic concepts",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\n\n\n\n\n\n\nLack of knowledge\n\n\n\nthe system is still unaware of the fact that the scene is composed of a set of distinct objects\nas the system lacks a representation of which objects are actually present in the scene, we cannot visualize the occluded parts"
  },
  {
    "objectID": "lec2.html#concluding-remarks-1",
    "href": "lec2.html#concluding-remarks-1",
    "title": "Basic concepts",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\n\n\n\nA different approach: model-based\n\n\nWe could have a set of predefined models of the objects that can be present in the scene and the system can try to decide if they are present or not in the image, and recover their parameters (i.e., pose, color).\nRecognition allows indexing properties that are not directly available in the image."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computer vision course",
    "section": "",
    "text": "Slides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nLab1\n\n\n\nLab2"
  },
  {
    "objectID": "index.html#lectures",
    "href": "index.html#lectures",
    "title": "Computer vision course",
    "section": "",
    "text": "Slides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nLab1\n\n\n\nLab2"
  },
  {
    "objectID": "nb/pset2_2024/pset2_2024.html",
    "href": "nb/pset2_2024/pset2_2024.html",
    "title": "Problem 1",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nfrom numpy.fft import fft2, ifft2, fftshift, ifftshift\nfrom numpy import angle, real\nfrom numpy import exp, abs, pi, sqrt\nimport matplotlib.pyplot as plt\nimport cv2\nimport scipy.ndimage as ndimage\n\ndef imshow(im, cmap='gray'):\n    # clip image from 0-1\n    im = np.clip(im, 0, 1)\n    plt.imshow(im, cmap=cmap)\n! curl http://6.869.csail.mit.edu/sp21/pset3_data/einsteinandwho.jpg &gt; einsteinandwho.jpg\n! curl http://6.869.csail.mit.edu/sp21/pset3_data/bill.avi &gt; bill.avi\n### TODO: ENTER YOUR CODE BELOW"
  },
  {
    "objectID": "nb/pset2_2024/pset2_2024.html#problem-2",
    "href": "nb/pset2_2024/pset2_2024.html#problem-2",
    "title": "Problem 1",
    "section": "Problem 2",
    "text": "Problem 2\n\n# scale image's intensity to [0,1] with mean value of 0.5 for better visualization.\ndef intensityscale(raw_img):\n\n    # scale an image's intensity from [min, max] to [0, 1].\n    v_min, v_max = raw_img.min(), raw_img.max()\n    scaled_im = (raw_img * 1.0 - v_min) / (v_max - v_min)\n\n    # keep the mean to be 0.5.\n    meangray = np.mean(scaled_im)\n    scaled_im = scaled_im - meangray + 0.5\n\n    # clip to [0, 1]\n    scaled_im = np.clip(scaled_im, 0, 1)\n\n    return scaled_im\n\n\n### ENTER YOUR CODE BELOW"
  },
  {
    "objectID": "nb/pset2_2024/pset2_2024.html#problem-3.a",
    "href": "nb/pset2_2024/pset2_2024.html#problem-3.a",
    "title": "Problem 1",
    "section": "Problem 3.a",
    "text": "Problem 3.a\n\n# 9x9 images\nimSize = 9\n\n# we would like to magnify the change between im1 and im2 by 4x\nmagnificationFactor = 4;\n\n# horizontal movement from (0, 0) to (0, 1)\nim1 = np.zeros([imSize, imSize])\nim2 = np.zeros([imSize, imSize])\nim1[0,0] = 1\nim2[0,1] = 1\n\nff1 = fftshift(fft2(im1))\nff2 = fftshift(fft2(im2))\n\nplt.figure()\nplt.subplot(121)\nimshow(im1)\nplt.subplot(122)\nimshow(im2)\n\nplt.figure()\nplt.subplot(121)\nimshow(angle(ff1))\nplt.subplot(122)\nimshow(angle(ff2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMagnify Change\n\ndef magnifyChange(im1, im2, magnificationFactor):\n\n    # find phase shift in frequency domain\n    im1Dft = fft2(im1)\n    im2Dft = fft2(im2)\n    phaseShift = # TODO\n\n    # magnify the phase change in frequency domain\n    magnifiedDft = # TODO\n\n    # what does the magnified phase change cause in image space?\n    magnified = ifft2(magnifiedDft).real;\n\n    return magnified\n\nHINT: If you‚Äôre not familiar with complex number in python, here‚Äôs a quickstart.\n\n# create a complex number\nx = 1 + 1j\nprint(\"x =\", x)\nprint(\"x.real\", x.real, \"x.imag\", x.imag)\n\n# magnitude and phase of complex number\nmag = abs(x)\nphase = angle(x)\n\nprint(\"Magnitude\", mag)\nprint(\"Phase\", phase)\n\n# Euler's formula\ny = mag * exp(phase * 1j)\nprint(\"y =\", y)\n\n\n# magnify position change\nmagnified = magnifyChange(im1, im2, magnificationFactor);\n\nplt.figure(figsize=(12,36))\nplt.subplot(131)\nimshow(im1); plt.title('im1');\n\nplt.subplot(132)\nimshow(im2); plt.title('im2');\n\nplt.subplot(133)\nimshow(magnified); plt.title('magnified');\nplt.savefig(\"problem_3a.png\", bbox=\"tight\")"
  },
  {
    "objectID": "nb/pset2_2024/pset2_2024.html#problem-3.b",
    "href": "nb/pset2_2024/pset2_2024.html#problem-3.b",
    "title": "Problem 1",
    "section": "Problem 3.b",
    "text": "Problem 3.b\n\n# 9x9 images\nimSize = 9\n\n# we would like to magnify the change between im1 and im2 by 4x\nmagnificationFactor = 4\n\n# horizontal movement from (1, 1) to (1, 2)\n# additional vertical movement from (9, 9) to (8, 9)\nim1 = np.zeros([imSize, imSize])\nim2 = np.zeros([imSize, imSize])\nim1[0,0] = 1\nim2[0,1] = 1\nim1[8,8] = 1\nim2[7,8] = 1\n\n\n### TODO: ENTER YOUR CODE BELOW\n### manually edit the expected matrix (currently set as zeros) by creating 1s to show the expected output\nexpected = np.zeros([imSize, imSize])\n\n\n\n# magnify position change\nmagnified = magnifyChange(im1, im2, magnificationFactor)\n\n\nplt.figure(figsize=(12,36))\nplt.subplot(141)\nimshow(im1); plt.title('im1');\n\nplt.subplot(142)\nimshow(im2); plt.title('im2');\n\nplt.subplot(143)\nimshow(expected); plt.title('expected');\n\nplt.subplot(144)\nimshow(magnified); plt.title('magnified');\nplt.savefig(\"problem_3b.png\", bbox=\"tight\")"
  },
  {
    "objectID": "nb/pset2_2024/pset2_2024.html#problem-3.c",
    "href": "nb/pset2_2024/pset2_2024.html#problem-3.c",
    "title": "Problem 1",
    "section": "Problem 3.c",
    "text": "Problem 3.c\n\n# 9x9 images\nimSize = 9\n\n# we would like to magnify the change between im1 and im2 by 4x\nmagnificationFactor = 4\n\n# width of our Gaussian window\nsigma = 2\n\n# horizontal movement from (1, 1) to (1, 2)\n# additional vertical movement from (9, 9) to (8, 9)\nim1 = np.zeros([imSize, imSize])\nim2 = np.zeros([imSize, imSize])\nim1[0,0] = 1\nim2[0,1] = 1\nim1[8,8] = 1\nim2[7,8] = 1\n\n# we will magnify windows of the image and aggregate the results\nmagnified = np.zeros([imSize, imSize])\n\n# meshgrid for computing Gaussian window\nX, Y = np.meshgrid(np.arange(imSize), np.arange(imSize))\n\nfor y in range(0, imSize, 2*sigma):\n    for x in range(0, imSize, 2*sigma):\n        gaussianMask = # TODO\n        windowMagnified = magnifyChange(# TODO,\\\n            magnificationFactor)\n        magnified = magnified + windowMagnified\n\nplt.figure(figsize=(12,36))\nplt.subplot(131)\nimshow(im1); plt.title('im1');\n\nplt.subplot(132)\nimshow(im2); plt.title('im2');\n\nplt.subplot(133)\nimshow(magnified); plt.title('magnified');\nplt.savefig(\"problem_3c.png\", bbox=\"tight\")"
  },
  {
    "objectID": "nb/pset2_2024/pset2_2024.html#problem-3.d",
    "href": "nb/pset2_2024/pset2_2024.html#problem-3.d",
    "title": "Problem 1",
    "section": "Problem 3.d",
    "text": "Problem 3.d\n\nimport numpy as np\nimport cv2\n\ncap = cv2.VideoCapture('bill.avi')\n\n# list of video frames\nframes = []\n\nwhile(cap.isOpened()):\n    # read frame from the video\n    ret, frame = cap.read()\n\n    if ret is False:\n        break\n\n    frames.append(frame)\n\ncap.release()\n\n# scale frame to 0-1\nframes = np.array(frames) / 255.\nprint(\"frames size:\", frames.shape, \"# (nb_frames, height, width, channel)\")\n\n# get height, width\nnumFrames = frames.shape[0]\nheight = frames.shape[1]\nwidth = frames.shape[2]\n\n\nMotion magnification\nFill out code here\n\n# 10x magnification of motion\nmagnificationFactor = 10\n\n# width of Gaussian window\nsigma = 13\n\n# alpha for moving average\nalpha = 0.5\n\n# we will magnify windows of the video and aggregate the results\nmagnified = np.zeros_like(frames)\n\n# meshgrid for computing Gaussian window\nX, Y = np.meshgrid(np.arange(width), np.arange(height))\n\n# iterate over windows of the frames\nxRange = list(range(0, width, 2*sigma))\nyRange = list(range(0, height, 2*sigma))\nnumWindows = len(xRange) * len(yRange)\nwindowIndex = 1\n\nfor y in yRange:\n    for x in xRange:\n        for channelIndex in range(3): # RGB channels\n            for frameIndex in range(numFrames):\n\n                # create windowed frames\n                gaussianMask = # TODO\n                windowedFrames = gaussianMask * frames[frameIndex,:,:,channelIndex]\n\n                # initialize moving average of phase for current window/channel\n                if frameIndex == 0:\n                    windowAveragePhase = angle(fft2(windowedFrames))\n\n                windowDft = fft2(windowedFrames)\n\n                # compute phase shift and constrain to [-pi, pi] since\n                # angle space wraps around\n                windowPhaseShift = angle(windowDft) - windowAveragePhase\n                windowPhaseShift[windowPhaseShift &gt; pi] = windowPhaseShift[windowPhaseShift &gt; pi] - 2 * pi\n                windowPhaseShift[windowPhaseShift &lt; -pi] = windowPhaseShift[windowPhaseShift &lt; -pi] + 2 * pi\n\n                # magnify phase shift\n                windowMagnifiedPhase = # TODO\n\n                # go back to image space\n                windowMagnifiedDft = # TODO\n                windowMagnified = abs(ifft2(windowMagnifiedDft))\n\n                # update moving average\n                windowPhaseUnwrapped = windowAveragePhase + windowPhaseShift\n                windowAveragePhase = alpha * windowAveragePhase + (1 - alpha) * windowPhaseUnwrapped\n\n                # aggregate\n                magnified[frameIndex,:,:,channelIndex] = magnified[frameIndex,:,:,channelIndex] + windowMagnified\n\n        # print progress\n        print('{}/{}'.format(windowIndex, numWindows), end='\\r')\n        windowIndex += 1\n\n\noutputs = magnified / np.max(magnified)\nfor channelIndex in range(3):\n    originalFrame = frames[0,:,:,channelIndex]\n    magnifiedFrame = outputs[0,:,:,channelIndex]\n    scale = np.std(originalFrame[:]) / np.std(magnifiedFrame[:])\n    originalMean = np.mean(originalFrame[:])\n    magnifiedMean = np.mean(magnifiedFrame[:])\n    outputs[:,:,:,channelIndex] = magnifiedMean + scale * (outputs[:,:,:,channelIndex] - magnifiedMean)\n\noutputs = np.clip(outputs, 0, 1)\n\n\n# create output video\nfourcc = cv2.VideoWriter_fourcc('M','J','P','G')\n# fourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter('bill_magnified.avi',fourcc, 30.0, (height, width))\n\nfor i in range(frames.shape[0]):\n    # scale the frame back to 0-255\n    frame = (np.clip(outputs[i], 0, 1) * 255).astype(np.uint8)\n\n    # write frame to output video\n    out.write(frame)\n\nout.release()\n\n\n# Only for colab downloading videos\ntry:\n    from google.colab import files\n    files.download('bill_magnified.avi')\nexcept:\n    print(\"Only for google colab\")"
  }
]