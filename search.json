[
  {
    "objectID": "nb/cv24_pset1/pset1_2024.html",
    "href": "nb/cv24_pset1/pset1_2024.html",
    "title": "PSET 1",
    "section": "",
    "text": "# Ignore the pip dependency error\n\nimport cv2\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import convolve2d as conv2d\nimport scipy.sparse as sps\nfrom PIL import Image\n\n# # Package for fast equation solving\nfrom sys import platform\nprint(platform)\nif platform == \"linux\" or platform == \"linux2\":\n    ! apt-get install libsuitesparse-dev\nelif platform == \"darwin\":\n    ! brew install suite-sparse\n\n! pip3 install sparseqr\nimport sparseqr\n\ndarwin\n==&gt; Auto-updating Homebrew...\nAdjust how often this is run with HOMEBREW_AUTO_UPDATE_SECS or disable with\nHOMEBREW_NO_AUTO_UPDATE. Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n==&gt; Auto-updated Homebrew!\nUpdated 3 taps (oven-sh/bun, homebrew/core and homebrew/cask).\n==&gt; New Formulae\nab-av1                     gnome-online-accounts      node-red\nfalcosecurity-libs         keyutils                   oven-sh/bun/bun@1.2.5\nfpm                        krep                       sentry-native\ngcr                        libgoa                     yalantinglibs\ngit-graph                  libgudev\n==&gt; New Casks\nkate                                     mouseless@preview\n\nYou have 14 outdated formulae installed.\n\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/suite-sparse/manifests/7.10.1\n######################################################################### 100.0%\n==&gt; Fetching dependencies for suite-sparse: isl, libmpc, gcc and metis\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/isl/manifests/0.27\n######################################################################### 100.0%\n==&gt; Fetching isl\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/isl/blobs/sha256:de143fddb0e20b\n######################################################################### 100.0%\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/libmpc/manifests/1.3.1\n######################################################################### 100.0%\n==&gt; Fetching libmpc\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/libmpc/blobs/sha256:5c8cdc4d460\n######################################################################### 100.0%\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/gcc/manifests/14.2.0_1\n######################################################################### 100.0%\n==&gt; Fetching gcc\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/gcc/blobs/sha256:96d8bf02f621cf\n######################################################################### 100.0%\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/metis/manifests/5.1.0\n######################################################################### 100.0%\n==&gt; Fetching metis\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/metis/blobs/sha256:60ef633238eb\n######################################################################### 100.0%\n==&gt; Fetching suite-sparse\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/suite-sparse/blobs/sha256:edc57\n######################################################################### 100.0%\n==&gt; Installing dependencies for suite-sparse: isl, libmpc, gcc and metis\n==&gt; Installing suite-sparse dependency: isl\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/isl/manifests/0.27\nAlready downloaded: /Users/vitvly/Library/Caches/Homebrew/downloads/40b1c5526f95db33208143fa79887179e758121659d8877597f553e6e6188879--isl-0.27.bottle_manifest.json\n==&gt; Pouring isl--0.27.arm64_sequoia.bottle.tar.gz\nüç∫  /opt/homebrew/Cellar/isl/0.27: 74 files, 7.6MB\n==&gt; Installing suite-sparse dependency: libmpc\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/libmpc/manifests/1.3.1\nAlready downloaded: /Users/vitvly/Library/Caches/Homebrew/downloads/fdfa98e0f8bb3ce075cb32776ac2345aa2f89252706c162aecfc841085fa76be--libmpc-1.3.1.bottle_manifest.json\n==&gt; Pouring libmpc--1.3.1.arm64_sequoia.bottle.tar.gz\nüç∫  /opt/homebrew/Cellar/libmpc/1.3.1: 13 files, 492.3KB\n==&gt; Installing suite-sparse dependency: gcc\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/gcc/manifests/14.2.0_1\nAlready downloaded: /Users/vitvly/Library/Caches/Homebrew/downloads/cece94dbe926093c968a24f66b9a0172afe5cc2ef22253029bc591147237045b--gcc-14.2.0_1.bottle_manifest.json\n==&gt; Pouring gcc--14.2.0_1.arm64_sequoia.bottle.tar.gz\nüç∫  /opt/homebrew/Cellar/gcc/14.2.0_1: 1,914 files, 459.8MB\n==&gt; Installing suite-sparse dependency: metis\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/metis/manifests/5.1.0\nAlready downloaded: /Users/vitvly/Library/Caches/Homebrew/downloads/f3ed9b4299ad5a23ac4e265012f58bed7574a85759c297dbea217aeb89707128--metis-5.1.0.bottle_manifest.json\n==&gt; Pouring metis--5.1.0.arm64_sequoia.bottle.tar.gz\nüç∫  /opt/homebrew/Cellar/metis/5.1.0: 19 files, 12.1MB\n==&gt; Installing suite-sparse\n==&gt; Pouring suite-sparse--7.10.1.arm64_sequoia.bottle.tar.gz\nüç∫  /opt/homebrew/Cellar/suite-sparse/7.10.1: 217 files, 49.3MB\n==&gt; Running `brew cleanup suite-sparse`...\nDisable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\nHide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\nCollecting sparseqr\n  Downloading sparseqr-1.4.1.tar.gz (18 kB)\n  Installing build dependencies ... \u001b[?done\n  Getting requirements to build wheel ... done\n  Installing backend dependencies ... done\n  Preparing metadata (pyproject.toml) ... done\nRequirement already satisfied: numpy&gt;1.2 in /Users/vitvly/c/lnu/md/compvision/venv/lib/python3.12/site-packages (from sparseqr) (2.1.1)\nRequirement already satisfied: scipy&gt;=1.0 in /Users/vitvly/c/lnu/md/compvision/venv/lib/python3.12/site-packages (from sparseqr) (1.15.1)\nRequirement already satisfied: cffi&gt;=1.0 in /Users/vitvly/c/lnu/md/compvision/venv/lib/python3.12/site-packages (from sparseqr) (1.17.1)\nRequirement already satisfied: setuptools&gt;35 in /Users/vitvly/c/lnu/md/compvision/venv/lib/python3.12/site-packages (from sparseqr) (75.8.0)\nRequirement already satisfied: pycparser in /Users/vitvly/c/lnu/md/compvision/venv/lib/python3.12/site-packages (from cffi&gt;=1.0-&gt;sparseqr) (2.22)\nBuilding wheels for collected packages: sparseqr\n  Building wheel for sparseqr (pyproject.toml) ... done\n  Created wheel for sparseqr: filename=sparseqr-1.4.1-cp312-cp312-macosx_15_0_arm64.whl size=29345 sha256=67d260633bd09b0e824e5dcafa1863f8d27d981c3594c2ad702b5efc20153c61\n  Stored in directory: /Users/vitvly/Library/Caches/pip/wheels/f9/95/5b/d45847ffa40c431a57bbd40628576944de186e3544cc6edca2\nSuccessfully built sparseqr\nInstalling collected packages: sparseqr\nSuccessfully installed sparseqr-1.4.1\n\n[notice] A new release of pip is available: 25.0 -&gt; 25.0.1\n[notice] To update, run: pip install --upgrade pip\n\n\n\n# Get images for plots (images are also included in the pset folder)\n! wget http://6.869.csail.mit.edu/fa19/psets19/pset1/img1.png\n! wget http://6.869.csail.mit.edu/fa19/psets19/pset1/img2.png\n! wget http://6.869.csail.mit.edu/fa19/psets19/pset1/img3.png\n! wget http://6.869.csail.mit.edu/fa19/psets19/pset1/img4.png\n! wget http://6.869.csail.mit.edu/sp23/pset1/pset_1_reference.png"
  },
  {
    "objectID": "nb/cv24_pset1/pset1_2024.html#import-the-necessary-dependencies",
    "href": "nb/cv24_pset1/pset1_2024.html#import-the-necessary-dependencies",
    "title": "PSET 1",
    "section": "",
    "text": "# Ignore the pip dependency error\n\nimport cv2\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import convolve2d as conv2d\nimport scipy.sparse as sps\nfrom PIL import Image\n\n# # Package for fast equation solving\nfrom sys import platform\nprint(platform)\nif platform == \"linux\" or platform == \"linux2\":\n    ! apt-get install libsuitesparse-dev\nelif platform == \"darwin\":\n    ! brew install suite-sparse\n\n! pip3 install sparseqr\nimport sparseqr\n\ndarwin\n==&gt; Auto-updating Homebrew...\nAdjust how often this is run with HOMEBREW_AUTO_UPDATE_SECS or disable with\nHOMEBREW_NO_AUTO_UPDATE. Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n==&gt; Auto-updated Homebrew!\nUpdated 3 taps (oven-sh/bun, homebrew/core and homebrew/cask).\n==&gt; New Formulae\nab-av1                     gnome-online-accounts      node-red\nfalcosecurity-libs         keyutils                   oven-sh/bun/bun@1.2.5\nfpm                        krep                       sentry-native\ngcr                        libgoa                     yalantinglibs\ngit-graph                  libgudev\n==&gt; New Casks\nkate                                     mouseless@preview\n\nYou have 14 outdated formulae installed.\n\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/suite-sparse/manifests/7.10.1\n######################################################################### 100.0%\n==&gt; Fetching dependencies for suite-sparse: isl, libmpc, gcc and metis\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/isl/manifests/0.27\n######################################################################### 100.0%\n==&gt; Fetching isl\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/isl/blobs/sha256:de143fddb0e20b\n######################################################################### 100.0%\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/libmpc/manifests/1.3.1\n######################################################################### 100.0%\n==&gt; Fetching libmpc\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/libmpc/blobs/sha256:5c8cdc4d460\n######################################################################### 100.0%\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/gcc/manifests/14.2.0_1\n######################################################################### 100.0%\n==&gt; Fetching gcc\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/gcc/blobs/sha256:96d8bf02f621cf\n######################################################################### 100.0%\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/metis/manifests/5.1.0\n######################################################################### 100.0%\n==&gt; Fetching metis\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/metis/blobs/sha256:60ef633238eb\n######################################################################### 100.0%\n==&gt; Fetching suite-sparse\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/suite-sparse/blobs/sha256:edc57\n######################################################################### 100.0%\n==&gt; Installing dependencies for suite-sparse: isl, libmpc, gcc and metis\n==&gt; Installing suite-sparse dependency: isl\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/isl/manifests/0.27\nAlready downloaded: /Users/vitvly/Library/Caches/Homebrew/downloads/40b1c5526f95db33208143fa79887179e758121659d8877597f553e6e6188879--isl-0.27.bottle_manifest.json\n==&gt; Pouring isl--0.27.arm64_sequoia.bottle.tar.gz\nüç∫  /opt/homebrew/Cellar/isl/0.27: 74 files, 7.6MB\n==&gt; Installing suite-sparse dependency: libmpc\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/libmpc/manifests/1.3.1\nAlready downloaded: /Users/vitvly/Library/Caches/Homebrew/downloads/fdfa98e0f8bb3ce075cb32776ac2345aa2f89252706c162aecfc841085fa76be--libmpc-1.3.1.bottle_manifest.json\n==&gt; Pouring libmpc--1.3.1.arm64_sequoia.bottle.tar.gz\nüç∫  /opt/homebrew/Cellar/libmpc/1.3.1: 13 files, 492.3KB\n==&gt; Installing suite-sparse dependency: gcc\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/gcc/manifests/14.2.0_1\nAlready downloaded: /Users/vitvly/Library/Caches/Homebrew/downloads/cece94dbe926093c968a24f66b9a0172afe5cc2ef22253029bc591147237045b--gcc-14.2.0_1.bottle_manifest.json\n==&gt; Pouring gcc--14.2.0_1.arm64_sequoia.bottle.tar.gz\nüç∫  /opt/homebrew/Cellar/gcc/14.2.0_1: 1,914 files, 459.8MB\n==&gt; Installing suite-sparse dependency: metis\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/metis/manifests/5.1.0\nAlready downloaded: /Users/vitvly/Library/Caches/Homebrew/downloads/f3ed9b4299ad5a23ac4e265012f58bed7574a85759c297dbea217aeb89707128--metis-5.1.0.bottle_manifest.json\n==&gt; Pouring metis--5.1.0.arm64_sequoia.bottle.tar.gz\nüç∫  /opt/homebrew/Cellar/metis/5.1.0: 19 files, 12.1MB\n==&gt; Installing suite-sparse\n==&gt; Pouring suite-sparse--7.10.1.arm64_sequoia.bottle.tar.gz\nüç∫  /opt/homebrew/Cellar/suite-sparse/7.10.1: 217 files, 49.3MB\n==&gt; Running `brew cleanup suite-sparse`...\nDisable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\nHide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\nCollecting sparseqr\n  Downloading sparseqr-1.4.1.tar.gz (18 kB)\n  Installing build dependencies ... \u001b[?done\n  Getting requirements to build wheel ... done\n  Installing backend dependencies ... done\n  Preparing metadata (pyproject.toml) ... done\nRequirement already satisfied: numpy&gt;1.2 in /Users/vitvly/c/lnu/md/compvision/venv/lib/python3.12/site-packages (from sparseqr) (2.1.1)\nRequirement already satisfied: scipy&gt;=1.0 in /Users/vitvly/c/lnu/md/compvision/venv/lib/python3.12/site-packages (from sparseqr) (1.15.1)\nRequirement already satisfied: cffi&gt;=1.0 in /Users/vitvly/c/lnu/md/compvision/venv/lib/python3.12/site-packages (from sparseqr) (1.17.1)\nRequirement already satisfied: setuptools&gt;35 in /Users/vitvly/c/lnu/md/compvision/venv/lib/python3.12/site-packages (from sparseqr) (75.8.0)\nRequirement already satisfied: pycparser in /Users/vitvly/c/lnu/md/compvision/venv/lib/python3.12/site-packages (from cffi&gt;=1.0-&gt;sparseqr) (2.22)\nBuilding wheels for collected packages: sparseqr\n  Building wheel for sparseqr (pyproject.toml) ... done\n  Created wheel for sparseqr: filename=sparseqr-1.4.1-cp312-cp312-macosx_15_0_arm64.whl size=29345 sha256=67d260633bd09b0e824e5dcafa1863f8d27d981c3594c2ad702b5efc20153c61\n  Stored in directory: /Users/vitvly/Library/Caches/pip/wheels/f9/95/5b/d45847ffa40c431a57bbd40628576944de186e3544cc6edca2\nSuccessfully built sparseqr\nInstalling collected packages: sparseqr\nSuccessfully installed sparseqr-1.4.1\n\n[notice] A new release of pip is available: 25.0 -&gt; 25.0.1\n[notice] To update, run: pip install --upgrade pip\n\n\n\n# Get images for plots (images are also included in the pset folder)\n! wget http://6.869.csail.mit.edu/fa19/psets19/pset1/img1.png\n! wget http://6.869.csail.mit.edu/fa19/psets19/pset1/img2.png\n! wget http://6.869.csail.mit.edu/fa19/psets19/pset1/img3.png\n! wget http://6.869.csail.mit.edu/fa19/psets19/pset1/img4.png\n! wget http://6.869.csail.mit.edu/sp23/pset1/pset_1_reference.png"
  },
  {
    "objectID": "nb/cv24_pset1/pset1_2024.html#define-the-sparse-matrix",
    "href": "nb/cv24_pset1/pset1_2024.html#define-the-sparse-matrix",
    "title": "PSET 1",
    "section": "Define the Sparse Matrix",
    "text": "Define the Sparse Matrix\n\ndef sparseMatrix(i, j, Aij, imsize):\n    \"\"\" Build a sparse matrix containing 2D linear neighborhood operators\n    Input:\n        Aij = [ni, nj, nc] nc: number of neighborhoods with contraints\n        i: row index\n        j: column index\n        imsize: [nrows ncols]\n    Returns:\n        A: a sparse matrix. Each row contains one 2D linear operator\n    \"\"\"\n    ni, nj, nc = Aij.shape\n    nij = ni*nj\n\n    a = np.zeros((nc*nij))\n    m = np.zeros((nc*nij))\n    n = np.zeros((nc*nij))\n    grid_range = np.arange(-(ni-1)/2, 1+(ni-1)/2)\n    jj, ii = np.meshgrid(grid_range, grid_range)\n    ii = ii.reshape(-1,order='F')\n    jj = jj.reshape(-1,order='F')\n\n\n    k = 0\n    for c in range(nc):\n        # Get matrix index\n        x = (i[c]+ii) + (j[c]+jj)*nrows\n        a[k:k+nij] = Aij[:,:,c].reshape(-1,order='F')\n        m[k:k+nij] = c\n        n[k:k+nij] = x\n\n        k += nij\n\n    m = m.astype(np.int32)\n    n = n.astype(np.int32)\n    A = sps.csr_matrix((a, (m,  n)))\n\n    return A"
  },
  {
    "objectID": "nb/cv24_pset1/pset1_2024.html#define-world-parameters-and-plot-the-edges",
    "href": "nb/cv24_pset1/pset1_2024.html#define-world-parameters-and-plot-the-edges",
    "title": "PSET 1",
    "section": "Define world parameters and plot the edges",
    "text": "Define world parameters and plot the edges\n\n# World parameters\ntheta = 35*math.pi/180;\n\nimg = cv2.imread('img1.png')\nimg = img[:, :, ::-1].astype(np.float32)\n\nnrows, ncols, colors = img.shape\nground = (np.min(img, axis=2) &gt; 110).astype(np.float32)\nprint('ground', ground.shape, ground)\nforeground = (ground == 0).astype(np.float32)\n\nm = np.mean(img, 2)\nkern = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=np.float32)\ndmdx = conv2d(m, kern, 'same')\ndmdy = conv2d(m, kern.transpose(), 'same')\n\nmag = np.sqrt(dmdx**2 + dmdy**2)\nmag[0, :] = 0\nmag[-1, :] = 0\nmag[:, 0] = 0\nmag[:, -1] = 0\n\nedges = mag &gt;= 30\nedges = edges * foreground\n\n## Occlusion and contact edges\n\n####################################################################\n### COMPLETE THE CODE BELOW (TODOs) AND COPY IT INTO YOUR REPORT ###\n####################################################################\n\npi = math.pi\nedge_orientation = ## TODO\nvertical_edges = ## TODO\nhorizontal_edges = ## TODO\n\n####################################################################\n###################### STOP COPYING HERE ###########################\n####################################################################\n\nkern = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=np.float32)\nhorizontal_ground_to_foreground_edges = (conv2d(ground, kern, 'same'))&gt;0;\nhorizontal_foreground_to_ground_edges = (conv2d(foreground, kern, 'same'))&gt;0;\nvertical_ground_to_foreground_edges = vertical_edges*np.abs(conv2d(ground, kern.transpose(), 'same'))&gt;0\n\n\nocclusion_edges = edges*(vertical_ground_to_foreground_edges + horizontal_ground_to_foreground_edges)\ncontact_edges   = horizontal_edges*(horizontal_foreground_to_ground_edges);\n\n\nE = np.concatenate([vertical_edges[:,:,None],\n                    horizontal_edges[:,:,None],\n                    np.zeros(occlusion_edges.shape)[:,:,None]], 2)\n\n\n# Plot\nplt.figure()\nplt.subplot(2,2,1)\nplt.imshow(img.astype(np.uint8))\nplt.axis('off')\nplt.title('Input image')\nplt.subplot(2,2,2)\nplt.imshow(edges == 0, cmap='gray')\nplt.axis('off')\nplt.title('Edges')\n\n# Normals\nK = 3\ney, ex = np.where(edges[::K, ::K])\nex *= K\ney *= K\nplt.figure()\nplt.subplot(2,2,3)\nplt.imshow(np.max(mag)-mag, cmap='gray')\ndxe = dmdx[::K, ::K][edges[::K, ::K] &gt; 0]\ndye = dmdy[::K, ::K][edges[::K, ::K] &gt; 0]\nn = np.sqrt(dxe**2 + dye**2)\ndxe = dxe/n\ndye = dye/n\nplt.quiver(ex, ey, dxe, -dye, color='r')\nplt.axis('off')\nplt.title('Normals')\n\n\nplt.subplot(2,2,4)\nplt.imshow(np.max(mag)-mag, cmap='gray')\n# Recreate the normals plot using sin and cos\n# Note: -dye_mod  used in plot because 0 is upper right corner here\ndxe_mod = nx = np.cos(edge_orientation[::K, ::K][edges[::K, ::K] &gt; 0])\ndye_mod = ny = np.sin(edge_orientation[::K, ::K][edges[::K, ::K] &gt; 0])\nplt.quiver(ex, ey, dxe_mod, -dye_mod, color='r') # at ex, ey location, plot dx, -dy\nplt.axis('off')\nplt.title('Recreated Normals')\nplt.show()\n\n\n# Edges and boundaries\nplt.figure()\nplt.subplot(2,2,1)\nplt.imshow(img.astype(np.uint8))\nplt.axis('off')\nplt.title('Input image')\n\n\nplt.subplot(2,2,2)\nplt.imshow(E+(edges == 0)[:, :, None])\nplt.axis('off')\nplt.title('Edges')\n\n\nplt.subplot(2,2,3)\nplt.imshow(1-(occlusion_edges&gt;0), cmap='gray')\nplt.axis('off')\nplt.title('Occlusion boundaries')\n\nplt.subplot(2,2,4)\nplt.imshow(1-contact_edges, cmap='gray')\nplt.axis('off')\nplt.title('Contact boundaries');"
  },
  {
    "objectID": "nb/cv24_pset1/pset1_2024.html#populate-edge-variables",
    "href": "nb/cv24_pset1/pset1_2024.html#populate-edge-variables",
    "title": "PSET 1",
    "section": "Populate edge variables",
    "text": "Populate edge variables\n\nNconstraints = nrows*ncols*20\nAij = np.zeros((3, 3, Nconstraints))\nb = np.zeros((Nconstraints, 1))\n\n#Indices and counters\nii = np.zeros((Nconstraints, 1))\njj = np.zeros((Nconstraints, 1))\nglobal c\nc = 0\n\n# These will always be updated with the current indices\ndef update_indices():\n  global c\n  ii[c] = i\n  jj[c] = j\n  c += 1\n\n# Create linear contraints\nfor i in range(1, nrows-1):\n  for j in range(1, ncols-1):\n    # Y = 0\n    if ground[i,j]:\n      Aij[:,:,c] = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]])\n      b[c]       = 0\n      update_indices()\n    else:\n      # Check if current neighborhood touches an edge\n      edgesum = np.sum(edges[i-1:i+2,j-1:j+2])\n      # Check if current neighborhood touches ground pixels\n      groundsum = np.sum(ground[i-1:i+2,j-1:j+2])\n      # Check if current neighborhood touches vertical pixels\n      verticalsum = np.sum(vertical_edges[i-1:i+2,j-1:j+2])\n      # Check if current neighborhood touches horizontal pixels\n      horizontalsum = np.sum(horizontal_edges[i-1:i+2,j-1:j+2])\n\n      ####################################################################\n      ### COMPLETE THE CODE BELOW (TODOs) AND COPY IT INTO YOUR REPORT ###\n      ####################################################################\n\n      # TODO: edge orientation (average of edge pixels in current neighborhood)\n      # Populate Aij, ii, jj, b, and c using theta, edge_orientation, and\n      # the constraint/transform matrices you derived in the written segment\n\n      # Contact edge: Y = 0\n      # Requires: a transform matrix\n      # TODO: Complete the code\n      if contact_edges[i, j]:\n        Aij[:,:,c] =\n        b[c]       =\n        update_indices()\n\n      # Vertical edge: dY/dy = 1/cos(theta)\n      # Requires: a transform matrix, theta\n      # The 1/8 is for normalization\n      # TODO: Complete the code\n      if verticalsum &gt; 0 and groundsum == 0:\n        Aij[:,:,c] =\n        b[c]       =\n        update_indices()\n\n      # Horizontal edge: dY/dt = 0\n      # Note: You'll have to express t using other variables\n      # Requires: a transform matrix, i, j, edge_orientation\n      # TODO: Complete the code\n      if horizontalsum &gt; 0 and groundsum == 0 and verticalsum == 0:\n        Aij[:,:,c] =\n        b[c]       =\n        update_indices()\n\n      # Second derivative = 0 (weighted by 0.1 to reduce constraint strength)\n      # Requires: multiple transform matrices\n      # TODO: Complete the code\n      if groundsum == 0:\n        Aij[:,:,c] =\n        b[c]       =\n        update_indices()\n\n        Aij[:,:,c] =\n        b[c]       =\n        update_indices()\n\n        Aij[:,:,c] =\n        b[c]       =\n        update_indices()\n\n      ####################################################################\n      ###################### STOP COPYING HERE ###########################\n      ####################################################################\n\n# Solve for constraints\nii = ii[:c]\njj = jj[:c]\nAij = Aij[:,:,:c]\nb = b[:c]\nA = sparseMatrix(ii, jj, Aij, nrows)\nY = sparseqr.solve( A, b)\n\n# Transform vector into image\nY = np.reshape(Y, [nrows, ncols], order='F')\n\n# Recover 3D world coordinates\nx, y = np.meshgrid(np.arange(ncols), np.arange(nrows))\nx = x.astype(np.float32)\ny = y.astype(np.float32)\nx -= nrows/2\ny -= ncols/2\n\n# Final coordinates\nX = x\nZ = Y*np.cos(theta)/np.sin(theta) - y/np.sin(theta)\nY = -Y\nY = np.maximum(Y, 0);\n\nE = occlusion_edges.astype(np.float32);\nE[E &gt; 0] = np.nan;\nZ = Z+E; #  remove occluded edges\n\n# Visualize solution\nplt.figure()\nplt.subplot(2,2,1)\nplt.imshow(img[1:-1, 1:-1].astype(np.uint8))\nplt.axis('off')\nplt.title('Edges')\n\nplt.subplot(2,2,2)\nplt.imshow(Z[1:-1, 1:-1], cmap='gray')\nplt.axis('off')\nplt.title('Z')\n\n\nplt.subplot(2,2,3)\nplt.imshow(Y[1:-1, 1:-1], cmap='gray', vmin=0, vmax=175)\nplt.axis('off')\nplt.title('Y')\n\nplt.subplot(2,2,4)\nplt.imshow(X[1:-1, 1:-1], cmap='gray')\nplt.axis('off')\nplt.title('X')\n\n# 3D projection\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.set_zlim(0, 175)\n\n# TODO for Problem 5\n# Rerun the script with at least two more of the provided images and for each\n# image try at least two different view angles.\n# Include the generated plots in your report (under Problem 5).\n# Note that we expect results to be quite brittle -- in answering Problem 6,\n# think about the strong assumptions that this approach makes.\n# We'll see more robust methods for similar problems later in the course\n\n# Specify here the angle you want to see\nax.view_init(20, -120)\nax.plot_surface(X,Z,Y, facecolors=img/255., shade=False)\n\n\nReference solution\nYours should render on a white background (note that the outline on Z is transparency) but be identical otherwise\nUploading the reference will be considered an honor code violation\n\nplt.imshow(cv2.cvtColor(cv2.imread('pset_1_reference.png'), cv2.COLOR_BGR2RGB))\nplt.show()"
  },
  {
    "objectID": "nb/Untitled.html",
    "href": "nb/Untitled.html",
    "title": "Computer Vision",
    "section": "",
    "text": "import cv2\n# Load images\n# Make sure to choose the same image format for both images (Ex- .png)\nA = cv2.imread('cat.png',cv2.IMREAD_COLOR) # high picture\nB = cv2.imread('panda.png',cv2.IMREAD_COLOR) # low picture\n\n\n\n# Convert both images to Grayscale to avoid any Color Channel Issue\nA_grayscale = cv2.cvtColor(A, cv2.COLOR_BGR2GRAY)\nB_grayscale = cv2.cvtColor(B, cv2.COLOR_BGR2GRAY)\n\n\nplt.imshow(A)\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt \n\n\n!ls\n\n1_intro_2024.pdf     cat.png              hybrid_image.jpg\nDolphinCarHybrid.jpg cv24_pset1           panda.png\nUntitled.ipynb       demo_simple_sys      pset2_2024"
  },
  {
    "objectID": "lab4.html",
    "href": "lab4.html",
    "title": "Lab 4: hybrid images",
    "section": "",
    "text": "Exercises\nImplement the code in this article.\n\n\nRecommended reading\n\nExample images https://github.com/ahmedwael19/Image-Filtering-and-Hybrid-Images-\nMore examples http://olivalab.mit.edu/hybrid_gallery/gallery.html\nOriginal paper http://olivalab.mit.edu/publications/OlivaTorralb_Hybrid_Siggraph06.pdf"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computer vision course",
    "section": "",
    "text": "Slides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides"
  },
  {
    "objectID": "index.html#lectures",
    "href": "index.html#lectures",
    "title": "Computer vision course",
    "section": "",
    "text": "Slides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides\n\n\n\nSlides"
  },
  {
    "objectID": "index.html#labs",
    "href": "index.html#labs",
    "title": "Computer vision course",
    "section": "Labs",
    "text": "Labs\n\nLab1: Getting started with images; basic manipulation\nLab1\n\n\nLab2: Image annotation/enhancement\nLab2\n\n\nLab3: Filtering with convolutions\nLab3\n\n\nLab4: hybrid images\nLab4\n\n\nLab5: Filters and image derivatives\nLab5"
  },
  {
    "objectID": "lab5.html",
    "href": "lab5.html",
    "title": "Lab 5: Image derivatives",
    "section": "",
    "text": "Smoothing (blur) is a low-pass filter:\n\nit filters out the ‚Äúhigh frequency‚Äù part of the image\nhelps in removing noise\n\n\n\n\nimport cv2 as cv\nimport numpy as np\nfrom matplotlib import pyplot as plt\n \nimg = cv.imread('img/cybertruck.png')\nassert img is not None, \"file could not be read, check with os.path.exists()\"\n \nblur = cv.blur(img,(15,15))\n \nplt.subplot(121),plt.imshow(img),plt.title('Original')\nplt.xticks([]), plt.yticks([])\nplt.subplot(122),plt.imshow(blur),plt.title('Blurred')\nplt.xticks([]), plt.yticks([])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBoth width and height of the kernel should be odd\n\n\n\ngblur = cv.GaussianBlur(img,(15,15),0)\nplt.subplot(121),plt.imshow(img),plt.title('Original')\nplt.xticks([]), plt.yticks([])\nplt.subplot(122),plt.imshow(gblur),plt.title('Gaussian-blurred')\nplt.xticks([]), plt.yticks([])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nUsed for noise reduction.\n\nnoisy_img = cv.imread('img/noisy_image.png')\nmedian = cv.medianBlur(noisy_img,15)\n\nplt.subplot(121),plt.imshow(noisy_img),plt.title('Original')\nplt.xticks([]), plt.yticks([])\nplt.subplot(122),plt.imshow(median),plt.title('Median-blurred')\nplt.xticks([]), plt.yticks([])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWhere d is Diameter of each pixel neighborhood that is used during filtering. If it is non-positive, it is computed from sigmaSpace. - used for noise removal - keeps edges sharp\nParameters:\n\nd: diameter of each pixel neighborhood\nsigmaColor is used to filter sigma in the color space.\nsigmaSpace is used to Filter sigma in the coordinate space\n\nMore info here and here.\n\nbilateral_blur = cv.bilateralFilter(img,90,75,75)\n\nplt.subplot(121),plt.imshow(img),plt.title('Original')\nplt.xticks([]), plt.yticks([])\nplt.subplot(122),plt.imshow(bilateral_blur),plt.title('Bilateral-blurred')\nplt.xticks([]), plt.yticks([])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nImage gradient is ‚Äúhigh-pass filter‚Äù:\n\nit filters out the low frequency part of the image in order to highlight the abrupt parts of the image\nused for edge detection\n\n\n\n\nthe image should be first converted to a single channel (grayscale)\nas Sobel kernels are susceptible to noise, we should blur the image\nthen apply a sharpening kernel\n\n\n\n\n\n\n\nKernel for Sobel\n\n\n\nSobel uses one-dimensional derivatives. It combines Gaussian smoothing and differentiation.\n\\[\nG_x =  \\begin{bmatrix}\n  1 & 0 & -1 \\\\\n\\end{bmatrix} \\circ \\begin{bmatrix}\n  1 \\\\\n  2 \\\\\n  1\n\\end{bmatrix} =\n\\begin{bmatrix}\n  1 & 0 & -1 \\\\\n  2 & 0 & -2 \\\\\n  1 & 0 & -1\n\\end{bmatrix}\n\\]\n\\[\nG_y =  \\begin{bmatrix}\n  -1 & -2 & -1 \\\\\n  0 & 0 & 0 \\\\\n  1 & 2 & 1\n\\end{bmatrix}\n\\tag{1}\\]\nAnd the overall gradient is: \\[\nG = \\sqrt{G_x^2 + G_y^2}\n\\]\n\n\nLaplacian uses two-dimensional derivatives. It calls internally the Sobel operator to perform its computation\n\n\n\n\n\n\nKernel for Laplacian (2D-approximation)\n\n\n\n\\[\n\\nabla_5^2 = \\begin{bmatrix}\n  0 & 1 & 0 \\\\\n  1 & -4 & 1\\\\\n  0 & 1 & 0\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\nimport cv2\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# loading image\nimg0 = cv2.imread('img/cybertruck.png',)\n\n# converting to gray scale\ngray = cv2.cvtColor(img0, cv2.COLOR_BGR2GRAY)\n\n# remove noise\nimg = cv2.GaussianBlur(gray,(3,3),0)\n\n# convolute with proper kernels\nlaplacian = cv2.Laplacian(img,cv2.CV_64F)\nsobelx = cv2.Sobel(img,cv2.CV_64F,1,0,ksize=5)  # x\nsobely = cv2.Sobel(img,cv2.CV_64F,0,1,ksize=5)  # y\n\nplt.subplot(2,2,1),plt.imshow(img,cmap = 'gray')\nplt.title('Original'), plt.xticks([]), plt.yticks([])\nplt.subplot(2,2,2),plt.imshow(laplacian,cmap = 'gray')\nplt.title('Laplacian'), plt.xticks([]), plt.yticks([])\nplt.subplot(2,2,3),plt.imshow(sobelx,cmap = 'gray')\nplt.title('Sobel X'), plt.xticks([]), plt.yticks([])\nplt.subplot(2,2,4),plt.imshow(sobely,cmap = 'gray')\nplt.title('Sobel Y'), plt.xticks([]), plt.yticks([])\n\nplt.show()"
  },
  {
    "objectID": "lab5.html#blur",
    "href": "lab5.html#blur",
    "title": "Lab 5: Image derivatives",
    "section": "",
    "text": "Smoothing (blur) is a low-pass filter:\n\nit filters out the ‚Äúhigh frequency‚Äù part of the image\nhelps in removing noise\n\n\n\n\nimport cv2 as cv\nimport numpy as np\nfrom matplotlib import pyplot as plt\n \nimg = cv.imread('img/cybertruck.png')\nassert img is not None, \"file could not be read, check with os.path.exists()\"\n \nblur = cv.blur(img,(15,15))\n \nplt.subplot(121),plt.imshow(img),plt.title('Original')\nplt.xticks([]), plt.yticks([])\nplt.subplot(122),plt.imshow(blur),plt.title('Blurred')\nplt.xticks([]), plt.yticks([])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBoth width and height of the kernel should be odd\n\n\n\ngblur = cv.GaussianBlur(img,(15,15),0)\nplt.subplot(121),plt.imshow(img),plt.title('Original')\nplt.xticks([]), plt.yticks([])\nplt.subplot(122),plt.imshow(gblur),plt.title('Gaussian-blurred')\nplt.xticks([]), plt.yticks([])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nUsed for noise reduction.\n\nnoisy_img = cv.imread('img/noisy_image.png')\nmedian = cv.medianBlur(noisy_img,15)\n\nplt.subplot(121),plt.imshow(noisy_img),plt.title('Original')\nplt.xticks([]), plt.yticks([])\nplt.subplot(122),plt.imshow(median),plt.title('Median-blurred')\nplt.xticks([]), plt.yticks([])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWhere d is Diameter of each pixel neighborhood that is used during filtering. If it is non-positive, it is computed from sigmaSpace. - used for noise removal - keeps edges sharp\nParameters:\n\nd: diameter of each pixel neighborhood\nsigmaColor is used to filter sigma in the color space.\nsigmaSpace is used to Filter sigma in the coordinate space\n\nMore info here and here.\n\nbilateral_blur = cv.bilateralFilter(img,90,75,75)\n\nplt.subplot(121),plt.imshow(img),plt.title('Original')\nplt.xticks([]), plt.yticks([])\nplt.subplot(122),plt.imshow(bilateral_blur),plt.title('Bilateral-blurred')\nplt.xticks([]), plt.yticks([])\nplt.show()"
  },
  {
    "objectID": "lab5.html#sharpening",
    "href": "lab5.html#sharpening",
    "title": "Lab 5: Image derivatives",
    "section": "",
    "text": "Image gradient is ‚Äúhigh-pass filter‚Äù:\n\nit filters out the low frequency part of the image in order to highlight the abrupt parts of the image\nused for edge detection\n\n\n\n\nthe image should be first converted to a single channel (grayscale)\nas Sobel kernels are susceptible to noise, we should blur the image\nthen apply a sharpening kernel\n\n\n\n\n\n\n\nKernel for Sobel\n\n\n\nSobel uses one-dimensional derivatives. It combines Gaussian smoothing and differentiation.\n\\[\nG_x =  \\begin{bmatrix}\n  1 & 0 & -1 \\\\\n\\end{bmatrix} \\circ \\begin{bmatrix}\n  1 \\\\\n  2 \\\\\n  1\n\\end{bmatrix} =\n\\begin{bmatrix}\n  1 & 0 & -1 \\\\\n  2 & 0 & -2 \\\\\n  1 & 0 & -1\n\\end{bmatrix}\n\\]\n\\[\nG_y =  \\begin{bmatrix}\n  -1 & -2 & -1 \\\\\n  0 & 0 & 0 \\\\\n  1 & 2 & 1\n\\end{bmatrix}\n\\tag{1}\\]\nAnd the overall gradient is: \\[\nG = \\sqrt{G_x^2 + G_y^2}\n\\]\n\n\nLaplacian uses two-dimensional derivatives. It calls internally the Sobel operator to perform its computation\n\n\n\n\n\n\nKernel for Laplacian (2D-approximation)\n\n\n\n\\[\n\\nabla_5^2 = \\begin{bmatrix}\n  0 & 1 & 0 \\\\\n  1 & -4 & 1\\\\\n  0 & 1 & 0\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\nimport cv2\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# loading image\nimg0 = cv2.imread('img/cybertruck.png',)\n\n# converting to gray scale\ngray = cv2.cvtColor(img0, cv2.COLOR_BGR2GRAY)\n\n# remove noise\nimg = cv2.GaussianBlur(gray,(3,3),0)\n\n# convolute with proper kernels\nlaplacian = cv2.Laplacian(img,cv2.CV_64F)\nsobelx = cv2.Sobel(img,cv2.CV_64F,1,0,ksize=5)  # x\nsobely = cv2.Sobel(img,cv2.CV_64F,0,1,ksize=5)  # y\n\nplt.subplot(2,2,1),plt.imshow(img,cmap = 'gray')\nplt.title('Original'), plt.xticks([]), plt.yticks([])\nplt.subplot(2,2,2),plt.imshow(laplacian,cmap = 'gray')\nplt.title('Laplacian'), plt.xticks([]), plt.yticks([])\nplt.subplot(2,2,3),plt.imshow(sobelx,cmap = 'gray')\nplt.title('Sobel X'), plt.xticks([]), plt.yticks([])\nplt.subplot(2,2,4),plt.imshow(sobely,cmap = 'gray')\nplt.title('Sobel Y'), plt.xticks([]), plt.yticks([])\n\nplt.show()"
  },
  {
    "objectID": "lab2.html",
    "href": "lab2.html",
    "title": "Lab 2",
    "section": "",
    "text": "Complete the following items in the course:\n\nImage Annotation.\nImage Enhancement."
  },
  {
    "objectID": "lec3.html#introduction",
    "href": "lec3.html#introduction",
    "title": "Filtering",
    "section": "Introduction",
    "text": "Introduction\n\n\n\nImportant\n\n\nHuman visual system is complex.\n\n\n\n\n\n\nTip\n\n\nWe have a fairly good idea of what happens at the initial stages of visual processing.\n\n\n\n\n\n\nNote\n\n\nWe will describe some mathematically simple processing that will help us to parse an image into useful tokens, or low-level features that will be useful later to construct visual interpretations."
  },
  {
    "objectID": "lec3.html#introduction-1",
    "href": "lec3.html#introduction-1",
    "title": "Filtering",
    "section": "Introduction",
    "text": "Introduction\n\n\n\nProcessing features\n\n\n\nenhance image structures of use for subsequent interpretation\nremove variability within the image that makes more difficult comparisons with previously learned visual signals.\n\n\n\n\n\n\n\nLinear filters\n\n\nThe simplest mathematical processing we can think of.\n\nnicely model some aspects of the processing carried our by early visual areas such as the retina, the lateral geniculate nucleus (LGN) and primary visual cortex (V1) @Hubel62."
  },
  {
    "objectID": "lec3.html#signals-and-images",
    "href": "lec3.html#signals-and-images",
    "title": "Filtering",
    "section": "Signals and Images",
    "text": "Signals and Images\n\n\n\nSignal\n\n\nA signal is a measurement of some physical quantity (light, sound, height, temperature, etc.) as a function of another independent quantity (time, space, wavelength, etc.).\n\n\n\n\n\n\nSystem\n\n\nA system is a process/function that transforms a signal into another."
  },
  {
    "objectID": "lec3.html#signals-and-images-1",
    "href": "lec3.html#signals-and-images-1",
    "title": "Filtering",
    "section": "Signals and Images",
    "text": "Signals and Images\nContinuous and Discrete Signals\n\nmost of the signals that exist in nature are infinite continuous signals\nhowever, when we introduce them into a computer they are sampled and transformed into a finite sequence of numbers, also called a discrete signal.\n\n\n\n\n\n\n\nNote\n\n\nSampling is the process of transforming a continuous signal into a discrete one."
  },
  {
    "objectID": "lec3.html#signals-and-images-2",
    "href": "lec3.html#signals-and-images-2",
    "title": "Filtering",
    "section": "Signals and Images",
    "text": "Signals and Images\n\n\n\nSignal representation\n\n\nIf we consider the light that reaches one camera photo sensor, we could write it as a one dimensional function of time, \\(\\ell(t)\\), where \\(\\ell(t)\\) denotes the incident \\(\\ell\\)ight at time \\(t\\), and \\(t\\) is a continuous variable that can take on any real value.\nThe signal \\(\\ell(t)\\) is then sampled in time (as is done in a video) by the camera where the values are only captured at discrete times (e.g., 30 times per second). In that case, the signal \\(\\ell\\) will be defined only on discrete time instants and we will write the sequence of measured values as: \\(\\ell\\left[ n \\right]\\), where \\(n\\) can only take on integer values."
  },
  {
    "objectID": "lec3.html#signals-and-images-3",
    "href": "lec3.html#signals-and-images-3",
    "title": "Filtering",
    "section": "Signals and Images",
    "text": "Signals and Images\n\n\n\nDiscrete-continuous Relationship\n\n\nThe relationship between the discrete and the continuous signals is given by the sampling equation: \\[\n\\ell\\left[n\\right] = \\ell\\left(n~\\Delta T \\right)\n\\] where \\(\\Delta T\\)is the sampling period.\nFor instance, \\(\\Delta T = 1/30\\)¬†s in the case of sampling the signal 30 times per second."
  },
  {
    "objectID": "lec3.html#signals-and-images-4",
    "href": "lec3.html#signals-and-images-4",
    "title": "Filtering",
    "section": "Signals and Images",
    "text": "Signals and Images\n\n\n\n\n\n\nNote\n\n\nAs is common in signal processing, we will use parenthesis to indicate continuous variables and brackets to denote discrete variables.\nFor instance, if we sample the signal shown in Figure¬†1 (a) once every second (\\(\\Delta T = 1\\)¬†s) we get the discrete signal shown in Figure¬†1 (b).\n\n\n\n\n\nFigure¬†1: Fig (a) A continuous signal, and (b) a discrete signal obtained by sampling the continuous signal at the times \\(t=n\\)."
  },
  {
    "objectID": "lec3.html#signals-and-images-5",
    "href": "lec3.html#signals-and-images-5",
    "title": "Filtering",
    "section": "Signals and Images",
    "text": "Signals and Images\n\n\n\nSignal values\n\n\nThe signal in Figure¬†1 (b) is a function that takes on the values \\(\\ell\\left[0\\right] = 3\\), \\(\\ell\\left[1\\right] = 2\\), \\(\\ell\\left[2\\right] = 1\\) and \\(\\ell\\left[3\\right] = 4\\) and all other values are zero, \\(\\ell\\left[n\\right] = 0\\). In most of the book we will work with discrete signals.\n\n\n\n\n\n\nVector notation\n\n\nIn many cases it will be convenient to write discrete signals as vectors. Using vector notation we will write the previous signal, in the interval \\(n \\in \\left[0,6 \\right],\\) as a column vector \\(\\boldsymbol\\ell= \\left[3, 2, 1, 4, 0, 0, 0\\right]^T\\), where \\(T\\) denotes transpose."
  },
  {
    "objectID": "lec3.html#signals-and-images-6",
    "href": "lec3.html#signals-and-images-6",
    "title": "Filtering",
    "section": "Signals and Images",
    "text": "Signals and Images\n\n\n\nImage\n\n\nAn image is a two dimensional array of values: \\[\n\\boldsymbol\\ell\\in \\mathbb{R}^{M \\times N},\n\\] where \\(N\\) is the image width and \\(M\\) is the image height."
  },
  {
    "objectID": "lec3.html#signals-and-images-7",
    "href": "lec3.html#signals-and-images-7",
    "title": "Filtering",
    "section": "Signals and Images",
    "text": "Signals and Images\n\n\n\nExample\n\n\nA grayscale image is then just an array of numbers such as the following (in this example, intensity values are scaled between 0 and 256):\n\\[\\begin{array}{cc}\n\\boldsymbol\\ell= &\n\\left[\n\\begin{smallmatrix} 160 & 175 & 171 & 168 & 168 & 172 & 164 & 158 & 167 & 173 & 167 & 163 & 162 & 164 & 160 & 159 & 163 & 162\\\\ 149 & 164 & 172 & 175 & 178 & 179 & 176 & 118 & 97 & 168 & 175 & 171 & 169 & 175 & 176 & 177 & 165 & 152\\\\ 161 & 166 & 182 & 171 & 170 & 177 & 175 & 116 & 109 & 169 & 177 & 173 & 168 & 175 & 175 & 159 & 153 & 123\\\\ 171 & 174 & 177 & 175 & 167 & 161 & 157 & 138 & 103 & 112 & 157 & 164 & 159 & 160 & 165 & 169 & 148 & 144\\\\ 163 & 163 & 162 & 165 & 167 & 164 & 178 & 167 & 77 & 55 & 134 & 170 & 167 & 162 & 164 & 175 & 168 & 160\\\\ 173 & 164 & 158 & 165 & 180 & 180 & 150 & 89 & 61 & 34 & 137 & 186 & 186 & 182 & 175 & 165 & 160 & 164\\\\ 152 & 155 & 146 & 147 & 169 & 180 & 163 & 51 & 24 & 32 & 119 & 163 & 175 & 182 & 181 & 162 & 148 & 153\\\\ 134 & 135 & 147 & 149 & 150 & 147 & 148 & 62 & 36 & 46 & 114 & 157 & 163 & 167 & 169 & 163 & 146 & 147\\\\ 135 & 132 & 131 & 125 & 115 & 129 & 132 & 74 & 54 & 41 & 104 & 156 & 152 & 156 & 164 & 156 & 141 & 144\\\\ 151 & 155 & 151 & 145 & 144 & 149 & 143 & 71 & 31 & 29 & 129 & 164 & 157 & 155 & 159 & 158 & 156 & 148\\\\ 172 & 174 & 178 & 177 & 177 & 181 & 174 & 54 & 21 & 29 & 136 & 190 & 180 & 179 & 176 & 184 & 187 & 182\\\\ 177 & 178 & 176 & 173 & 174 & 180 & 150 & 27 & 101 & 94 & 74 & 189 & 188 & 186 & 183 & 186 & 188 & 187\\\\ 160 & 160 & 163 & 163 & 161 & 167 & 100 & 45 & 169 & 166 & 59 & 136 & 184 & 176 & 175 & 177 & 185 & 186\\\\ 147 & 150 & 153 & 155 & 160 & 155 & 56 & 111 & 182 & 180 & 104 & 84 & 168 & 172 & 171 & 164 & 168 & 167\\\\ 184 & 182 & 178 & 175 & 179 & 133 & 86 & 191 & 201 & 204 & 191 & 79 & 172 & 220 & 217 & 205 & 209 & 200\\\\ 184 & 187 & 192 & 182 & 124 & 32 & 109 & 168 & 171 & 167 & 163 & 51 & 105 & 203 & 209 & 203 & 210 & 205\\\\ 191 & 198 & 203 & 197 & 175 & 149 & 169 & 189 & 190 & 173 & 160 & 145 & 156 & 202 & 199 & 201 & 205 & 202\\\\ 153 & 149 & 153 & 155 & 173 & 182 & 179 & 177 & 182 & 177 & 182 & 185 & 179 & 177 & 167 & 176 & 182 & 180 \\end{smallmatrix}\\right]\n\\end{array}\\]"
  },
  {
    "objectID": "lec3.html#signals-and-images-8",
    "href": "lec3.html#signals-and-images-8",
    "title": "Filtering",
    "section": "Signals and Images",
    "text": "Signals and Images\n\nGrayscale image showing a person walking in the street. This tiny image has only \\(18\\times18\\) pixels."
  },
  {
    "objectID": "lec3.html#signals-and-images-9",
    "href": "lec3.html#signals-and-images-9",
    "title": "Filtering",
    "section": "Signals and Images",
    "text": "Signals and Images\n\n\n\nPixel location\n\n\nWhen we want to make explicit the location of a pixel we will write \\(\\ell\\left[n, m \\right]\\), where \\(n \\in \\left[0,N-1 \\right]\\) and \\(m \\in \\left[0,M-1 \\right]\\) are the indices for the horizontal and vertical dimensions, respectively.\n\neach value in the array indicates the intensity of the image in that location.\nfor color images we will have three channels, one for each color."
  },
  {
    "objectID": "lec3.html#signals-and-images-10",
    "href": "lec3.html#signals-and-images-10",
    "title": "Filtering",
    "section": "Signals and Images",
    "text": "Signals and Images\n\n\n\nApproximation\n\n\nWorking on the continuous domain simplifies the derivation of analytical solutions. In those cases we will write images as \\[\n\\ell(x,y)\n\\] and video sequences as \\[\n\\ell(x,y,t)\n\\]."
  },
  {
    "objectID": "lec3.html#signal-properties",
    "href": "lec3.html#signal-properties",
    "title": "Filtering",
    "section": "Signal Properties",
    "text": "Signal Properties\n\n\n\nSignal length\n\n\nInfinite length signals are signals that extend over the entire support \\(\\ell\\left[n \\right]\\) for \\(n \\in (-\\infty, \\infty)\\).\nFinite length signals have non-zero values on a compact time interval and they are zero outside, i.e.¬†\\(\\ell\\left[n \\right] = 0\\) for \\(n \\notin S\\) where \\(S\\) is a finite length interval.\n\n\n\n\n\n\nPeriodicity\n\n\nA signal \\(\\ell\\left[n \\right]\\) is periodic if there is a value \\(N\\) such that \\(\\ell\\left[n \\right] = \\ell\\left[n + k N\\right]\\) for all \\(n\\) and \\(k\\). A periodic signal is an infinite length signal."
  },
  {
    "objectID": "lec3.html#signal-properties-1",
    "href": "lec3.html#signal-properties-1",
    "title": "Filtering",
    "section": "Signal Properties",
    "text": "Signal Properties\n\n\n\nMean value\n\n\nMean value of a signal often called the DC value.\nIn the case of an image, the DC component is the average intensity of the image.\nThe DC value is computed as \\[\n\\mu = \\begin{cases}\n\\frac{1}{N} \\sum_{n=0}^{N-1} \\ell\\left[n\\right], \\, \\text{finite case} \\\\\n\\lim_{N \\xrightarrow{} \\infty} \\frac{1}{2N+1} \\sum_{n=-N}^{N} \\ell\\left[n\\right], \\, \\text{infinite case}\n\\end{cases}\n\\]\n\n\n\n\n\nDC means direct current and it comes from electrical engineering. Although most signals have nothing to do with currents, the term DC is still commonly used."
  },
  {
    "objectID": "lec3.html#signal-properties-2",
    "href": "lec3.html#signal-properties-2",
    "title": "Filtering",
    "section": "Signal Properties",
    "text": "Signal Properties\n\n\n\nEnergy\n\n\nThe energy of a signal is defined as the sum of squared magnitude values: \\[\nE = \\sum_{n = -\\infty}^{\\infty} \\left| \\ell\\left[n\\right] \\right| ^2\n\\]\n\n\n\n\n\n\n\n\n\nNote\n\n\nSignal are further classified as finite energy and infinite energy signals\n\nfinite length signals are finite energy\nperiodic signals are infinite energy signals when measuring the energy in the whole time axis."
  },
  {
    "objectID": "lec3.html#signal-properties-3",
    "href": "lec3.html#signal-properties-3",
    "title": "Filtering",
    "section": "Signal Properties",
    "text": "Signal Properties\n\n\n\nComparison\n\n\nIf we want to compare two signals, we can use the squared Euclidean distance (squared \\(L_2\\) norm) between them: \\[\nD^2 = \\frac{1}{N} \\sum_{n=0}^{N-1} \\left| \\ell_1 \\left[n\\right] - \\ell_2 \\left[n\\right] \\right| ^2\n\\]\n\n\n\n\n\n\nWarning\n\n\nHowever, the euclidean distance (\\(L_2\\)) is a poor metric when we are interested in comparing the content of the two images and the building better metrics is an important area of research. Sometimes, the metric is \\(L_2\\) but in a different representation space than pixel values."
  },
  {
    "objectID": "lec3.html#signal-properties-4",
    "href": "lec3.html#signal-properties-4",
    "title": "Filtering",
    "section": "Signal Properties",
    "text": "Signal Properties\n\n\n\nContinuous signal properties\n\n\nAll the equations are analogous by replacing the sums with integrals.\nFor instance, in the case of the energy of a continuous signal, we can write\n\\[\nE = \\int_{-\\infty}^{\\infty} \\left| \\ell\\left( t \\right) \\right|^2 dt,\n\\] which assumes that the integral is finite.\nMost natural signals will have infinite energy."
  },
  {
    "objectID": "lec3.html#systems-1",
    "href": "lec3.html#systems-1",
    "title": "Filtering",
    "section": "Systems",
    "text": "Systems\n\n\n\nSystems flow\n\n\n\ninput: \\(\\boldsymbol\\ell_{\\texttt{in}}\\)\ncomputation: \\(f\\)\noutput: another signal \\(\\boldsymbol\\ell_{\\texttt{out}}= f(\\boldsymbol\\ell_{\\texttt{in}})\\).\n\n\n\n\n\n\nFigure¬†2: System processing one signal."
  },
  {
    "objectID": "lec3.html#systems-2",
    "href": "lec3.html#systems-2",
    "title": "Filtering",
    "section": "Systems",
    "text": "Systems\n\n\n\nWhat can this do?\n\n\nAll kinds of things:\n\ndetect edges in images\nrecognize objects\ndetect motion in sequences\napply aesthetic transformations to a picture.\n\n\n\n\n\n\nThe function \\(f\\) could be specified as a mathematical operation or as an algorithm. As a consequence, it is very difficult to find a simple way of characterizing what the function \\(f\\) does"
  },
  {
    "objectID": "lec3.html#linear-systems",
    "href": "lec3.html#linear-systems",
    "title": "Filtering",
    "section": "Linear Systems",
    "text": "Linear Systems\n\n\n\nLinear systems\n\n\nA function \\(f\\) is linear is it satisfies the following two properties:\n\\[\\begin{aligned}\n& 1. \\; f\\left( \\boldsymbol\\ell_1+\\boldsymbol\\ell_2 \\right) = f(\\boldsymbol\\ell_1)+ f(\\boldsymbol\\ell_2) \\\\ \\nonumber\n& 2. \\; f(a\\boldsymbol\\ell) = af(\\boldsymbol\\ell) ~~\\text{for any scalar} ~ a\n\\end{aligned}\\]"
  },
  {
    "objectID": "lec3.html#systems-3",
    "href": "lec3.html#systems-3",
    "title": "Filtering",
    "section": "Systems",
    "text": "Systems\n\n\n\nWhich operations are linear?\n\n\nWhich ones of the image transformations can be written as linear systems?\n\n\n\n\n\n\nFigure¬†3"
  },
  {
    "objectID": "lec3.html#systems-4",
    "href": "lec3.html#systems-4",
    "title": "Filtering",
    "section": "Systems",
    "text": "Systems\n\n\n\nLinear system: inputs/outputs\n\n\n\ninput: a 1D signal with length \\(N\\) that we will write as \\(\\ell_{\\texttt{in}}\\left[n \\right]\\),\noutput another 1D signal with length \\(M\\) that we will write as \\(\\ell_{\\texttt{out}}\\left[n \\right]\\).\n\nMost of the times we will work with input and output pairs with the same length \\(M=N\\).\n\n\n\n\n\n\nLinear system: definition\n\n\nA linear system, in its most general form, can be written as follows:\n\\[\\ell_{\\texttt{out}}\\left[n \\right] = \\sum_{k=0}^{N-1} h \\left[n,k\\right] \\ell_{\\texttt{in}}\\left[k \\right] ~~~ for ~~~ n \\in \\left[0, M-1 \\right]\n\\qquad(1)\\]"
  },
  {
    "objectID": "lec3.html#systems-5",
    "href": "lec3.html#systems-5",
    "title": "Filtering",
    "section": "Systems",
    "text": "Systems\n\n\n\nLinear system: properties\n\n\n\neach output value \\(\\ell_{\\texttt{out}}\\left[n \\right]\\) is a linear combination of values of the input signal \\(\\ell_{\\texttt{in}}\\left[n \\right]\\) with weights \\(h \\left[n,k\\right]\\)\nthe value \\(h \\left[n,k\\right]\\) is the weight applied to the input sample \\(\\ell_{\\texttt{in}}\\left[k \\right]\\) to compute the output sample \\(\\ell_{\\texttt{out}}\\left[n \\right]\\)."
  },
  {
    "objectID": "lec3.html#systems-6",
    "href": "lec3.html#systems-6",
    "title": "Filtering",
    "section": "Systems",
    "text": "Systems\n\n\n\nMatrix form\n\n\n\\[\\begin{bmatrix}\\ell_{\\texttt{out}}\\left[ 0 \\right] \\\\\n\\ell_{\\texttt{out}}\\left[ 1 \\right] \\\\\n\\vdots \\\\\n\\ell_{\\texttt{out}}\\left[ n \\right] \\\\\n\\vdots \\\\\n\\ell_{\\texttt{out}}\\left[ M-1 \\right]\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n  h\\left[0,0\\right] ~&~ h\\left[0,1\\right] ~&~ ... ~&~ h\\left[0,N-1\\right] \\\\\n  h\\left[1,0\\right] ~&~ h\\left[1,1\\right] ~&~ ... ~&~ h\\left[1,N-1\\right] \\\\\n  \\vdots ~&~  \\vdots ~&~  \\vdots ~&~  \\vdots \\\\\n  \\vdots ~&~  \\vdots ~&~  h \\left[n,k\\right] ~&~  \\vdots \\\\\n\\vdots ~&~  \\vdots ~&~  \\vdots ~&~  \\vdots \\\\\n  h\\left[M-1,0\\right] ~&~ h\\left[M-1,1\\right] ~&~ ... ~&~ h\\left[M-1,N-1\\right]\n\\end{bmatrix}\n~\n\\begin{bmatrix}\n  \\ell_{\\texttt{in}}\\left[0\\right] \\\\\n  \\ell_{\\texttt{in}}\\left[1\\right] \\\\\n  \\vdots \\\\\n  \\ell_{\\texttt{in}}\\left[k\\right] \\\\\n  \\vdots \\\\\n  \\ell_{\\texttt{in}}\\left[N-1\\right]\n\\end{bmatrix}\\]\n\n\n\n\n\n\nShort form\n\n\n\\[\n\\boldsymbol\\ell_{\\texttt{out}}=  \\mathbf{H} \\boldsymbol\\ell_{\\texttt{in}}.\n\\]\nThe matrix \\(\\mathbf{H}\\) has size \\(M \\times N\\)."
  },
  {
    "objectID": "lec3.html#systems-7",
    "href": "lec3.html#systems-7",
    "title": "Filtering",
    "section": "Systems",
    "text": "Systems\n\n\n\nLinear function as a neural network\n\n\nA linear function can also be drawn as a fully connected layer in a neural network, with weights \\(\\mathbf{W} = \\mathbf{H}\\), where the output unit \\(i\\), \\(\\ell_{\\texttt{out}}\\left[i \\right]\\), is a linear combination of the input signal \\(\\boldsymbol\\ell_{\\texttt{in}}\\) with weights given by the row vectors of \\(\\mathbf{H}\\). Graphically it looks like this:\n\n\n\n\n\n\nFigure¬†4: A linear function drawn as a fully connected layer in a neural network."
  },
  {
    "objectID": "lec3.html#systems-8",
    "href": "lec3.html#systems-8",
    "title": "Filtering",
    "section": "Systems",
    "text": "Systems\n\n\n\n2D case\n\n\nIn two dimensions the equations are analogous. Each pixel of the output image, \\(\\ell_{\\texttt{out}}\\left[n,m\\right]\\), is computed as a linear combination of pixels of the input image, \\(\\ell_{\\texttt{in}}\\left[n, m\\right]\\):\n\\[\n\\ell_{\\texttt{out}}\\left[n,m \\right] = \\sum_{k=0}^{M-1} \\sum_{l=0}^{N-1} h \\left[n,m,k,l \\right] \\ell_{\\texttt{in}}\\left[k,l \\right]\n\\qquad(2)\\]\nBy writing the images as column vectors, concatenating all the image columns into a long vector, we can also write the previous equation using matrices and vectors: \\(\\boldsymbol\\ell_{\\texttt{out}}=  \\mathbf{H} \\boldsymbol\\ell_{\\texttt{in}}\\)."
  },
  {
    "objectID": "lec3.html#linear-translation-invariant-systems",
    "href": "lec3.html#linear-translation-invariant-systems",
    "title": "Filtering",
    "section": "Linear Translation Invariant Systems",
    "text": "Linear Translation Invariant Systems\n\n\n\nLinear translation invariant (LTI) systems"
  },
  {
    "objectID": "lec3.html#linear-translation-invariant-systems-1",
    "href": "lec3.html#linear-translation-invariant-systems-1",
    "title": "Filtering",
    "section": "Linear Translation Invariant Systems",
    "text": "Linear Translation Invariant Systems\n\n\n\nMotivation\n\n\nWe don‚Äôt know where within the image we expect to find any given item, so we often want to process the image in a spatially invariant manner, the same processing algorithm at every pixel.\n\n\n\nA fundamental property of images is translation invariance‚Äìthe same image may appear at arbitrary spatial positions within the image. Source: Fredo Durand."
  },
  {
    "objectID": "lec3.html#linear-translation-invariant-systems-2",
    "href": "lec3.html#linear-translation-invariant-systems-2",
    "title": "Filtering",
    "section": "Linear Translation Invariant Systems",
    "text": "Linear Translation Invariant Systems\n\n\n\nExample\n\n\nAn example of a translation invariant system is a function that takes as input an image and computes at each location a local average value of the pixels around in a window of \\(5 \\times 5\\) pixels:\n\\[\\ell_{\\texttt{out}}\\left[n,m \\right] = \\frac{1}{25} \\sum_{k=-2}^{2} \\sum_{l=-2}^{2} \\ell_{\\texttt{in}}\\left[n+k,m+l \\right]\\]"
  },
  {
    "objectID": "lec3.html#linear-translation-invariant-systems-3",
    "href": "lec3.html#linear-translation-invariant-systems-3",
    "title": "Filtering",
    "section": "Linear Translation Invariant Systems",
    "text": "Linear Translation Invariant Systems\n\n\n\nDefinition\n\n\nA system is an LTI system if it is linear and when we translate the input signal by \\(n_0, m_0\\), then output is also translated by \\(n_0, m_0\\): \\[\n\\ell_{\\texttt{out}}\\left[ n - n_0, m-m_0 \\right] = f \\left( \\ell_{\\texttt{in}}\\left[ n-n_0, m-m_0 \\right] \\right)\n\\] for any \\(n_0,m_0\\). This property is called equivariant with respect to translation."
  },
  {
    "objectID": "lec3.html#convolution",
    "href": "lec3.html#convolution",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nDefinition\n\n\nThe convolution, denoted \\(\\circ\\), between a signal \\(\\ell_{\\texttt{in}}\\left[n \\right]\\) and the convolutional kernel \\(h\\left[n \\right]\\) is defined as follows: \\[\n\\ell_{\\texttt{out}}\\left[n\\right] = h \\left[n\\right] \\circ \\ell_{\\texttt{in}}\\left[n\\right] =  \\sum_{k=-\\infty}^{\\infty} h \\left[n-k \\right] \\ell_{\\texttt{in}}\\left[k \\right]\n\\qquad(3)\\]\nIf the signal \\(\\ell_{\\texttt{in}}\\left[n \\right]\\) has a finite length, \\(N\\), then the sum is only done in the interval \\(k \\in \\left[0,N-1\\right]\\).\n\n\n\n\n\n\nWeight\n\n\nThe convolution computes the output values as a linear weighted sum. The weight between the input sample \\(\\ell_{\\texttt{in}}\\left[k \\right]\\) and the output sample \\(\\ell_{\\texttt{out}}\\left[n\\right]\\) is a function \\(h\\left[n, k \\right]\\) that depends only on their relative position \\(n-k\\). Therefore, \\(h\\left[n, k \\right]=h\\left[n-k \\right]\\)."
  },
  {
    "objectID": "lec3.html#convolution-1",
    "href": "lec3.html#convolution-1",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nCorrelation\n\n\nThe convolution is related to the correlation operator. In the correlation, the weights are defined as \\(h\\left[n, k \\right]=h\\left[k-n \\right]\\). The convolution and the correlation use the same kernel but mirrored around the origin. The correlation is also translation invariant as we will discuss at the end of the chapter."
  },
  {
    "objectID": "lec3.html#convolution-2",
    "href": "lec3.html#convolution-2",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nCommutativity\n\n\nOne important property of the convolution is that it is commutative: \\[\nh\\left[n\\right] \\circ \\ell\\left[n\\right] = \\ell\\left[n\\right] \\circ h\\left[n\\right].\n\\]\nThis property is easy to prove by changing the variables, \\(k = n - k'\\), so that:\n\\[h \\left[n\\right] \\circ \\ell_{\\texttt{in}}\\left[n\\right] =\\sum_{k=-\\infty}^{\\infty} h \\left[n-k \\right] \\ell_{\\texttt{in}}\\left[k \\right] = \\sum_{k'=-\\infty}^{\\infty} h \\left[k' \\right] \\ell_{\\texttt{in}}\\left[n-k' \\right]  =\n\\ell_{\\texttt{in}}\\left[n\\right] \\circ h \\left[n\\right]\\]"
  },
  {
    "objectID": "lec3.html#convolution-3",
    "href": "lec3.html#convolution-3",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nTextual description\n\n\n\nTake the kernel \\(h\\left[k\\right]\\) and mirror it \\(h\\left[-k \\right]\\)\nShift the mirrored kernel so that the origin is at location \\(n\\)\nMultiply the input values around location \\(n\\) by the mirrored kernel and sum the result.\nStore the result in the output vector \\(\\ell_{\\texttt{out}}\\left[n\\right]\\).\n\nFor instance, if the convolutional kernel has three non-zero values: \\[h\\left[-1 \\right] =1, ~ h\\left[0 \\right] = 2, ~ h\\left[1 \\right] = 3,\n\\] then the output value at location \\(n\\) is \\[\\ell_{\\texttt{out}}\\left[n\\right] = 3 \\ell_{\\texttt{in}}\\left[n-1\\right]+2 \\ell_{\\texttt{in}}\\left[n\\right] + 1 \\ell_{\\texttt{in}}\\left[n+1\\right].\\]"
  },
  {
    "objectID": "lec3.html#convolution-4",
    "href": "lec3.html#convolution-4",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nMatrix form\n\n\nIf the convolutional kernel has only three non-zero values, then:\n\\[\\begin{bmatrix}\n\\ell_{\\texttt{out}}\\left[ 0 \\right] \\\\\n\\ell_{\\texttt{out}}\\left[ 1 \\right] \\\\\n\\ell_{\\texttt{out}}\\left[ 2 \\right] \\\\\n\\vdots \\\\\n\\ell_{\\texttt{out}}\\left[ N-1 \\right]\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n  h\\left[0\\right] ~&~ h\\left[-1\\right] ~&~ 0 ~&~ ... ~&~ 0 \\\\\n  h\\left[1\\right] ~&~ h\\left[0\\right] ~&~ h\\left[-1\\right] ~&~... ~&~ 0 \\\\\n  0 ~&~ h\\left[1\\right] ~&~ h\\left[0\\right] ~&~... ~&~ 0 \\\\\n  \\vdots ~&~  \\vdots ~&~  \\vdots ~&~  \\vdots \\\\\n  0 ~&~ 0 ~&~ 0 ~&~... ~&~ h\\left[0\\right]\n\\end{bmatrix}\n~\n\\begin{bmatrix}\n  \\ell_{\\texttt{in}}\\left[0\\right] \\\\\n  \\ell_{\\texttt{in}}\\left[1\\right] \\\\\n  \\ell_{\\texttt{in}}\\left[2\\right] \\\\\n  \\vdots \\\\\n  \\ell_{\\texttt{in}}\\left[N-1\\right]\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "lec3.html#convolution-5",
    "href": "lec3.html#convolution-5",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nNeural network representation\n\n\n\n\n\n\n\n\nFigure¬†5: convolution represented as a layer in a neural network. Each output unit \\(i\\), \\(\\ell_{\\text {out }}[i]\\), is a linear combination of the input signal values around location \\(i\\) always using the same set of weights given by \\(\\mathbf{h}\\)."
  },
  {
    "objectID": "lec3.html#convolution-6",
    "href": "lec3.html#convolution-6",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\n2D case\n\n\n\nthe input filter is flipped vertically and horizontally\nthen slid over the image to record the inner product with the image everywhere\n\n\\[\n\\ell_{\\texttt{out}}\\left[n,m\\right] = h \\left[n,m\\right] \\circ \\ell_{\\texttt{in}}\\left[n,m\\right] =  \\sum_{k,l}h \\left[n-k,m-l \\right] \\ell_{\\texttt{in}}\\left[k,l \\right]\n\\qquad(4)\\]"
  },
  {
    "objectID": "lec3.html#convolution-7",
    "href": "lec3.html#convolution-7",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\nFigure¬†6: Illustration of a 2D convolution of an \\(9 \\times 9\\) input image with a kernel of size \\(3 \\times 3\\).For the pixels in the boundary we assumed that input image has zero values outside its boundary. The red and green boxes show the input pixels used to compute the corresponding output pixels."
  },
  {
    "objectID": "lec3.html#convolution-8",
    "href": "lec3.html#convolution-8",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nWhich operations are convolutions?"
  },
  {
    "objectID": "lec3.html#convolution-9",
    "href": "lec3.html#convolution-9",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\nFigure¬†7: Fig (a) Defocusing an image can be written as a convolution. (b) Rotation can‚Äôt be written as a convolution: it‚Äôs not translation invariant."
  },
  {
    "objectID": "lec3.html#convolution-10",
    "href": "lec3.html#convolution-10",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nProperties\n\n\n\nCommutative. As we have already discussed before the convolution is commutative, \\[h\\left[n\\right] \\circ \\ell\\left[n\\right] = \\ell\\left[n\\right] \\circ h\\left[n\\right]\\] which means that the order of convolutions is irrelevant. This is not true for the correlation.\nAssociative. Convolutions can be applied in any order: \\[\\ell_1 \\left[n\\right] \\circ \\ell_2 \\left[n\\right] \\circ \\ell_3 \\left[n\\right] = \\ell_1\\left[n\\right] \\circ (\\ell_2\\left[n\\right] \\circ \\ell_3\\left[n\\right]) = (\\ell_1\\left[n\\right] \\circ \\ell_2\\left[n\\right]) \\circ \\ell_3\\left[n\\right] \\qquad(5)\\] In practice, for finite length signals, the associative property might be affected by how boundary conditions are implemented."
  },
  {
    "objectID": "lec3.html#convolution-11",
    "href": "lec3.html#convolution-11",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nProperties\n\n\n\nDistributive with respect to the sum: \\[\\ell_1\\left[n\\right] \\circ (\\ell_2\\left[n\\right] + \\ell_3\\left[n\\right] ) = \\ell_1\\left[n\\right] \\circ \\ell_2\\left[n\\right] +  \\ell_1 \\left[n\\right] \\circ \\ell_3 \\left[n\\right]\\]\nShift. Another interesting property involves shifting the two convolved functions. Let‚Äôs consider the convolution \\(\\ell_{\\texttt{out}}\\left[n\\right] = h\\left[n\\right] \\circ \\ell_{\\texttt{in}}\\left[n\\right]\\). If the input signal, \\(\\ell_{\\texttt{in}}\\left[n\\right]\\), is translated by a constant \\(n_0\\), i.e. \\(\\ell_{\\texttt{in}}\\left[n - n_0\\right]\\), the result of the convolution with the same kernel, \\(h\\left[n\\right]\\), is the same output as before but translated by the same constant \\(n_0\\): \\[\\ell_{\\texttt{out}}\\left[n-n_0\\right] =  h\\left[n\\right] \\circ \\ell_{\\texttt{in}}\\left[n-n_0\\right]\\] Translating the kernel is equivalent: \\[\\ell_{\\texttt{out}}\\left[n-n_0\\right] =  h\\left[n\\right] \\circ \\ell_{\\texttt{in}}\\left[n-n_0\\right]  = h\\left[n-n_0\\right] \\circ \\ell_{\\texttt{in}}\\left[n\\right]\\]"
  },
  {
    "objectID": "lec3.html#convolution-12",
    "href": "lec3.html#convolution-12",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nProperties\n\n\n\nSupport. The convolution of a discrete signal of length \\(N\\) with another discrete signal of length \\(M\\) results in a discrete signal with length \\(L \\leq M+N-1\\).\nIdentity. The convolution also has an identity function, that is the impulse, \\(\\delta \\left[n\\right]\\), which takes the value 1 for \\(n=0\\) and it is zero everywhere else: \\[\\delta \\left[n\\right] =\n  \\begin{cases}\n    1       & \\quad \\text{if } n=0\\\\\n    0       & \\quad \\text{if } n \\neq 0 \\\\\n  \\end{cases}\\] The convolution of the impulse with any other signal, \\(\\ell\\left[n\\right]\\), returns the same signal: \\[\\delta \\left[n\\right] \\circ \\ell\\left[n\\right] = \\ell\\left[n\\right]\\]"
  },
  {
    "objectID": "lec3.html#convolution-13",
    "href": "lec3.html#convolution-13",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\nGiven the signal:\n\nThe signal \\(\\ell \\left[n - n_0\\right]\\) is a shifted version of \\(\\ell \\left[n\\right]\\). With \\(n_0 = 2\\) this is\n\n\n\nThe impulse function is"
  },
  {
    "objectID": "lec3.html#convolution-14",
    "href": "lec3.html#convolution-14",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\nFigure¬†8: Fig a) An impulse convolved with the input image gives no change, kernel \\(\\delta \\left[n,m\\right]\\). (b) A shifted impulse shifts the image, kernel \\(\\delta \\left[n,m-2\\right]\\). (c) Sum of two shifted copies of the image, kernel \\(0.5 \\delta \\left[n-2,m-2\\right]+ 0.5 \\delta \\left[n+2,m+2\\right]\\). (d) Image defocusing by computing a local average over windows of \\(5 \\times 5\\) pixels, uniform kernel of all \\(1/25\\). All the examples use zero padding for handling boundary conditions."
  },
  {
    "objectID": "lec3.html#convolution-15",
    "href": "lec3.html#convolution-15",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\nHandling Boundaries\n\nOnly the green region contains values that can be computed, the rest will be affected by how we decide to handle the boundaries:"
  },
  {
    "objectID": "lec3.html#convolution-16",
    "href": "lec3.html#convolution-16",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nBoundary handling: extension\n\n\nFor a kernel with support \\[\n\\left[ -N, N \\right] \\times \\left[-M, M\\right],\n\\] one has to add \\(N/2\\) additional pixels left and right of the image and \\(M/2\\) pixels at the top and bottom.\nThen, the output will have the same size as the original input image."
  },
  {
    "objectID": "lec3.html#convolution-17",
    "href": "lec3.html#convolution-17",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nBoundary handling types\n\n\n\nZero padding: Set the pixels outside the boundary to zero (or to some other constant such as the mean image value). This is the default option in most neural networks.\nCircular padding: Extend the image by replicating the pixels from the other side. If the image has size \\(P \\times Q\\), circular padding consists of the following: \\[\\ell_{\\texttt{in}}\\left[n,m\\right] = \\ell_{\\texttt{in}}\\left[(n)_P,(m)_Q\\right]\\] where \\((n)_P\\) denotes the modulo operation and \\((n)_P\\) is the reminder of \\(n/P\\).\nThis padding transform the finite length signal into a periodic infinite length signal. Although this will introduce many artifacts, it is a convenient extension for analytical derivations."
  },
  {
    "objectID": "lec3.html#convolution-18",
    "href": "lec3.html#convolution-18",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nTip\n\n\n\nMirror padding: Reflect the valid image pixels over the boundary of valid output pixels. This is the most common approach and the one that gives the best results.\nRepeat padding: Set the value to that of the nearest output image pixel with valid mask inputs."
  },
  {
    "objectID": "lec3.html#convolution-19",
    "href": "lec3.html#convolution-19",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\nFigure¬†9: (a) Different types of boundary extension. (b) The output of convolving the image with a uniform kernel of size \\(11 \\times 11\\) with all the values equal to \\(1/121\\). (c) The difference between each output and the ground truth output; see last column of (b)."
  },
  {
    "objectID": "lec3.html#convolution-20",
    "href": "lec3.html#convolution-20",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nCircular Convolution\n\n\nSpecial case: the circular convolution uses circular padding.\nThe circular convolution of two signals, \\(h\\) and \\(\\ell_{\\texttt{in}}\\), of length \\(N\\) is defined as follows:\n\\[\n\\ell_{\\texttt{out}}\\left[n\\right] = h \\left[n\\right] \\circ_N \\ell_{\\texttt{in}}\\left[n\\right] =  \\sum_{k=0}^{N-1} h \\left[(n-k)_N \\right] \\ell_{\\texttt{in}}\\left[k \\right]\n\\]\n\n\n\n\n\n\nProperties\n\n\nIn this formulation, the signal \\(h \\left[ (n)_N \\right]\\) is the infinite periodic extension, with period \\(N\\), of the finite length signal \\(h \\left[n \\right]\\). The output of the circular convolution is a periodic signal with period \\(N\\), \\(\\ell_{\\texttt{out}}\\left[n \\right] = \\ell_{\\texttt{out}}\\left[(n)_N \\right]\\).\nThe circular convolution has the same properties than the convolution (e.g., is commutative.)"
  },
  {
    "objectID": "lec3.html#convolution-21",
    "href": "lec3.html#convolution-21",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nCircular Confolution: Matrix Form"
  },
  {
    "objectID": "lec3.html#convolution-22",
    "href": "lec3.html#convolution-22",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nConvolution in the Continuous Domain\n\n\nGiven two continuous signals \\(h(t)\\) and \\(\\ell_{\\texttt{in}}(t)\\), the convolution operator is defined as follows: \\[\n\\ell_{\\texttt{out}}(t) = h(t) \\circ \\ell_{\\texttt{in}}(t) = \\int_{-\\infty}^{\\infty} h(t-\\tau) \\ell_{\\texttt{in}}(\\tau) d\\tau\n\\]\nThe properties of the continuous domain convolution are analogous to the properties of the discrete convolution, with the commutative, associative and distributive relationships still holding."
  },
  {
    "objectID": "lec3.html#convolution-23",
    "href": "lec3.html#convolution-23",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nContinuous impulse\n\n\nWe can also define the impulse, \\(\\delta (t)\\), in the continuous domain such that \\(\\ell(t) \\circ \\delta(t) = \\ell(t)\\).\nThe impulse is defined as being zero everywhere but at the origin where it takes an infinitely high value so that its integral is equal to 1: \\[\n\\int_{-\\infty}^{\\infty} \\delta(t) dt = 1\n\\] The impulse function is also called the impulse distribution (as it is not a function in the strict sense) or the Dirac delta function."
  },
  {
    "objectID": "lec3.html#convolution-24",
    "href": "lec3.html#convolution-24",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\nThe delta distribution is usually represented with an arrow of height 1, indicating that it has an finite value at that point, and a finite area equal to 1:"
  },
  {
    "objectID": "lec3.html#convolution-25",
    "href": "lec3.html#convolution-25",
    "title": "Filtering",
    "section": "Convolution",
    "text": "Convolution\n\n\n\nImpulse properties\n\n\n\nScaling property: \\(\\delta (at) =  \\delta (t) / |a|\\)\nSymmetry: \\(\\delta (-t) = \\delta (t)\\)\nSampling property: \\(\\ell(t) \\delta (t-a) = \\ell(a) \\delta (t-a)\\) where \\(a\\) is a constant. From this, we have the following: \\[\\int_{-\\infty}^{\\infty} \\ell(t) \\delta (t-a) dt = \\ell(a)\\]\nAs in the discrete case, the continuous impulse is also the identity for the convolution: \\(\\ell(t) \\circ \\delta (t) = \\ell(t)\\)."
  },
  {
    "objectID": "lec3.html#cross-correlation-versus-convolution",
    "href": "lec3.html#cross-correlation-versus-convolution",
    "title": "Filtering",
    "section": "Cross-Correlation Versus Convolution",
    "text": "Cross-Correlation Versus Convolution\n\n\n\nCross-correlation\n\n\nThe cross-correlation (denoted by \\(\\star\\)) between the image \\(\\ell_{\\texttt{in}}\\) and the kernel \\(h\\): \\[\n\\ell_{\\texttt{out}}\\left[n,m\\right] = \\ell_{\\texttt{in}}\\star h =  \\sum_{k,l=-N}^N \\ell_{\\texttt{in}}\\left[n+k,m+l \\right] h \\left[k,l \\right]\n\\] where the sum is done over the support of the filter \\(h \\equiv (-N,N)\\times(-N,N)\\)."
  },
  {
    "objectID": "lec3.html#cross-correlation-versus-convolution-1",
    "href": "lec3.html#cross-correlation-versus-convolution-1",
    "title": "Filtering",
    "section": "Cross-Correlation Versus Convolution",
    "text": "Cross-Correlation Versus Convolution\n\n\n\nCompared to convolution\n\n\nIn the convolution, the kernel is inverted left-right and up-down, while in the cross-correlation is not. Remember that the convolution between image \\(\\ell_{\\texttt{in}}\\) and kernel \\(h\\): \\[\n\\ell_{\\texttt{out}}\\left[n,m\\right] = \\ell_{\\texttt{in}}\\circ h = \\sum_{k,l=-N}^N \\ell_{\\texttt{in}}\\left[n-k,m-l \\right] h \\left[k,l \\right]\n\\]\n\nthe cross-correlation provides a simple technique to locate a template in an image.\nconvolution is commutative and associative while the correlation is not. The correlation breaks the symmetry between the two functions \\(h\\) and \\(\\ell_{\\texttt{in}}\\). For instance, in the correlation, shifting \\(h\\) is not equivalent to shifting \\(\\ell_{\\texttt{in}}\\) (the output will move in opposite directions)."
  },
  {
    "objectID": "lec3.html#cross-correlation-versus-convolution-2",
    "href": "lec3.html#cross-correlation-versus-convolution-2",
    "title": "Filtering",
    "section": "Cross-Correlation Versus Convolution",
    "text": "Cross-Correlation Versus Convolution\n\n\nFigure¬†10: Cross-correlation versus convolution. (a) Kernel. (b) and (e) are two input images. (c) and (f) are the output convolution with the kernel (a). (d) and (g) are the cross-correlation output with the kernel (a)."
  },
  {
    "objectID": "lec3.html#cross-correlation-versus-convolution-3",
    "href": "lec3.html#cross-correlation-versus-convolution-3",
    "title": "Filtering",
    "section": "Cross-Correlation Versus Convolution",
    "text": "Cross-Correlation Versus Convolution\n\n\n\nCorrelation for template matching\n\n\n\n\n\n\n\n\nFigure¬†11: Fig (a) Template. (b) Input image. (c) Correlation between input (b) and template (a). (d) Normalized correlation. e) Locations with cross-correlation above 75 percent of its maximum value."
  },
  {
    "objectID": "lec3.html#cross-correlation-versus-convolution-4",
    "href": "lec3.html#cross-correlation-versus-convolution-4",
    "title": "Filtering",
    "section": "Cross-Correlation Versus Convolution",
    "text": "Cross-Correlation Versus Convolution\n\n\n\nNormalized cross-correlation\n\n\nThe normalized cross-correlation, at location \\((n,m)\\), is the dot product between a template, \\(h\\), (normalized to be zero mean and unit norm) and the local image patch, \\(p\\), centered at that location and with the same size as the kernel, normalized to be unit norm. This can be implemented with the following steps:\n\nNormalize the kernel \\(h\\). The normalized kernel \\(\\hat h\\) is the result of making the kernel \\(h\\) zero mean and unit norm.\nThe normalized cross-correlation is as follows (Figure¬†11 (d)):\n\n\\[\n\\ell_{\\texttt{out}}\\left[n,m\\right]\n                =\n                \\frac{1}{\\sigma \\left[n,m \\right]}\n                \\sum_{k,l=-N}^N\n                \\ell_{\\texttt{in}}\\left[n+k,m+l \\right]\n                \\hat{h} \\left[k,l \\right]\n\\]"
  },
  {
    "objectID": "lec3.html#cross-correlation-versus-convolution-5",
    "href": "lec3.html#cross-correlation-versus-convolution-5",
    "title": "Filtering",
    "section": "Cross-Correlation Versus Convolution",
    "text": "Cross-Correlation Versus Convolution\n\n\n\nNormalized cross-correlation: computation\n\n\nNote that this equation is similar to the correlation function, but the output is scaled by the local standard deviation, \\(\\sigma \\left[m,n \\right]\\), of the image patch centered in \\((n,m)\\) and with support \\((-N,N)\\times(-N,N)\\), where \\((-N,N)\\times(-N,N)\\) is the size of the kernel \\(h\\). To compute the local standard deviation, we first compute the local mean, \\(\\mu \\left[n,m \\right]\\), as follows: \\[\n\\mu \\left[n,m \\right] = \\frac{1}{(2N+1)^2} \\sum_{k,l=-N}^N \\ell_{\\texttt{in}}\\left[n+k,m+l \\right]\n\\] And then we compute: \\[\n\\sigma^2 \\left[n,m \\right] = \\frac{1}{(2N+1)^2} \\sum_{k,l=-N}^N \\left( \\ell_{\\texttt{in}}\\left[n+k,m+l \\right] - \\mu \\left[n,m \\right] \\right) ^2\n\\]"
  },
  {
    "objectID": "lec3.html#cross-correlation-versus-convolution-6",
    "href": "lec3.html#cross-correlation-versus-convolution-6",
    "title": "Filtering",
    "section": "Cross-Correlation Versus Convolution",
    "text": "Cross-Correlation Versus Convolution\n\n\n\nNormalized cross-correlation: features\n\n\n\nwill detect the object independently of location,\nnot robust to changes such as:\n\nrotation\nscaling\nappearance"
  },
  {
    "objectID": "lec3.html#system-identification",
    "href": "lec3.html#system-identification",
    "title": "Filtering",
    "section": "System Identification",
    "text": "System Identification\nOne important task that we will study is that of explaining the behavior of complex systems such as neural networks. For an LTI filter with kernel \\(h\\left[n\\right]\\), the output from an impulse is \\(y\\left[n\\right] = h\\left[n\\right]\\). The output of a translated impulse \\(\\delta \\left[n -n_0 \\right]\\) is \\(h \\left[n -n_0 \\right]\\). This is why the kernel that defines an LTI system, \\(h\\left[n\\right]\\), is also called the impulse response of the system."
  },
  {
    "objectID": "lab1.html",
    "href": "lab1.html",
    "title": "Lab 1",
    "section": "",
    "text": "Please enroll at OpenCV intro course.\nComplete the following items in the course:\n2.1. Getting Started with Images.\n2.2. Basic Image Manipulation.\nPractice cropping/resizing on a couple of images downloaded from the net."
  },
  {
    "objectID": "lec1.html#euclid",
    "href": "lec1.html#euclid",
    "title": "Computer vision: intro",
    "section": "Euclid",
    "text": "Euclid\nEuclid (ca. 300 BCE): natural perspective: ray OP joining the center of projection O to the point P."
  },
  {
    "objectID": "lec1.html#aristotle",
    "href": "lec1.html#aristotle",
    "title": "Computer vision: intro",
    "section": "Aristotle",
    "text": "Aristotle\nThought that eyes are emitting vision (emission theory)."
  },
  {
    "objectID": "lec1.html#medieval",
    "href": "lec1.html#medieval",
    "title": "Computer vision: intro",
    "section": "Medieval",
    "text": "Medieval\n·∏§asan Ibn al-Haytham (Alhazen) - father of modern optics"
  },
  {
    "objectID": "lec1.html#renaissance",
    "href": "lec1.html#renaissance",
    "title": "Computer vision: intro",
    "section": "Renaissance",
    "text": "Renaissance\nLeon Battista Alberti\n\n\n\nA truly universal genius (Jacob Burckhardt, The Civilization of the Renaissance in Italy)"
  },
  {
    "objectID": "lec1.html#descartes",
    "href": "lec1.html#descartes",
    "title": "Computer vision: intro",
    "section": "Descartes",
    "text": "Descartes\nCamera eye"
  },
  {
    "objectID": "lec1.html#earlier-technologies",
    "href": "lec1.html#earlier-technologies",
    "title": "Computer vision: intro",
    "section": "Earlier technologies",
    "text": "Earlier technologies\n\n\n\ncamera obscura\nthe stereoscope\nfilm photography"
  },
  {
    "objectID": "lec1.html#tank-detector",
    "href": "lec1.html#tank-detector",
    "title": "Computer vision: intro",
    "section": "Tank Detector",
    "text": "Tank Detector\n\n\n\nRecognition system design by statistical analysis (https://dl.acm.org/doi/pdf/10.1145/800257.808903)"
  },
  {
    "objectID": "lec1.html#facial-recognition",
    "href": "lec1.html#facial-recognition",
    "title": "Computer vision: intro",
    "section": "Facial recognition",
    "text": "Facial recognition\nWoody Bledsoe, Charles Bisson and Helen Chan: facial recognition for military (1964)"
  },
  {
    "objectID": "lec1.html#summer-vision-project",
    "href": "lec1.html#summer-vision-project",
    "title": "Computer vision: intro",
    "section": "Summer Vision project",
    "text": "Summer Vision project\n\n\n\n\n\nSeymour Papert: https://dspace.mit.edu/bitstream/handle/1721.1/6125/AIM-100.pdf"
  },
  {
    "objectID": "lec1.html#summer-vision-project-1",
    "href": "lec1.html#summer-vision-project-1",
    "title": "Computer vision: intro",
    "section": "Summer Vision project",
    "text": "Summer Vision project\nWhat was the plan? Divide and conquer\nSplit teams doing different tasks:\n\n\nwriting a program to detect edges, corners, and other pixel-level information\nforming continous shapes out of these low-level features\narranging the shapes in three-dimensional space\netc."
  },
  {
    "objectID": "lec1.html#perceptron",
    "href": "lec1.html#perceptron",
    "title": "Computer vision: intro",
    "section": "Perceptron",
    "text": "Perceptron\nFrank Rosenblatt"
  },
  {
    "objectID": "lec1.html#generalized-cylinders",
    "href": "lec1.html#generalized-cylinders",
    "title": "Computer vision: intro",
    "section": "Generalized cylinders",
    "text": "Generalized cylinders\nT.O. Binford (1970)\n\n\n\nApplied in Rodney Brooks‚Äô ACRONYM (1981) - CIA aircraft detection"
  },
  {
    "objectID": "lec1.html#deformable-templates",
    "href": "lec1.html#deformable-templates",
    "title": "Computer vision: intro",
    "section": "Deformable templates",
    "text": "Deformable templates\nMartin Fischler and Robert Elschlager (1972)"
  },
  {
    "objectID": "lec1.html#mobots",
    "href": "lec1.html#mobots",
    "title": "Computer vision: intro",
    "section": "Mobots",
    "text": "Mobots\nRodney Brooks (1987): perception as action"
  },
  {
    "objectID": "lec1.html#neocognitron",
    "href": "lec1.html#neocognitron",
    "title": "Computer vision: intro",
    "section": "Neocognitron",
    "text": "Neocognitron\n\n\n\nNeocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position (https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf)"
  },
  {
    "objectID": "lec1.html#lecun-cnn-paper",
    "href": "lec1.html#lecun-cnn-paper",
    "title": "Computer vision: intro",
    "section": "LeCun CNN paper",
    "text": "LeCun CNN paper\n\n\n\n\n\n\n\n\nHandwritten Digit Recognition with a Back-Propagation Network (https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf)"
  },
  {
    "objectID": "lec1.html#what-is-vision",
    "href": "lec1.html#what-is-vision",
    "title": "Computer vision: intro",
    "section": "What is vision?",
    "text": "What is vision?\nVision is a perceptual channel that accepts a stimulus and reports some representation of the world."
  },
  {
    "objectID": "lec1.html#problem",
    "href": "lec1.html#problem",
    "title": "Computer vision: intro",
    "section": "Problem",
    "text": "Problem\nAzriel Rosenfeld, Picture Processing by Computer (1969)\n\nIf we want to give our computers eyes, we must first give them an education in the facts of life."
  },
  {
    "objectID": "lec1.html#sensing-types",
    "href": "lec1.html#sensing-types",
    "title": "Computer vision: intro",
    "section": "Sensing types",
    "text": "Sensing types\n\n\nPassive\nNot sending out light to see.\n\nActive\nSending out a signal and sensing a reflection"
  },
  {
    "objectID": "lec1.html#active-sensing-examples",
    "href": "lec1.html#active-sensing-examples",
    "title": "Computer vision: intro",
    "section": "Active sensing examples",
    "text": "Active sensing examples\n\nbats (ultrasound),\ndolphins (sound)\nabyssal fishes (light)\nsome robots (light, sound, radar)"
  },
  {
    "objectID": "lec1.html#features",
    "href": "lec1.html#features",
    "title": "Computer vision: intro",
    "section": "Features",
    "text": "Features\nA feature is a number obtained by applying simple computations to an image. Very useful information can be obtained directly from features.\nFeature extraction: simple, direct computations applied to sensor responses."
  },
  {
    "objectID": "lec1.html#model-based-approach",
    "href": "lec1.html#model-based-approach",
    "title": "Computer vision: intro",
    "section": "Model-based approach",
    "text": "Model-based approach\nTwo kinds of models:\n\nobject model: precise and geometric\nrendering model: describes the physical, geometric, and statistical processes that produce the stimulus"
  },
  {
    "objectID": "lec1.html#core-problems",
    "href": "lec1.html#core-problems",
    "title": "Computer vision: intro",
    "section": "Core problems",
    "text": "Core problems\nThe two core problems of computer vision are\n\nreconstruction\nrecognition"
  },
  {
    "objectID": "lec1.html#reconstruction",
    "href": "lec1.html#reconstruction",
    "title": "Computer vision: intro",
    "section": "Reconstruction",
    "text": "Reconstruction\nAn agent builds a model of the world from an image(s)"
  },
  {
    "objectID": "lec1.html#recognition",
    "href": "lec1.html#recognition",
    "title": "Computer vision: intro",
    "section": "Recognition",
    "text": "Recognition\nAgent draws distinctions among the objects it encounters based on visual and other information"
  },
  {
    "objectID": "lec1.html#goals",
    "href": "lec1.html#goals",
    "title": "Computer vision: intro",
    "section": "Goals",
    "text": "Goals\nThe goal of vision is to extract information needed for tasks such as:\n\nmanipulation\nnavigation\nobject recognition"
  },
  {
    "objectID": "lec1.html#computer-vision-vs-graphics",
    "href": "lec1.html#computer-vision-vs-graphics",
    "title": "Computer vision: intro",
    "section": "Computer Vision vs Graphics",
    "text": "Computer Vision vs Graphics\n\n\nVision\nEmphasis on analyzing images \n\nGraphics\nEmphasis on creating images"
  },
  {
    "objectID": "lec1.html#challenge",
    "href": "lec1.html#challenge",
    "title": "Computer vision: intro",
    "section": "Challenge",
    "text": "Challenge\nGeometry distortion"
  },
  {
    "objectID": "lec1.html#challenge-1",
    "href": "lec1.html#challenge-1",
    "title": "Computer vision: intro",
    "section": "Challenge",
    "text": "Challenge\nIllumination effects"
  },
  {
    "objectID": "lec1.html#challenge-2",
    "href": "lec1.html#challenge-2",
    "title": "Computer vision: intro",
    "section": "Challenge",
    "text": "Challenge\nAppearance variation"
  },
  {
    "objectID": "lec1.html#aside-on-cameras",
    "href": "lec1.html#aside-on-cameras",
    "title": "Computer vision: intro",
    "section": "Aside on cameras",
    "text": "Aside on cameras\nPinhole camera"
  },
  {
    "objectID": "lec1.html#aside-on-cameras-1",
    "href": "lec1.html#aside-on-cameras-1",
    "title": "Computer vision: intro",
    "section": "Aside on cameras",
    "text": "Aside on cameras\nLens camera"
  },
  {
    "objectID": "lec1.html#aside-on-cameras-2",
    "href": "lec1.html#aside-on-cameras-2",
    "title": "Computer vision: intro",
    "section": "Aside on cameras",
    "text": "Aside on cameras\nPhone camera"
  },
  {
    "objectID": "lec1.html#image-properties",
    "href": "lec1.html#image-properties",
    "title": "Computer vision: intro",
    "section": "Image properties",
    "text": "Image properties\nFour general properties of images and video\n\nedges\ntexture\noptical flow\nsegmentation into regions"
  },
  {
    "objectID": "lec1.html#edges",
    "href": "lec1.html#edges",
    "title": "Computer vision: intro",
    "section": "Edges",
    "text": "Edges\n\n\ndepth discontinuities\nsurface orientation discontinuities\nreflectance discontinuities\nillumination discontinuities (shadows)"
  },
  {
    "objectID": "lec1.html#texture",
    "href": "lec1.html#texture",
    "title": "Computer vision: intro",
    "section": "Texture",
    "text": "Texture"
  },
  {
    "objectID": "lec1.html#optical-flow",
    "href": "lec1.html#optical-flow",
    "title": "Computer vision: intro",
    "section": "Optical flow",
    "text": "Optical flow"
  },
  {
    "objectID": "lec1.html#segmentation",
    "href": "lec1.html#segmentation",
    "title": "Computer vision: intro",
    "section": "Segmentation",
    "text": "Segmentation"
  },
  {
    "objectID": "lec1.html#applications",
    "href": "lec1.html#applications",
    "title": "Computer vision: intro",
    "section": "Applications",
    "text": "Applications\n\n\nunderstanding human actions\ncaptioning\ngeometry reconstruction\nimage transformation\nmovement control"
  },
  {
    "objectID": "lec1.html#augmented-reality",
    "href": "lec1.html#augmented-reality",
    "title": "Computer vision: intro",
    "section": "Augmented Reality",
    "text": "Augmented Reality"
  },
  {
    "objectID": "lec1.html#augmented-reality-1",
    "href": "lec1.html#augmented-reality-1",
    "title": "Computer vision: intro",
    "section": "Augmented Reality",
    "text": "Augmented Reality"
  },
  {
    "objectID": "lec1.html#augmented-reality-2",
    "href": "lec1.html#augmented-reality-2",
    "title": "Computer vision: intro",
    "section": "Augmented Reality",
    "text": "Augmented Reality"
  },
  {
    "objectID": "lec1.html#autonomous-driving",
    "href": "lec1.html#autonomous-driving",
    "title": "Computer vision: intro",
    "section": "Autonomous driving",
    "text": "Autonomous driving\n\n\nobject detection and lane recognition\nadaptive cruise control\nreal-time environmental perception and decision-making"
  },
  {
    "objectID": "lec1.html#opencv",
    "href": "lec1.html#opencv",
    "title": "Computer vision: intro",
    "section": "OpenCV",
    "text": "OpenCV\n\nwhen?: at Intel in 1999.\ngoal: democratize computer vision"
  },
  {
    "objectID": "lec1.html#architecture",
    "href": "lec1.html#architecture",
    "title": "Computer vision: intro",
    "section": "Architecture",
    "text": "Architecture"
  },
  {
    "objectID": "lec1.html#opencv-modules",
    "href": "lec1.html#opencv-modules",
    "title": "Computer vision: intro",
    "section": "OpenCV modules",
    "text": "OpenCV modules"
  },
  {
    "objectID": "lec1.html#features-1",
    "href": "lec1.html#features-1",
    "title": "Computer vision: intro",
    "section": "Features",
    "text": "Features\nThe library has more than 2500 optimized algorithms for:\n\nface recognition\nobject identification\nhuman action classification\nobject tracking\n3D model extraction\naugmented reality"
  },
  {
    "objectID": "lec1.html#where-is-opencv-5",
    "href": "lec1.html#where-is-opencv-5",
    "title": "Computer vision: intro",
    "section": "Where is OpenCV 5?",
    "text": "Where is OpenCV 5?\n\nWill the future be open? Or will our algorithms be lost in time, like tears in rain?\n\n\nhttps://opencv.org/blog/where-is-opencv-5"
  },
  {
    "objectID": "lec1.html#yolo",
    "href": "lec1.html#yolo",
    "title": "Computer vision: intro",
    "section": "YOLO",
    "text": "YOLO\nA single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes \n\n\n\nYou Only Look Once: Unified, Real-Time Object Detection (https://arxiv.org/pdf/1506.02640)"
  },
  {
    "objectID": "lec1.html#yolo-model",
    "href": "lec1.html#yolo-model",
    "title": "Computer vision: intro",
    "section": "YOLO model",
    "text": "YOLO model\nDetection as a regression problem"
  },
  {
    "objectID": "lec1.html#yolo-architecture",
    "href": "lec1.html#yolo-architecture",
    "title": "Computer vision: intro",
    "section": "YOLO architecture",
    "text": "YOLO architecture"
  },
  {
    "objectID": "lec1.html#detrs",
    "href": "lec1.html#detrs",
    "title": "Computer vision: intro",
    "section": "DETRs",
    "text": "DETRs\nEnd-to-end Transformer-based detectors (DETRs)\n\n\n\nEnd-to-End Object Detection with Transformers (https://arxiv.org/pdf/2005.12872)"
  },
  {
    "objectID": "lec1.html#rt-detr",
    "href": "lec1.html#rt-detr",
    "title": "Computer vision: intro",
    "section": "RT-DETR",
    "text": "RT-DETR"
  },
  {
    "objectID": "lec1.html#segment-anything",
    "href": "lec1.html#segment-anything",
    "title": "Computer vision: intro",
    "section": "Segment Anything",
    "text": "Segment Anything\nFoundation model for image segmentation  \n\n\nSegment Anything (https://arxiv.org/pdf/2304.02643)"
  },
  {
    "objectID": "lec1.html#segment-anything-1",
    "href": "lec1.html#segment-anything-1",
    "title": "Computer vision: intro",
    "section": "Segment Anything",
    "text": "Segment Anything"
  },
  {
    "objectID": "lec1.html#natural-language-supervision",
    "href": "lec1.html#natural-language-supervision",
    "title": "Computer vision: intro",
    "section": "Natural Language Supervision",
    "text": "Natural Language Supervision\n\n\n\nLearning Transferable Visual Models From Natural Language Supervision (https://arxiv.org/pdf/2103.00020)"
  },
  {
    "objectID": "lec1.html#basic-image-operations",
    "href": "lec1.html#basic-image-operations",
    "title": "Computer vision: intro",
    "section": "Basic image operations",
    "text": "Basic image operations\nReading\n\nPython codeExample\n\n\n\nretval = cv2.imread( filename[, flags] )\n\n\n\n\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimg=cv2.imread(\"img/yolo_detection_system.png\", cv2.IMREAD_GRAYSCALE)\n\n#Displaying image using plt.imshow() method\nplt.imshow(img)\n\n\n\n\n\n\n\nFigure¬†1: Image reading example\n\n\n\n\n\n\n\n\nWriting\n\nPython codeExample\n\n\n\ncv2.imwrite( filename, img[, params] )\n\n\n\n\ncv2.imwrite(\"new_file.jpg\", img)\n\nTrue\n\n\n\n\n\nDimensions\n\n\nprint(\"Image size (H, W, C) is:\", img.shape)\n\nImage size (H, W, C) is: (466, 2140)"
  },
  {
    "objectID": "lec1.html#image-cropping",
    "href": "lec1.html#image-cropping",
    "title": "Computer vision: intro",
    "section": "Image cropping",
    "text": "Image cropping\n\ncropped = img[100:300, 500:800]\nplt.imshow(cropped)"
  },
  {
    "objectID": "lec1.html#image-resizing-rotation",
    "href": "lec1.html#image-resizing-rotation",
    "title": "Computer vision: intro",
    "section": "Image resizing / rotation",
    "text": "Image resizing / rotation\nResizing\n\nPython codeExample\n\n\n\ndst = resize( src, dsize[, dst[, fx[, fy[, interpolation]]]] )\n\n\n\n\nresized = cv2.resize(img, None, fx=0.1, fy=0.1)\nplt.imshow(resized)\n\n\n\n\n\n\n\n\n\n\n\nRotation\n\nPython codeExample\n\n\n\ndst = cv.flip( src, flipCode )\n\n\n\n\nrotated = cv2.flip(img, 0)\nplt.imshow(rotated)\n\n\n\n\n\n\n\n\n\nrotated = cv2.flip(img, 1)\nplt.imshow(rotated)\n\n\n\n\n\n\n\n\n\nrotated = cv2.flip(img, -1)\nplt.imshow(rotated)"
  },
  {
    "objectID": "lec1.html#image-annotation",
    "href": "lec1.html#image-annotation",
    "title": "Computer vision: intro",
    "section": "Image annotation",
    "text": "Image annotation\n\nPython codeExample\n\n\n\nimg = cv2.line(img, pt1, pt2, color[, thickness[, lineType[, shift]]])\nimg = cv2.circle(img, center, radius, color[, thickness[, lineType[, shift]]])\nimg = cv2.rectangle(img, pt1, pt2, color[, thickness[, lineType[, shift]]])\n\n\n\n\nannotated = cv2.line(img, (200, 100), (800, 100), (0, 255, 255), thickness=50, lineType=cv2.LINE_AA);\n\nplt.imshow(annotated)\n\n\n\n\n\n\n\n\n\nannotated2 = cv2.circle(annotated, (200,200), 100, (0, 0, 255), thickness=20, lineType=cv2.LINE_AA);\nplt.imshow(annotated2)"
  },
  {
    "objectID": "lec1.html#adding-text",
    "href": "lec1.html#adding-text",
    "title": "Computer vision: intro",
    "section": "Adding text",
    "text": "Adding text\n\nPython codeExample\n\n\n\nimg = cv2.putText(img, text, org, fontFace, fontScale, color[, thickness[, lineType[, bottomLeftOrigin]]])\n\n\n\n\nimageText = img.copy()\ntext = \"Random text\"\nfontScale = 5.3\nfontFace = cv2.FONT_HERSHEY_PLAIN\nfontColor = (0, 255, 0)\nfontThickness = 5\n\ncv2.putText(imageText, text, (100, 300), fontFace, fontScale, fontColor, fontThickness, cv2.LINE_AA);\n\n# Display the image\nplt.imshow(imageText)"
  },
  {
    "objectID": "lec1.html#image-thresholding",
    "href": "lec1.html#image-thresholding",
    "title": "Computer vision: intro",
    "section": "Image Thresholding",
    "text": "Image Thresholding\n\nPython codeExample\n\n\n\nretval, dst = cv2.threshold( src, thresh, maxval, type[, dst] )\n\n\n\n\nretval, img_thresh = cv2.threshold(img, 100, 255, cv2.THRESH_BINARY)\n\n# Show the images\nplt.figure(figsize=[18, 5])\n\nplt.subplot(121);plt.imshow(img, cmap=\"gray\");  plt.title(\"Original\")\nplt.subplot(122);plt.imshow(img_thresh, cmap=\"gray\");plt.title(\"Thresholded\")\n\nprint(img_thresh.shape)\n\n(466, 2140)"
  },
  {
    "objectID": "lec1.html#haar-cascade-classifiers",
    "href": "lec1.html#haar-cascade-classifiers",
    "title": "Computer vision: intro",
    "section": "Haar cascade classifiers",
    "text": "Haar cascade classifiers\nA Haar feature is essentially calculations that are performed on adjacent rectangular regions at a specific location in a detection window."
  },
  {
    "objectID": "lec1.html#code-example",
    "href": "lec1.html#code-example",
    "title": "Computer vision: intro",
    "section": "Code example",
    "text": "Code example\n\nPreparationExecution\n\n\n\n# Load the Haar Cascade Classifier\nface_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n\n# Read the image\nimg = cv2.imread('diverse_faces.jpg')\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# Detect faces\nfaces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n\n# Draw rectangles around faces\nfor (x, y, w, h) in faces:\n    cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\nplt.imshow(img)"
  },
  {
    "objectID": "lec1.html#pose-estimation",
    "href": "lec1.html#pose-estimation",
    "title": "Computer vision: intro",
    "section": "Pose estimation",
    "text": "Pose estimation"
  },
  {
    "objectID": "lec1.html#pose-estimation-1",
    "href": "lec1.html#pose-estimation-1",
    "title": "Computer vision: intro",
    "section": "Pose estimation",
    "text": "Pose estimation\n\nimport os\nfrom IPython.display import YouTubeVideo, display, Image\n\nprotoFile   = \"pose_deploy_linevec_faster_4_stages.prototxt\"\nweightsFile = os.path.join(\"model\", \"pose_iter_160000.caffemodel\")\nnet = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)\nim = cv2.imread(\"jump.png\") #\"Tiger_Woods_crop.png\")\nim = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n\ninWidth  = im.shape[1]\ninHeight = im.shape[0]\n\nnPoints = 15\nPOSE_PAIRS = [\n    [0, 1],\n    [1, 2],\n    [2, 3],\n    [3, 4],\n    [1, 5],\n    [5, 6],\n    [6, 7],\n    [1, 14],\n    [14, 8],\n    [8, 9],\n    [9, 10],\n    [14, 11],\n    [11, 12],\n    [12, 13],\n]\n\nnetInputSize = (368, 368)\ninpBlob = cv2.dnn.blobFromImage(im, 1.0 / 255, netInputSize, (0, 0, 0), swapRB=True, crop=False)\nnet.setInput(inpBlob)\n\n# Forward Pass\noutput = net.forward()\n\n# Display probability maps\nplt.figure(figsize=(20, 5))\nfor i in range(nPoints):\n    probMap = output[0, i, :, :]\n    displayMap = cv2.resize(probMap, (inWidth, inHeight), cv2.INTER_LINEAR)\n\n    plt.subplot(2, 8, i + 1)\n    plt.axis(\"off\")\n    plt.imshow(displayMap, cmap=\"jet\")"
  },
  {
    "objectID": "lec1.html#pose-estimation-2",
    "href": "lec1.html#pose-estimation-2",
    "title": "Computer vision: intro",
    "section": "Pose estimation",
    "text": "Pose estimation\n\n# X and Y Scale\nscaleX = inWidth  / output.shape[3]\nscaleY = inHeight / output.shape[2]\n\n# Empty list to store the detected keypoints\npoints = []\n\n# Treshold\nthreshold = 0.1\n\nfor i in range(nPoints):\n    # Obtain probability map\n    probMap = output[0, i, :, :]\n\n    # Find global maxima of the probMap.\n    minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n\n    # Scale the point to fit on the original image\n    x = scaleX * point[0]\n    y = scaleY * point[1]\n\n    if prob &gt; threshold:\n        # Add the point to the list if the probability is greater than the threshold\n        points.append((int(x), int(y)))\n    else:\n        points.append(None)\n\nimPoints = im.copy()\nimSkeleton = im.copy()\n\n# Draw points\nfor i, p in enumerate(points):\n    cv2.circle(imPoints, p, 8, (255, 255, 0), thickness=-1, lineType=cv2.FILLED)\n    cv2.putText(imPoints, \"{}\".format(i), p, cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, lineType=cv2.LINE_AA)\n\n# Draw skeleton\nfor pair in POSE_PAIRS:\n    partA = pair[0]\n    partB = pair[1]\n\n    if points[partA] and points[partB]:\n        cv2.line(imSkeleton, points[partA], points[partB], (255, 255, 0), 2)\n        cv2.circle(imSkeleton, points[partA], 8, (255, 0, 0), thickness=-1, lineType=cv2.FILLED)\n\nplt.figure() #figsize=(50, 50))\n\nplt.subplot(121)\nplt.axis(\"off\")\nplt.imshow(imPoints)\n\nplt.subplot(122)\nplt.axis(\"off\")\nplt.imshow(imSkeleton)"
  },
  {
    "objectID": "lec1.html#east",
    "href": "lec1.html#east",
    "title": "Computer vision: intro",
    "section": "EAST",
    "text": "EAST\n\n\n\nEAST: An Efficient and Accurate Scene Text Detector"
  },
  {
    "objectID": "lec1.html#east-1",
    "href": "lec1.html#east-1",
    "title": "Computer vision: intro",
    "section": "EAST",
    "text": "EAST"
  },
  {
    "objectID": "lec1.html#east-2",
    "href": "lec1.html#east-2",
    "title": "Computer vision: intro",
    "section": "EAST",
    "text": "EAST\n\n# load the input image and grab the image dimensions\nimage = cv2.imread(\"img/chocolate.png\")\norig = image.copy()\n(H, W) = image.shape[:2]\n\nwidth = 320\nheight = 320\n# set the new width and height and then determine the ratio in change\n# for both the width and height\n(newW, newH) = (width, height)\nrW = W / float(newW)\nrH = H / float(newH)\n\n# resize the image and grab the new image dimensions\nimage = cv2.resize(image, (newW, newH))\n(H, W) = image.shape[:2]"
  },
  {
    "objectID": "lec1.html#east-3",
    "href": "lec1.html#east-3",
    "title": "Computer vision: intro",
    "section": "EAST",
    "text": "EAST\n\n# define the two output layer names for the EAST detector model that\n# we are interested -- the first is the output probabilities and the\n# second can be used to derive the bounding box coordinates of text\nlayerNames = [\n    \"feature_fusion/Conv_7/Sigmoid\",\n    \"feature_fusion/concat_3\"]\n\n# load the pre-trained EAST text detector\nprint(\"[INFO] loading EAST text detector...\")\nnet = cv2.dnn.readNet(\"frozen_east_text_detection.pb\")\n\n# construct a blob from the image and then perform a forward pass of\n# the model to obtain the two output layer sets\nblob = cv2.dnn.blobFromImage(image, 1.0, (W, H),\n                             (123.68, 116.78, 103.94), swapRB=True, crop=False)\n\n[INFO] loading EAST text detector..."
  },
  {
    "objectID": "lec1.html#east-4",
    "href": "lec1.html#east-4",
    "title": "Computer vision: intro",
    "section": "EAST",
    "text": "EAST\n\nimport time\nfrom imutils.object_detection import non_max_suppression\n\n\nconfidence = 0.5\n\nstart = time.time()\nnet.setInput(blob)\n(scores, geometry) = net.forward(layerNames)\nend = time.time()\n\n# show timing information on text prediction\nprint(\"[INFO] text detection took {:.6f} seconds\".format(end - start))\n\n# grab the number of rows and columns from the scores volume, then\n# initialize our set of bounding box rectangles and corresponding\n# confidence scores\n(numRows, numCols) = scores.shape[2:4]\nrects = []\nconfidences = []\n\n[INFO] text detection took 0.099429 seconds"
  },
  {
    "objectID": "lec1.html#east-5",
    "href": "lec1.html#east-5",
    "title": "Computer vision: intro",
    "section": "EAST",
    "text": "EAST\n\n# loop over the number of rows\nfor y in range(0, numRows):\n    # extract the scores (probabilities), followed by the geometrical\n    # data used to derive potential bounding box coordinates that\n    # surround text\n    scoresData = scores[0, 0, y]\n    xData0 = geometry[0, 0, y]\n    xData1 = geometry[0, 1, y]\n    xData2 = geometry[0, 2, y]\n    xData3 = geometry[0, 3, y]\n    anglesData = geometry[0, 4, y]\n\n    # loop over the number of columns\n    for x in range(0, numCols):\n        # if our score does not have sufficient probability, ignore it\n        if scoresData[x] &lt; confidence:\n            continue\n\n        # compute the offset factor as our resulting feature maps will\n        # be 4x smaller than the input image\n        (offsetX, offsetY) = (x * 4.0, y * 4.0)\n\n        # extract the rotation angle for the prediction and then\n        # compute the sin and cosine\n        angle = anglesData[x]\n        cos = np.cos(angle)\n        sin = np.sin(angle)\n\n        # use the geometry volume to derive the width and height of\n        # the bounding box\n        h = xData0[x] + xData2[x]\n        w = xData1[x] + xData3[x]\n\n        # compute both the starting and ending (x, y)-coordinates for\n        # the text prediction bounding box\n        endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n        endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n        startX = int(endX - w)\n        startY = int(endY - h)\n\n        # add the bounding box coordinates and probability score to\n        # our respective lists\n        rects.append((startX, startY, endX, endY))\n        confidences.append(scoresData[x])"
  },
  {
    "objectID": "lec1.html#east-6",
    "href": "lec1.html#east-6",
    "title": "Computer vision: intro",
    "section": "EAST",
    "text": "EAST\n\n\n# apply non-maxima suppression to suppress weak, overlapping bounding\n# boxes\nboxes = non_max_suppression(np.array(rects), probs=confidences)\n\n# loop over the bounding boxes\nfor (startX, startY, endX, endY) in boxes:\n    # scale the bounding box coordinates based on the respective\n    # ratios\n    startX = int(startX * rW)\n    startY = int(startY * rH)\n    endX = int(endX * rW)\n    endY = int(endY * rH)\n\n    # draw the bounding box on the image\n    cv2.rectangle(orig, (startX, startY), (endX, endY), (0, 255, 0), 2)\n\n# show the output image\nplt.imshow(orig)"
  },
  {
    "objectID": "lec1.html#image-segmentation",
    "href": "lec1.html#image-segmentation",
    "title": "Computer vision: intro",
    "section": "Image segmentation",
    "text": "Image segmentation\n\n\nPreparationGrayscaleOtsuNoise removal\n\n\n\nimport cv2\nimport numpy as np\nfrom IPython.display import Image, display\nfrom matplotlib import pyplot as plt\n\n# Plot the image\ndef imshow(img, ax=None):\n    if ax is None:\n        plt.imshow(img)\n    else:\n        ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n        ax.axis('off')\n\n#Image loading\nimg = cv2.imread(\"img/coins.png\")\n# Show image\nimshow(img)\n\n\n\n\n\n\n\n\n\n\n\n#image grayscale conversion\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nimshow(gray)\n\n\n\n\n\n\n\n\n\n\n\n#Threshold Processing\nret, bin_img = cv2.threshold(gray,\n                            0, 255, \n                            cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\nimshow(bin_img)\n\n\n\n\n\n\n\n\n\n\n\nkernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\nbin_img = cv2.morphologyEx(bin_img, \n                        cv2.MORPH_OPEN,\n                        kernel,\n                        iterations=2)\nimshow(bin_img)"
  },
  {
    "objectID": "lec1.html#image-segmentation-1",
    "href": "lec1.html#image-segmentation-1",
    "title": "Computer vision: intro",
    "section": "Image segmentation",
    "text": "Image segmentation\n\nBg/fg/unknownMarkersFinal\n\n\n\n# Create subplots with 1 row and 2 columns\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(8, 8))\n# sure background area\nsure_bg = cv2.dilate(bin_img, kernel, iterations=3)\nimshow(sure_bg, axes[0,0])\naxes[0, 0].set_title('Sure Background')\n\n# Distance transform\ndist = cv2.distanceTransform(bin_img, cv2.DIST_L2, 5)\nimshow(dist, axes[0,1])\naxes[0, 1].set_title('Distance Transform')\n\n#foreground area\nret, sure_fg = cv2.threshold(dist, 0.5 * dist.max(), 255, cv2.THRESH_BINARY)\nsure_fg = sure_fg.astype(np.uint8) \nimshow(sure_fg, axes[1,0])\naxes[1, 0].set_title('Sure Foreground')\n\n# unknown area\nunknown = cv2.subtract(sure_bg, sure_fg)\nimshow(unknown, axes[1,1])\naxes[1, 1].set_title('Unknown')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n# Marker labelling\n# sure foreground \nret, markers = cv2.connectedComponents(sure_fg)\n\n# Add one to all labels so that background is not 0, but 1\nmarkers += 1\n# mark the region of unknown with zero\nmarkers[unknown == 255] = 0\n\nfig, ax = plt.subplots(figsize=(6, 6))\nax.imshow(markers, cmap=\"tab20b\")\nax.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n# watershed Algorithm\nmarkers = cv2.watershed(img, markers)\n\nfig, ax = plt.subplots(figsize=(5, 5))\nax.imshow(markers, cmap=\"tab20b\")\nax.axis('off')\nplt.show()\n\n\nlabels = np.unique(markers)\n\ncoins = []\nfor label in labels[2:]: \n\n# Create a binary image in which only the area of the label is in the foreground \n#and the rest of the image is in the background \n    target = np.where(markers == label, 255, 0).astype(np.uint8)\n\n# Perform contour extraction on the created binary image\n    contours, hierarchy = cv2.findContours(\n        target, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n    )\n    coins.append(contours[0])\n\n# Draw the outline\nimg = cv2.drawContours(img, coins, -1, color=(0, 23, 223), thickness=2)\nimshow(img)"
  },
  {
    "objectID": "lec1.html#overlays",
    "href": "lec1.html#overlays",
    "title": "Computer vision: intro",
    "section": "Overlays",
    "text": "Overlays\n\n\n\nPython codeOutputOutput 2\n\n\n\nimport cv2\nimport numpy as np\n\n# Load the Haar Cascade XML file for face detection\nface_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\neye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n\n# Load the accessory image\naccessory_image = cv2.imread('img/ar_overlay.png', cv2.IMREAD_UNCHANGED)\n\n# Initialize the video capture\nvideo_capture = cv2.VideoCapture(0)\n\nwhile True:\n    # Read the video frame\n    ret, frame = video_capture.read()\n\n    # Convert the frame to grayscale\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n    # Perform face detection\n    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n\n    # Iterate over detected faces\n    for (x, y, w, h) in faces:\n        # Resize the accessory image to fit the face\n        resized_accessory = cv2.resize(accessory_image, (w, h))\n        \n        # Calculate the region of interest (ROI) for the accessory\n        roi = frame[y:y+h, x:x+w]\n\n        # Create a mask for the accessory\n        accessory_mask = resized_accessory[:, :, 3] / 255.0\n        bg_mask = 1.0 - accessory_mask\n\n        # Blend the accessory and the frame\n        accessory_pixels = resized_accessory[:, :, 0:3]\n        bg_pixels = roi[:, :, 0:3]\n\n        blended_pixels = (accessory_pixels * accessory_mask[:, :, np.newaxis]) + (bg_pixels * bg_mask[:, :, np.newaxis])\n\n        # Replace the ROI with the blended image\n        frame[y:y+h, x:x+w] = blended_pixels\n\n    # Display the resulting frame\n    cv2.imshow('Face Detection with Accessories', frame)\n\n    # Break the loop if 'q' is pressed\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# Release the video capture\nvideo_capture.release()\n\n# Close all OpenCV windows\ncv2.destroyAllWindows()"
  },
  {
    "objectID": "lec1.html#yolo-object-recognition",
    "href": "lec1.html#yolo-object-recognition",
    "title": "Computer vision: intro",
    "section": "YOLO object recognition",
    "text": "YOLO object recognition\n\n\nInitializationDetectionResult\n\n\n\nimport matplotlib.pyplot as plt\n\nimport datetime\nfrom ultralytics import YOLO\nimport cv2\nfrom imutils.video import VideoStream\n\n# define some constants\nCONFIDENCE_THRESHOLD = 0.8\nGREEN = (0, 255, 0)\n\n# load the pre-trained YOLOv8n model\nmodel = YOLO(\"yolov8n.pt\")\n\n\n\n\nframe = cv2.imread('./img/yolo_test.png')\ndetections = model(frame)[0]\nfor box in detections.boxes:\n    #extract the label name\n    label=model.names.get(box.cls.item())\n        \n    # extract the confidence (i.e., probability) associated with the detection\n    data=box.data.tolist()[0]\n    confidence = data[4]\n\n    # filter out weak detections by ensuring the\n    # confidence is greater than the minimum confidence\n    if float(confidence) &lt; CONFIDENCE_THRESHOLD:\n        continue\n\n    # if the confidence is greater than the minimum confidence,\n    # draw the bounding box on the frame\n    xmin, ymin, xmax, ymax = int(data[0]), int(data[1]), int(data[2]), int(data[3])\n    cv2.rectangle(frame, (xmin, ymin) , (xmax, ymax), GREEN, 2)\n\n    #draw confidence and label\n    y = ymin - 15 if ymin - 15 &gt; 15 else ymin + 15\n    cv2.putText(frame, \"{} {:.1f}%\".format(label,float(confidence*100)), (xmin, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, GREEN, 2)\n\n\n0: 576x640 4 persons, 12 cars, 41.2ms\nSpeed: 2.0ms preprocess, 41.2ms inference, 1.0ms postprocess per image at shape (1, 3, 576, 640)\n\n\n\n\n\nplt.imshow(frame)"
  },
  {
    "objectID": "lec1.html#thank-you",
    "href": "lec1.html#thank-you",
    "title": "Computer vision: intro",
    "section": "Thank you",
    "text": "Thank you"
  },
  {
    "objectID": "lab3.html",
    "href": "lab3.html",
    "title": "Lab 3: filtering with convolutions",
    "section": "",
    "text": "Exercises\nWork through the notebook.\n\n\nRecommended reading\n\nhttps://learnopencv.com/image-filtering-using-convolution-in-opencv/"
  },
  {
    "objectID": "lec2.html#goal",
    "href": "lec2.html#goal",
    "title": "Basic concepts",
    "section": "Goal",
    "text": "Goal\n\nsimulate original research from 1960s\nhand-design an end-to-end visual system.\ncover some of the main concepts"
  },
  {
    "objectID": "lec2.html#history",
    "href": "lec2.html#history",
    "title": "Basic concepts",
    "section": "History",
    "text": "History\n\n\n\nSeymour Papert (1966)\n\n\n\n‚ÄúThe summer vision project is an attempt to use our summer workers effectively in the construction of a significant part of a visual system.‚Äù\n\n\n\n\n\n\n\nWarning\n\n\nToo optimistic!"
  },
  {
    "objectID": "lec2.html#problem-setup",
    "href": "lec2.html#problem-setup",
    "title": "Basic concepts",
    "section": "Problem setup",
    "text": "Problem setup\nVision has many different goals:\n\nobject recognition\nscene interpretation\nthree-dimensional [3D] interpretation"
  },
  {
    "objectID": "lec2.html#simple-world",
    "href": "lec2.html#simple-world",
    "title": "Basic concepts",
    "section": "Simple world",
    "text": "Simple world\n\n\n\nBlock world\n\n\nIntroduced by Larry G. Roberts in 1963."
  },
  {
    "objectID": "lec2.html#simple-world-1",
    "href": "lec2.html#simple-world-1",
    "title": "Basic concepts",
    "section": "Simple world",
    "text": "Simple world\n\n\n\nDescription\n\n\n\nworld is composed of a very simple (yet varied) set of objects.\nthese simple objects are composed of flat surfaces that can be horizontal or vertical\nthese objects will be resting on a white horizontal ground plane\nwe can build these objects by cutting, folding, and gluing together some pieces of colored paper\n\n\n\n\n\n\n\nFigure¬†1: A world of simple objects. Print, cut, and build your own blocks world!"
  },
  {
    "objectID": "lec2.html#a-simple-image-formation-model-1",
    "href": "lec2.html#a-simple-image-formation-model-1",
    "title": "Basic concepts",
    "section": "A Simple Image Formation Model",
    "text": "A Simple Image Formation Model\n\n\n\nParallel of orthographic projection\n\n\n\nlight rays travel parallel to each other and perpendicular to the camera plane\nthis type of projection produces images in which objects do not change size as they move closer or farther from the camera\nparallel lines in 3D remain parallel in the 2D image."
  },
  {
    "objectID": "lec2.html#a-simple-image-formation-model-2",
    "href": "lec2.html#a-simple-image-formation-model-2",
    "title": "Basic concepts",
    "section": "A Simple Image Formation Model",
    "text": "A Simple Image Formation Model\n\n\n\nPerspective projection\n\n\n\nthe image is formed by the convergence of the light rays into a single point (focal point).\nmost pictures taken with a camera will be better described by perspective projection"
  },
  {
    "objectID": "lec2.html#a-simple-image-formation-model-3",
    "href": "lec2.html#a-simple-image-formation-model-3",
    "title": "Basic concepts",
    "section": "A Simple Image Formation Model",
    "text": "A Simple Image Formation Model\n\n\n\n\n\n\nFigure¬†2\n\n\n\n\n\n\nNote that near edges are larger than far edges, and parallel lines in 3D are not parallel.\nPicture taken from far away using zoom resulting in an image that can be described by parallel projection."
  },
  {
    "objectID": "lec2.html#a-simple-image-formation-model-4",
    "href": "lec2.html#a-simple-image-formation-model-4",
    "title": "Basic concepts",
    "section": "A Simple Image Formation Model",
    "text": "A Simple Image Formation Model"
  },
  {
    "objectID": "lec2.html#a-simple-image-formation-model-5",
    "href": "lec2.html#a-simple-image-formation-model-5",
    "title": "Basic concepts",
    "section": "A Simple Image Formation Model",
    "text": "A Simple Image Formation Model\n\nhow a point in world coordinates \\((X,Y,Z)\\) projects into the image plane\ncamera center is inside the 3D plane \\(X=0\\)\nthe horizontal axis of the camera (\\(x\\)) is parallel to the ground plane (\\(Y=0\\))\nthe camera is tilted so that the line connecting the origin of the world coordinates system and the image center is perpendicular to the image plane\nthe angle \\(\\theta\\) is the angle between this line and the \\(Z\\)-axis\nthe image is parameterized by coordinates \\((x,y)\\)."
  },
  {
    "objectID": "lec2.html#a-simple-image-formation-model-6",
    "href": "lec2.html#a-simple-image-formation-model-6",
    "title": "Basic concepts",
    "section": "A Simple Image Formation Model",
    "text": "A Simple Image Formation Model\n\n\n\n\n\n\nFigure¬†3\n\n\n\n\n\nThe \\(Z\\)-axis is identical to the \\(Y\\)-axis up to a sign change and a scaling."
  },
  {
    "objectID": "lec2.html#a-simple-image-formation-model-7",
    "href": "lec2.html#a-simple-image-formation-model-7",
    "title": "Basic concepts",
    "section": "A Simple Image Formation Model",
    "text": "A Simple Image Formation Model"
  },
  {
    "objectID": "lec2.html#a-simple-image-formation-model-8",
    "href": "lec2.html#a-simple-image-formation-model-8",
    "title": "Basic concepts",
    "section": "A Simple Image Formation Model",
    "text": "A Simple Image Formation Model\n\n\n\nFeatures\n\n\n\nthe origin of the world coordinates projects on the origin of the image coordinates\ntherefore, the world point \\((0,0,0)\\) projects into \\((0,0)\\)\nthe resolution of the image will also affect the transformation from world coordinates to image coordinates via a constant factor \\(\\alpha\\)\nfor now we assume that pixels are square and we will see a more general form in ) and that this constant is \\(\\alpha=1\\)."
  },
  {
    "objectID": "lec2.html#a-simple-image-formation-model-9",
    "href": "lec2.html#a-simple-image-formation-model-9",
    "title": "Basic concepts",
    "section": "A Simple Image Formation Model",
    "text": "A Simple Image Formation Model"
  },
  {
    "objectID": "lec2.html#a-simple-image-formation-model-10",
    "href": "lec2.html#a-simple-image-formation-model-10",
    "title": "Basic concepts",
    "section": "A Simple Image Formation Model",
    "text": "A Simple Image Formation Model\n\n\n\nCoordinate transformation\n\n\nThe transformation between world coordinates and image coordinates can be written as follows:\n\\[\\begin{aligned}\nx &=& X \\\\\ny &=&  \\cos(\\theta) Y - \\sin(\\theta) Z\n\\end{aligned} \\qquad(1)\\]\n\n\n\n\n\nWith this particular parametrization of the world and camera coordinate systems, the world coordinates \\(Y\\) and \\(Z\\) are mixed after projection. From the camera, a point moving parallel to the \\(Z\\)-axis will be indistinguishable from a point moving parallel to the \\(Y\\)-axis."
  },
  {
    "objectID": "lec2.html#a-simple-goal",
    "href": "lec2.html#a-simple-goal",
    "title": "Basic concepts",
    "section": "A Simple Goal",
    "text": "A Simple Goal\n\n\n\nOur goal\n\n\n\nrecovering the world coordinates of all the pixels seen by the camera.\n\n\n\n\n\n\n\nNon-goal\n\n\n\nrecover the actual color of the surface seen by each pixel \\((x,y)\\)\n\n(requires discounting for illumination effects as the color of the pixel is a combination of the surface albedo and illumination (color of the light sources and interreflections)."
  },
  {
    "objectID": "lec2.html#from-images-to-edges",
    "href": "lec2.html#from-images-to-edges",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\nWe want to recover \\(X(x,y),Y(x,y), Z(x,y)\\) from \\(l(x,y)\\)"
  },
  {
    "objectID": "lec2.html#from-images-to-edges-1",
    "href": "lec2.html#from-images-to-edges-1",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\nThe observed image is a function: \\[\\ell(x,y)\\]\n\n\n\nDescription\n\n\n\ninput: location, \\((x,y)\\)\noutput: the intensity at that location.\n\n\n\n\n\n\n\nWhat is an image?\n\n\nIn this representation, the image is an array of intensity values (color values) indexed by location."
  },
  {
    "objectID": "lec2.html#from-images-to-edges-2",
    "href": "lec2.html#from-images-to-edges-2",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\n\nAny planar line-drawing is geometrically consistent with infinitely many 3-D structures (Sinha-Adelson ‚Äô93 paper)"
  },
  {
    "objectID": "lec2.html#from-images-to-edges-3",
    "href": "lec2.html#from-images-to-edges-3",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\nFigure¬†4: Image as a surface. The vertical axis corresponds to image intensity. For clarity here, we have reversed the vertical axis. Dark values are shown higher than lighter values."
  },
  {
    "objectID": "lec2.html#from-images-to-edges-4",
    "href": "lec2.html#from-images-to-edges-4",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\nBenefits\n\n\n\nthis representation is ideal for determining the light intensity originating from different directions in space and striking the camera plane, as it provides explicit representation of this information\nthe array of pixel intensities, \\(\\ell(x,y)\\), is a reasonable representation as input to the early stages of visual processing because, although we do not know the distance of surfaces in the world, the direction of each light ray in the world is well defined.\n\n\n\n\n\n\n\nAlternative options\n\n\nOther initial representations:\n\nimages could be coded in the Fourier domain\npixels could combine light coming in different directions."
  },
  {
    "objectID": "lec2.html#from-images-to-edges-5",
    "href": "lec2.html#from-images-to-edges-5",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\n\n\n\nNote\n\n\nWe are interested in interpreting the 3D structure of the scene and the objects within.\n\n\n\n\n\n\nChallenges\n\n\nNeed to represent:\n\nboundaries between objects\nchanges in the surface orientation"
  },
  {
    "objectID": "lec2.html#from-images-to-edges-6",
    "href": "lec2.html#from-images-to-edges-6",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\nAlternative representation\n\n\nFor scene interpretation:\n\ncollections of small image patches\nregions of uniform properties\nedges"
  },
  {
    "objectID": "lec2.html#from-images-to-edges-7",
    "href": "lec2.html#from-images-to-edges-7",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\nAlternative initial representations\n\n\nGekko‚Äôs eye. Their pupil has a four-diamond-shaped pinhole aperture that could allow them to encode distance to a target in the retinal image."
  },
  {
    "objectID": "lec2.html#from-images-to-edges-8",
    "href": "lec2.html#from-images-to-edges-8",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\n\n\n\nWhat is an edge\n\n\nAn image regions where there are strong changes of the image with respect to location.\n\n\n\n\n\n\nEdge factors\n\n\n\nocclusion boundaries\nchanges in surface orientation\nchanges in surface albedo\nshadows"
  },
  {
    "objectID": "lec2.html#from-images-to-edges-9",
    "href": "lec2.html#from-images-to-edges-9",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\nFigure¬†5"
  },
  {
    "objectID": "lec2.html#from-images-to-edges-10",
    "href": "lec2.html#from-images-to-edges-10",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\nImage edge classification\n\n\n\nObject boundaries: These indicate pixels that delineate the boundaries of any object. Boundaries between objects generally correspond to changes in surface color, texture, and orientation.\nChanges in surface orientation: These indicate locations where there are strong image variations due to changes in the surface orientations. A change in surface orientation produces changes in the image intensity because intensity is a function of the angle between the surface and the incident light.\nShadow edges: This can be harder than it seems. In this simple world, shadows are soft, creating slow transitions between dark and light."
  },
  {
    "objectID": "lec2.html#from-images-to-edges-11",
    "href": "lec2.html#from-images-to-edges-11",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\nObject boundary classification\n\n\n\nContact edges: This is a boundary between two objects that are in physical contact. Therefore, there is no depth discontinuity.\nOcclusion boundaries: Occlusion boundaries happen when an object is partially in front of another. Occlusion boundaries generally produce depth discontinuities. In this simple world, we will position the objects in such a way that objects do not occlude each other but they will occlude the background."
  },
  {
    "objectID": "lec2.html#from-images-to-edges-12",
    "href": "lec2.html#from-images-to-edges-12",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\n\n\n\nChallenges\n\n\n\nin most natural scenes, this classification is very hard\nrequires the interpretation of the scene at different levels."
  },
  {
    "objectID": "lec2.html#from-images-to-edges-13",
    "href": "lec2.html#from-images-to-edges-13",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\nFirst step: detecting candidate edges in the image.\nIf we think of the image as a function \\(\\ell (x,y) \\in C(\\mathbb{R}^2)\\), we can measure the degree of variation using the gradient:\n\\[\\begin{aligned}\n\\nabla \\ell = \\left( \\frac{\\partial \\ell}{\\partial x}, \\frac{\\partial \\ell}{\\partial y} \\right)\n\\end{aligned}\\]\n\n\n\nThe direction of the gradient indicates the direction in which the variation of intensities is larger. If we are on top of an edge, the direction of larger variation will be in the direction perpendicular to the edge."
  },
  {
    "objectID": "lec2.html#from-images-to-edges-14",
    "href": "lec2.html#from-images-to-edges-14",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\nImportant\n\n\nThe image is not a continuous function as we only know the values of the \\(\\ell (x,y)\\) at discrete locations (pixels).\n\n\n\n\n\n\nApproximation\n\n\nWe approximate the partial derivatives by:\n\\[\\begin{aligned}\n\\frac{\\partial \\ell}{\\partial x} &\\simeq \\ell(x,y) - \\ell(x-1,y) \\\\\n\\frac{\\partial \\ell}{\\partial y} &\\simeq \\ell(x,y) - \\ell(x,y-1)\n\\end{aligned} \\qquad(2)\\]"
  },
  {
    "objectID": "lec2.html#from-images-to-edges-15",
    "href": "lec2.html#from-images-to-edges-15",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\nBetter approximation (TBD)\n\n\nCombine the image pixels around \\((x,y)\\) with the weights: \\[\\begin{aligned}\n\\frac{1}{4} \\times\n\\left [\n\\begin{matrix}\n-1 & 0 & 1 \\\\\n-2 & 0 & 2 \\\\\n-1 & 0 & 1\n\\end{matrix}\n\\right ] \\nonumber\n\\end{aligned}\\]"
  },
  {
    "objectID": "lec2.html#from-images-to-edges-16",
    "href": "lec2.html#from-images-to-edges-16",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\nEdge properties\n\n\nFrom the image gradient, we can extract a number of interesting quantities: \\[\\begin{aligned}\n    e(x,y) &= \\lVert \\nabla \\ell(x,y) \\rVert   & \\quad\\quad \\triangleleft \\quad \\texttt{edge strength}\\\\\n    \\theta(x,y) &= \\angle \\nabla \\ell =  \\arctan \\left( \\frac{\\partial \\ell / \\partial y}{\\partial \\ell / \\partial x} \\right) & \\quad\\quad \\triangleleft \\quad \\texttt{edge orientation}\n\\end{aligned}\\]\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nedge strength is the gradient magnitude\nedge orientation is perpendicular to the gradient direction"
  },
  {
    "objectID": "lec2.html#from-images-to-edges-17",
    "href": "lec2.html#from-images-to-edges-17",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\nThe unit norm vector perpendicular to an edge is:\n\\[\\begin{aligned}\n{\\bf n} = \\frac{\\nabla \\ell}{\\lVert \\nabla \\ell \\rVert}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lec2.html#from-images-to-edges-18",
    "href": "lec2.html#from-images-to-edges-18",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\nDecision\n\n\nWhich pixels belong to:\n\nedges (regions of the image with sharp intensity variations)\nuniform regions (flat surfaces)\n\n\n\n\n\n\n\nHow?\n\n\nBy thresholding the edge strength \\(e(x,y)\\).\nIn the pixels with edges, we can also measure the edge orientation \\(\\theta(x,y)\\)."
  },
  {
    "objectID": "lec2.html#from-images-to-edges-19",
    "href": "lec2.html#from-images-to-edges-19",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges\n\n\n\n\n\n\nFigure¬†6: Gradient and edge types.\n\n\n\n\n\n\n\n\n\nFigure¬†7\n\n\n\n\n\nFigure¬†6 and Figure¬†7 visualize the edges and the normal vector on each edge."
  },
  {
    "objectID": "lec2.html#from-images-to-edges-20",
    "href": "lec2.html#from-images-to-edges-20",
    "title": "Basic concepts",
    "section": "From Images to Edges",
    "text": "From Images to Edges"
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-1",
    "href": "lec2.html#from-edges-to-surfaces-1",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nObjective\n\n\nWe want to recover world coordinates \\(X(x,y)\\), \\(Y(x,y)\\), and \\(Z(x,y)\\) for each image location \\((x,y)\\)\n\n\n\n\n\n\nSteps\n\n\n\nrecovering the \\(X\\) world coordinates is trivial as they are directly observed: for each pixel with image coordinates \\((x,y)\\) the world coordinate is \\(X(x,y) = x\\)\nrecovering \\(Y\\) and \\(Z\\) will be harder as we only observe a mixture of the two world coordinates.\n\n\n\n\n\n\nHere we have written the world coordinates as functions of image location \\((x,y)\\) to make explicit that we want to recover the 3D locations of the visible points.\nIn this simple world, we will formulate this problem as a set of linear equations."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-2",
    "href": "lec2.html#from-edges-to-surfaces-2",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nFigure/Ground Segmentation\n\n\nSegmentation of an image into figure and ground is a classical problem in human perception and computer vision that was introduced by Gestalt psychology.\n\n\n\n\n\n\nThe classical visual illusion ‚Äútwo faces or a vase‚Äù is an example of figure-ground segmentation problem."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-3",
    "href": "lec2.html#from-edges-to-surfaces-3",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\n\n\n\nSegmentation: goal\n\n\nDecide whether a pixel belongs to one of the foreground objects or to the background.\n\n\n\n\n\n\n\n\n\nSegmentation: basic approach\n\n\nWe can simply look at the color values of each pixel\n\nbright pixels that have low saturation (similar values of the red-blue-green [RBG] components) correspond to the white ground plane\nand the rest of the pixels are likely to belong to the colored blocks that compose our simple world\n\n\n\n\n\n\nIn general, the problem of image segmentation into distinct objects is a very challenging task."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-4",
    "href": "lec2.html#from-edges-to-surfaces-4",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nBackground\n\n\nIf we assume that the background corresponds to a horizontal ground plane, then for all pixels that belong to the ground we can set \\(Y(x,y)=0\\).\n\n\n\n\n\n\nObjects\n\n\nFor pixels that belong to objects we will have to measure additional image properties before we can deduce any geometric scene constraints."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-5",
    "href": "lec2.html#from-edges-to-surfaces-5",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nOcclusion Edges\n\n\nAn occlusion boundary separates two different surfaces at different distances from the observer.\nThe object in front is the one owning the boundary.\n\n\n\n\n\n\nKnowing who owns the boundary is important as an edge provides cues about the 3D geometry, but those cues only apply to the surface that owns the boundary."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-6",
    "href": "lec2.html#from-edges-to-surfaces-6",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nAssumption\n\n\nIn this simple world, we will assume that objects do not occlude each other (this can be relaxed) and that the only occlusion boundaries are the boundaries between the objects and the ground.\n\n\n\n\n\n\n\n\n\nNote\n\n\nNot all boundaries between the objects and the ground correspond to depth gradients."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-7",
    "href": "lec2.html#from-edges-to-surfaces-7",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nContact Edges\n\n\nContact edges are boundaries between two distinct objects but where there exists no depth discontinuity.\nDespite that there is not a depth discontinuity, there is an occlusion here (as one surface is hidden behind another), and the edge shape is only owned by one of the two surfaces.\n\n\n\n\n\n\nCalculation\n\n\nIf we assume that all the objects rest on the ground plane, then we can set \\(Y(x,y)=0\\) on the contact edges.\nContact edges can be detected as transitions between the object (above) and ground (below). In our simple world only horizontal edges can be contact edges."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-8",
    "href": "lec2.html#from-edges-to-surfaces-8",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\nFigure¬†8: For each vertical line (shown in red), scanning from top to bottom, transitions from ground to figure are occlusion boundaries, and transitions from figure to ground are contact edges. This heuristic will fails when an object occludes another."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-9",
    "href": "lec2.html#from-edges-to-surfaces-9",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\n\n\n\nInvariant Scene Properties (world \\(\\rightarrow\\) image)\n\n\n\nCollinearity: a straight 3D line will project into a straight line in the image.\nCotermination: if two or more 3D lines terminate at the same point, the corresponding projections will also terminate at a common point.\nSmoothness: a smooth 3D curve will project into a smooth 2D curve."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-10",
    "href": "lec2.html#from-edges-to-surfaces-10",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nReverse invariants (image \\(\\rightarrow\\) world)\n\n\n\na straight line in the image could correspond to a curved line in the 3D world but that happens to be precisely aligned with respect to the viewers point of view to appear as a straight line\ntwo lines that intersect in the image plane could be disjointed in the 3D space."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-11",
    "href": "lec2.html#from-edges-to-surfaces-11",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nReverse invariants (image \\(\\rightarrow\\) world)\n\n\n\nif two lines coterminate in the image, then, one can conclude that it is very likely that they also touch each other in 3D\nif the 3D lines do not touch each other, then it will require a very specific alignment between the observer and the lines for them to appear to coterminate in the image. Therefore, one can safely conclude that the lines might also touch in 3D."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-12",
    "href": "lec2.html#from-edges-to-surfaces-12",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nNonaccidental properties\n\n\nThese properties are called nonaccidental properties because they will only be observed in the image if they also exist in the world or by accidental alignments between the observer and scene structures.\n\n\n\n\n\n\nGeneric view\n\n\nUnder a generic view, nonaccidental properties will be shared by the image and the 3D world."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-13",
    "href": "lec2.html#from-edges-to-surfaces-13",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nGeneric view assumption: the observer should not assume that he has a special position in the world‚Ä¶ The most generic interpretation is to see a vertical line as a vertical line in 3D."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-14",
    "href": "lec2.html#from-edges-to-surfaces-14",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nApplication to simple world\n\n\n\nin the simple world all 3D edges are either vertical or horizontal\nunder parallel projection and with the camera having its horizontal axis parallel to the ground, we know that vertical 3D lines will project into vertical 2D lines in the image\nhorizontal lines will, in general, project into oblique lines\ntherefore, we can assume than any vertical line in the image is also a vertical line in the world."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-15",
    "href": "lec2.html#from-edges-to-surfaces-15",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\n\n\n\nChallenge\n\n\nThe assumption that vertical 2D lines are also 3D vertical lines will not always work!\n\n\n\n\n\n\n\n\n\nFigure¬†9\n\n\n\n\n\nIn the case of the cube, there is a particular viewpoint that will make an horizontal line project into a vertical line, but this will require an accidental alignment between the cube and the line of sight of the observer."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-16",
    "href": "lec2.html#from-edges-to-surfaces-16",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nFormulation\n\n\nWe can now translate the inferred 3D edge orientation into linear constraints on the global 3D structure. We will formulate these constraints in terms of \\(Y(x,y)\\). Once \\(Y(x,y)\\) is recovered we can also recover \\(Z(x,y)\\) from Equation¬†1.\nIn a 3D vertical edge, using the projection equations, the derivative of \\(Y\\) along the edge will be\n\\[\\begin{aligned}\n\\partial Y / \\partial y &= 1/ \\cos(\\theta)\\end{aligned}\n\\qquad(3)\\]"
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-17",
    "href": "lec2.html#from-edges-to-surfaces-17",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nFormulation\n\n\nIn a 3D horizontal edge, the coordinate \\(Y\\) will not change. Therefore, the derivative along the edge should be zero:\n\\[\\begin{aligned}\n\\partial Y / \\partial {\\bf t} &= 0\n\\end{aligned}\n\\qquad(4)\\]\nwhere the vector \\(\\bf t\\) denotes direction tangent to the edge, \\({\\bf t}=(-n_y, n_x)\\)."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-18",
    "href": "lec2.html#from-edges-to-surfaces-18",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nFormulation\n\n\nWe can write this derivative as a function of derivatives along the \\(x\\) and \\(y\\) image coordinates: \\[\\begin{aligned}\n\\partial Y / \\partial {\\bf t} =  \\nabla Y \\cdot {\\bf t} = -n_y \\partial Y / \\partial x + n_x \\partial Y / \\partial y\n\\end{aligned} \\qquad(5)\\]\nWhen the edges coincide with occlusion edges, special care should be taken so that these constraints are only applied to the surface that owns the boundary."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-19",
    "href": "lec2.html#from-edges-to-surfaces-19",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\n\n\n\nApproximation\n\n\nWe represent the world coordinates \\(X(x,y)\\), \\(Y(x,y)\\), and \\(Z(x,y)\\) as images where the coordinates \\(x,y\\) correspond to pixel locations.\nTherefore, it is useful to approximate the partial derivatives in the same way that we approximated the image partial derivatives in equations (Equation¬†2). Using this approximation, Equation¬†3 can be written as follows: \\[\\begin{aligned}\nY(x,y)-Y(x,y-1) &= 1/ \\cos(\\theta)\n\\end{aligned}\\]\nSimilar relationships can be obtained from equations Equation¬†4 and Equation¬†5."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-20",
    "href": "lec2.html#from-edges-to-surfaces-20",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\nNo edges\n\n\n\nThe ‚ÄúRule of Nothing‚Äù (Ted Adelson): where you see nothing, assume nothing happens, and just propagate information from where something happened."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-21",
    "href": "lec2.html#from-edges-to-surfaces-21",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nConstraint Propagation\n\n\nMost of the image consists of flat regions where we do not have such edge constraints and we thus don‚Äôt have enough local information to infer the surface orientation.\nTherefore, we need some criteria in order to propagate information from the boundaries, where we do have information about the 3D structure, into the interior of flat image regions.\n\n\n\n\n\n\nNote\n\n\nThis problem is common in many visual domains."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-22",
    "href": "lec2.html#from-edges-to-surfaces-22",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nAn image patch without context is not enough to infer its 3D shape.\n\n\nThe same patch shown in the original image. Information about its 3D orientation its propagated from the surrounding edges."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-23",
    "href": "lec2.html#from-edges-to-surfaces-23",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nAssumption\n\n\nIn this case we will assume that the object faces are planar. Thus, flat image regions impose the following constraints on the local 3D structure: \\[\\begin{aligned}\n\\partial^2 Y / \\partial x^2 &= 0  \\\\\n\\partial^2 Y / \\partial y^2 &= 0 \\\\  \n\\partial^2 Y / \\partial y \\partial x &= 0\n\\end{aligned}\\]\nThat is, the second order derivative of \\(Y\\) should be zero. As before, we want to approximate the continuous partial derivatives."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-24",
    "href": "lec2.html#from-edges-to-surfaces-24",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nApproximation\n\n\nThe approximation to the second derivative can be obtained by applying twice the first order derivative approximated by equations (Equation¬†2). The result is\n\\[\\partial^2 Y / \\partial x^2 \\simeq 2Y(x,y)-Y(x+1,y)-Y(x-1,y),\n\\] and similarly for \\(\\partial^2 X / \\partial x^2\\)."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-25",
    "href": "lec2.html#from-edges-to-surfaces-25",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\nA Simple Inference Scheme\n\n\n\nEquation system\n\n\nAll the different constraints described previously can be written as an overdetermined system of linear equations. Each equation will have the form: \\[\\begin{aligned}\n\\mathbf{a}_i \\mathbf{Y} = b_i\n\\end{aligned}\\]\nwhere \\(\\mathbf{Y}\\) is a vectorized version of the image \\(Y\\) (i.e., all rows of pixels have been concatenated into a flat vector).\n\n\n\n\n\n\n\n\n\nNote\n\n\nThere might be many more equations than there are image pixels."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-26",
    "href": "lec2.html#from-edges-to-surfaces-26",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nTranslation\n\n\nWe can translate all the constraints described in the previous sections into this form:\n\nfor instance, if the index \\(i\\) corresponds to one of the pixels inside one of the planar faces of a foreground object, then there will be three equations\none of the planarity constraint can be written as \\[\\mathbf{a}_i = [0, \\dots, 0, -1, 2, -1, 0, \\dots, 0], \\, b_i=0,\\] and analogous equations can be written for the other two."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-27",
    "href": "lec2.html#from-edges-to-surfaces-27",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\n\n\n\nHow to solve?\n\n\nWe can solve the system of equations by minimizing the following cost function: \\[\\begin{aligned}\nJ = \\sum_i (\\mathbf{a}_i\\mathbf{Y} - b_i)^2\n\\end{aligned}\\] where the sum is over all the constraints.\n\n\n\n\n\n\nWeights\n\n\nIf some constraints are more important than others, we can use weights \\(w_i\\). \\[\\begin{aligned}\nJ = \\sum_i w_i (\\mathbf{a}_i \\mathbf{Y} - b_i)^2\n\\end{aligned}\\]"
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-28",
    "href": "lec2.html#from-edges-to-surfaces-28",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\n\n\n\nNote\n\n\nOur formulation has resulted on a big system of linear constraints (there are more equations than there are pixels in the image).\n\n\n\n\n\n\nSystem of equations\n\n\nIt is convenient to write the system of equations in matrix form: \\[\\begin{aligned}\n\\mathbf{A} \\mathbf{Y}  = \\mathbf{b}\n\\end{aligned}\\]\nwhere row \\(i\\) of the matrix \\({\\bf A}\\) contains the constraint coefficients \\(\\mathbf{a}_i\\).\nThe system of equations is overdetermined (\\(\\mathbf{A}\\) has more rows than columns)."
  },
  {
    "objectID": "lec2.html#from-edges-to-surfaces-29",
    "href": "lec2.html#from-edges-to-surfaces-29",
    "title": "Basic concepts",
    "section": "From Edges to Surfaces",
    "text": "From Edges to Surfaces\n\n\n\nSolution\n\n\nWe can use the pseudoinverse to compute the solution:\n\\[\\begin{aligned}\n\\bf Y = (\\mathbf{A}^T \\mathbf{A})^{-1} \\mathbf{A}^T \\mathbf{b}\n\\end{aligned}\\]\nThis problem can be solved efficiently as the matrix \\(\\mathbf{A}\\) is very sparse (most of the elements are zero)."
  },
  {
    "objectID": "lec2.html#results",
    "href": "lec2.html#results",
    "title": "Basic concepts",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\nFigure¬†10\n\n\n\n\n\nCoordinates shows the resulting world coordinates \\(X(x,y)\\), \\(Y(x,y)\\), \\(Z(x,y)\\) for each pixel.\nWorld coordinates \\(X\\), \\(Y\\), and \\(Z\\) are shown as images with the gray level coding the value of each coordinate (black corresponds to the value 0)."
  },
  {
    "objectID": "lec2.html#results-1",
    "href": "lec2.html#results-1",
    "title": "Basic concepts",
    "section": "Results",
    "text": "Results\n\n\n\nWhat do we get?\n\n\nThere are a few things to reflect on:\n\nIt works. At least it seems to work pretty well. Knowing how well it works will require having some way of evaluating performance. This will be important.\nBut it cannot possibly work all the time. We have made lots of assumptions that will work only in this simple world. The rest of the book will involve upgrading this approach to apply to more general input images.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nDespite that this approach will not work on general images, many of the general ideas will carry over to more sophisticated solutions (e.g., gather and propagate local evidence)."
  },
  {
    "objectID": "lec2.html#results-2",
    "href": "lec2.html#results-2",
    "title": "Basic concepts",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\nEvaluation\n\n\nEvaluationof performance is a very important topic. Here, one simple way to visually verify that the solution is correct is to render the objects under new view points.\n\n\n\n\n\n\n\n\n\nFigure¬†11\n\n\n\n\n\nTo show that the algorithm for 3D interpretation gives reasonable results we can re-render the inferred 3D structure from different viewpoints."
  },
  {
    "objectID": "lec2.html#results-3",
    "href": "lec2.html#results-3",
    "title": "Basic concepts",
    "section": "Results",
    "text": "Results\nYou may also try the interactive demo below to see the 3D structure. (The demo supports mouse zoom in and out, pan, and rotate.)"
  },
  {
    "objectID": "lec2.html#generalization",
    "href": "lec2.html#generalization",
    "title": "Basic concepts",
    "section": "Generalization",
    "text": "Generalization\n\n\n\nNote\n\n\nOne desired property of any vision system is it ability to generalize outside of the domain for which it was designed to operate.\nOut of domain generalization refers to the ability of a system to operate outside the domain for which it was designed.\nWe have listed several assumptions earlier.\nIn learning-based approaches the training dataset specifies the domain."
  },
  {
    "objectID": "lec2.html#generalization-1",
    "href": "lec2.html#generalization-1",
    "title": "Basic concepts",
    "section": "Generalization",
    "text": "Generalization\n\n\n\n\n\n\nFigure¬†12\n\n\n\n\n\n\nshadows are not soft\nthe green cube occludes the red one\none object on top of the other"
  },
  {
    "objectID": "lec2.html#generalization-2",
    "href": "lec2.html#generalization-2",
    "title": "Basic concepts",
    "section": "Generalization",
    "text": "Generalization\n\n\n\n\n\n\nFigure¬†13: Impossible steps\n\n\n\n\n\n\nleft: this shape looks rectangular and the stripes appear to be painted on the surface\nright: the shape looks as though it has steps, with the stripes corresponding to shading due to the surface orientation\nmiddle: the shape is ambiguous."
  },
  {
    "objectID": "lec2.html#concluding-remarks",
    "href": "lec2.html#concluding-remarks",
    "title": "Basic concepts",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\n\n\n\n\n\n\nLack of knowledge\n\n\n\nthe system is still unaware of the fact that the scene is composed of a set of distinct objects\nas the system lacks a representation of which objects are actually present in the scene, we cannot visualize the occluded parts"
  },
  {
    "objectID": "lec2.html#concluding-remarks-1",
    "href": "lec2.html#concluding-remarks-1",
    "title": "Basic concepts",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\n\n\n\nA different approach: model-based\n\n\nWe could have a set of predefined models of the objects that can be present in the scene and the system can try to decide if they are present or not in the image, and recover their parameters (i.e., pose, color).\nRecognition allows indexing properties that are not directly available in the image."
  },
  {
    "objectID": "lec6.html#notation",
    "href": "lec6.html#notation",
    "title": "Intro to learning",
    "section": "Notation",
    "text": "Notation\n\n\\(\\mathbf{x}\\): model inputs (instead f \\({\\boldsymbol\\ell}\\))\n\\(\\mathbf{y}\\): a model‚Äôs final output.\n\\(\\mathbf{x}_0 \\rightarrow \\mathbf{x}_1 \\rightarrow \\ldots \\rightarrow \\mathbf{y}\\): neural network‚Äôs sequence of transformations\nfor single layer: \\({\\mathbf{x}_{\\texttt{in}}}\\) as input and \\({\\mathbf{x}_{\\texttt{out}}}\\) as output\n\\(\\mathbf{h}\\) and \\(\\mathbf{z}\\): intermediate representations in neural nets"
  },
  {
    "objectID": "lec6.html#introduction",
    "href": "lec6.html#introduction",
    "title": "Intro to learning",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\nGoal\n\n\nThe goal of learning is to extract lessons from past experience in order to solve future problems.\nTypically, this involves searching for an algorithm that solves past instances of the problem.\n\n\n\n\n\n\n\n\n\nNote\n\n\nPast and future do not necessarily refer to the calendar date; instead they refer to what the has previously seen and what the learner will see next."
  },
  {
    "objectID": "lec6.html#introduction-1",
    "href": "lec6.html#introduction-1",
    "title": "Intro to learning",
    "section": "Introduction",
    "text": "Introduction\n\n\n\nAlgorithm\n\n\nBecause learning is itself an algorithm, it can be understood as a meta-algorithm: an algorithm that outputs algorithms:\n\n\n\n\n\nFigure¬†1: Learning is an algorithm that outputs algorithms."
  },
  {
    "objectID": "lec6.html#introduction-2",
    "href": "lec6.html#introduction-2",
    "title": "Intro to learning",
    "section": "Introduction",
    "text": "Introduction\n\n\n\nPhases\n\n\nLearning usually consists of two phases:\n\nthe training phase, where we search for an algorithm that performs well on past instances of the problem (training data)\nthe testing phase, where we deploy our learned algorithm to solve new instances of the problem."
  },
  {
    "objectID": "lec6.html#learning-from-examples",
    "href": "lec6.html#learning-from-examples",
    "title": "Intro to learning",
    "section": "Learning from Examples",
    "text": "Learning from Examples\n\n\n\nExample\n\n\nImagine you find an ancient mathematics text: \\[\\begin{aligned}\n    2 \\star 3 &= 36\\nonumber \\\\\n    7 \\star 1 &= 49\\nonumber \\\\\n    5 \\star 2 &= 100\\nonumber \\\\\n    2 \\star 2 &= 16\\nonumber\n\\end{aligned}\n\\]\n\n\n\n\n\n\nQuestion\n\n\nWhat do you think \\(\\star\\) represents?"
  },
  {
    "objectID": "lec6.html#learning-from-examples-1",
    "href": "lec6.html#learning-from-examples-1",
    "title": "Intro to learning",
    "section": "Learning from Examples",
    "text": "Learning from Examples\n\n\nFigure¬†2: How your brain may have solved the star problem."
  },
  {
    "objectID": "lec6.html#learning-from-examples-2",
    "href": "lec6.html#learning-from-examples-2",
    "title": "Intro to learning",
    "section": "Learning from Examples",
    "text": "Learning from Examples\n\n\n\nTip\n\n\nThis kind of learning, where you observe example input-output behavior and infer a functional mapping that explains this behavior, is called supervised learning.\nAnother name for this kind of learning is fitting a model to data.\n\n\n\n\n\n\n\n\n\nNon-computability\n\n\nSome things are not learnable from examples, such as noncomputable functions. An example of a noncomputable function is a function that takes as input a program and outputs a 1 if the program will eventually finish running, and a 0 if it will run forever."
  },
  {
    "objectID": "lec6.html#learning-from-examples-3",
    "href": "lec6.html#learning-from-examples-3",
    "title": "Intro to learning",
    "section": "Learning from Examples",
    "text": "Learning from Examples\n\n\n\nExample - formal definition\n\n\nA formal definition of example, is an {input, output} pair.\nThe examples you were given for \\(\\star\\) consisted of four such pairs:\n\\[\\begin{aligned}\n    &\\{\\texttt{input:} [2,3], \\texttt{output:} 36\\}\\nonumber \\\\\n    &\\{\\texttt{input:} [7,1], \\texttt{output:} 49\\}\\nonumber \\\\\n    &\\{\\texttt{input:} [5,2], \\texttt{output:} 100\\}\\nonumber \\\\\n    &\\{\\texttt{input:} [2,2], \\texttt{output:}16\\}\\nonumber\n\\end{aligned}\\]"
  },
  {
    "objectID": "lec6.html#learning-from-examples-4",
    "href": "lec6.html#learning-from-examples-4",
    "title": "Intro to learning",
    "section": "Learning from Examples",
    "text": "Learning from Examples\n\n\nFigure¬†3: A complicated function that could be learned from examples."
  },
  {
    "objectID": "lec6.html#learning-without-examples",
    "href": "lec6.html#learning-without-examples",
    "title": "Intro to learning",
    "section": "Learning without Examples",
    "text": "Learning without Examples\n\n\n\nUnsupervised learning\n\n\n\ninput data \\(\\{x^{(i)}\\}^N_{i=1}\\) - given\ntarget outputs \\(\\{y^{(i)}\\}^N_{i=1}\\) - unknown.\n\nLearner has to come up with a model or representation of the input data that has useful properties, as measured by some objective function.\n\n\n\n\n\n\nReinforcement learning\n\n\nWe suppose that we are given an reward function: \\[\nr: \\mathcal{Y} \\rightarrow \\mathbb{R}.\n\\]\nThe learner tries to come up with a function that maximizes rewards.\n\n\n\n\n\n\n\n\n\nDifference\n\n\nUnsupervised learning has access to training data whereas reinforcement learning usually does not; instead the reinforcement learner has to collect its own training data."
  },
  {
    "objectID": "lec6.html#key-ingredients",
    "href": "lec6.html#key-ingredients",
    "title": "Intro to learning",
    "section": "Key Ingredients",
    "text": "Key Ingredients\n\n\n\nIngredients\n\n\n\nWhat does it mean for the learner to succeed, or, at least, to perform well?\nWhat is the set of possible mappings from inputs to outputs that we will search over?\nHow, exactly, do we search the hypothesis space for a specific mapping that maximizes the objective?"
  },
  {
    "objectID": "lec6.html#key-ingredients-1",
    "href": "lec6.html#key-ingredients-1",
    "title": "Intro to learning",
    "section": "Key Ingredients",
    "text": "Key Ingredients\n\n\n\nLearner‚Äôs algorithm\n\n\n\\[\nf: \\mathcal{X} \\rightarrow \\mathcal{Y},\n\\] Commonly, \\(f\\) is referred to as the learned function.\n\n\n\n\n\n\nLearner‚Äôs objective\n\n\nFunction that scores model outputs: \\[\n\\mathcal{L}: \\mathcal{Y} \\rightarrow \\mathbb{R},\n\\] or function that compares model outputs to target answers: \\[\n\\mathcal{L}: \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow \\mathbb{R}.\n\\]\nWe will interchangeably call this \\(\\mathcal{L}\\) either the objective function, the loss function, or the loss."
  },
  {
    "objectID": "lec6.html#hypothesis-space",
    "href": "lec6.html#hypothesis-space",
    "title": "Intro to learning",
    "section": "Hypothesis space",
    "text": "Hypothesis space\n\n\n\nDescription\n\n\nThe hypothesis space is a set \\(\\mathcal{F}\\) of all the possible functions considered by the learner.\nExamples:\n\nAll mappings from \\(\\mathbb{R}^2 \\rightarrow \\mathbb{R}\\)\nAll functions \\(\\mathbb{R} \\times \\mathbb{R} \\rightarrow \\mathbb{R}_{\\geq 0}\\) that satisfy the conditions of being a distance metric.\n\n\n\n\n\n\n\nParameterization\n\n\nWe may say that our parameterized hypothesis space is \\[\ny = \\theta_1 x + \\theta_0m\n\\] where \\(\\theta_0\\) and \\(\\theta_1\\) are the parameters.\nThis example corresponds to the space of affine functions from \\(\\mathbb{R} \\rightarrow \\mathbb{R}\\)."
  },
  {
    "objectID": "lec6.html#hypothesis-space-1",
    "href": "lec6.html#hypothesis-space-1",
    "title": "Intro to learning",
    "section": "Hypothesis space",
    "text": "Hypothesis space\n\n\n\nParameterization: another way\n\n\nAnother choice could be \\[\ny = \\theta_2\\theta_1 x + \\theta_0,\n\\] with parameters \\(\\theta_0\\), \\(\\theta_1\\), and \\(\\theta_2\\): same space, but different parameterizations!\n\n\n\n\n\nOverparameterized models, where you use more parameters than the minimum necessary to fit the data, are especially important in modern computer vision; most neural networks are overparameterized."
  },
  {
    "objectID": "lec6.html#empirical-risk-minimization",
    "href": "lec6.html#empirical-risk-minimization",
    "title": "Intro to learning",
    "section": "Empirical Risk Minimization",
    "text": "Empirical Risk Minimization\n\n\n\nERM model\n\n\nLearn a function predicting \\(\\mathbf{y}\\) from \\(\\mathbf{x}\\) given many training examples \\(\\{\\mathbf{x}^{(i)},\\mathbf{y}^{(i)}\\}^N_{i=1}\\).\nThe idea is to minimize the average error (i.e., risk) we incur over all the training data (i.e., empirical distribution). The ERM problem is stated as follows:\n\\[\n\\begin{aligned}\n    \\mathop{\\mathrm{arg\\,min}}_{f \\in \\mathcal{F}} \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}(f(\\mathbf{x}^{(i)}),\\mathbf{y}^{(i)}) \\quad\\triangleleft\\quad \\text{ERM}\n\\end{aligned}\n\\]\nHere, \\(\\mathcal{F}\\) is the hypothesis space, \\(\\mathcal{L}\\) is the loss function, and \\(\\{\\mathbf{x}^{(i)}, \\mathbf{y}^{(i)}\\}_{i=1}^N\\) is the training data (example {input, output} pairs), and \\(f\\) is the learned function."
  },
  {
    "objectID": "lec6.html#learning-as-probabilistic-inference",
    "href": "lec6.html#learning-as-probabilistic-inference",
    "title": "Intro to learning",
    "section": "Learning as Probabilistic Inference",
    "text": "Learning as Probabilistic Inference\n\n\n\nProbabilistic inference\n\n\nWe can interpret ERM as doing maximum likelihood probabilistic inference.\nIn this interpretation, we are trying to infer the hypothesis \\(f\\) that assigns the highest probability to the data.\nFor a model that predicts \\(\\mathbf{y}\\) given \\(\\mathbf{x}\\), the max likelihood \\(f\\) is:\n\\[\\begin{aligned}\n    \\mathop{\\mathrm{arg\\,max}}_f p\\big(\\{\\mathbf{y}^{(i)}\\}_{i=1}^N \\bigm | \\{\\mathbf{x}^{(i)}\\}_{i=1}^N, f\\big) \\quad\\quad \\triangleleft \\quad\\text{Max likelihood learning}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lec6.html#learning-as-probabilistic-inference-1",
    "href": "lec6.html#learning-as-probabilistic-inference-1",
    "title": "Intro to learning",
    "section": "Learning as Probabilistic Inference",
    "text": "Learning as Probabilistic Inference\n\n\n\nProbabilistic inference\n\n\nThe term \\(p\\big(\\{\\mathbf{y}^{(i)}\\}_{i=1}^N \\bigm | \\{\\mathbf{x}^{(i)}\\}_{i=1}^N, f\\big)\\) is called the likelihood of the \\(\\mathbf{y}\\) values given the model \\(f\\) and the observed \\(\\mathbf{x}\\) values, and maximizing this quantity is called maximum likelihood learning.\n\n\n\n\n\n\nMAP learning\n\n\nWhen a prior \\(p(f)\\) is used in conjunction with a likelihood function, we arrive at maximum a posteriori learning (MAP learning), which infers the most probable hypothesis given the training data:\n\\[\\begin{aligned}\n    &\\mathop{\\mathrm{arg\\,max}}_f p\\big(f \\bigm | \\{\\mathbf{x}^{(i)}, \\mathbf{y}^{(i)}\\}_{i=1}^N\\big) \\quad\\quad \\triangleleft \\quad \\text{MAP learning}\\\\\n    & = \\mathop{\\mathrm{arg\\,max}}_f p\\big(\\{\\mathbf{y}^{(i)}\\}_{i=1}^N \\bigm | \\{\\mathbf{x}^{(i)}\\}_{i=1}^N, f\\big)p\\big(f\\big) \\quad\\quad \\triangleleft \\quad \\text{by Bayes' rule}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lec6.html#linear-least-squares-regression",
    "href": "lec6.html#linear-least-squares-regression",
    "title": "Intro to learning",
    "section": "Linear Least-Squares Regression",
    "text": "Linear Least-Squares Regression\n\n\nFigure¬†4: The goal of learning is to use the training data to predict the \\(y\\) value of the test query. In our example we find that for every 1 degree increase in temperature, we can expect \\(\\sim 10\\) more people to go to the beach."
  },
  {
    "objectID": "lec6.html#linear-least-squares-regression-1",
    "href": "lec6.html#linear-least-squares-regression-1",
    "title": "Intro to learning",
    "section": "Linear Least-Squares Regression",
    "text": "Linear Least-Squares Regression\n\n\n\nHypothesis space\n\n\nThe relationship between \\(x\\) and our predictions \\(\\hat{y}\\) of \\(y\\) has the form \\(\\hat{y} = f_{\\theta}(x) = \\theta_1 x + \\theta_0\\).\n\n\n\n\n\n\nParameterization\n\n\nThis hypothesis space is parameterized by a two scalars, \\(\\theta_0, \\theta_1 \\in \\mathbb{R}\\), the intercept and slope of the line.\nWe denote \\(\\theta = [\\theta_0, \\theta_1]\\).\n\n\n\n\n\n\n\n\n\nImportant\n\n\nLearning consists of finding the value of these parameters that maximizes the objective."
  },
  {
    "objectID": "lec6.html#linear-least-squares-regression-2",
    "href": "lec6.html#linear-least-squares-regression-2",
    "title": "Intro to learning",
    "section": "Linear Least-Squares Regression",
    "text": "Linear Least-Squares Regression\n\n\n\nObjective\n\n\nOur objective is that \\((\\hat{y}^{(i)} - y^{(i)})^2\\) should be small for all training examples \\(\\{x^{(i)}, y^{(i)}\\}_{i=1}^N\\). We call this objective the \\(L_2\\) loss:\n\\[\\begin{aligned}\n    J(\\theta) &= \\sum_i \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)})\\\\\n    &\\quad \\mathcal{L}(\\hat{y}, y) = (\\hat{y} - y)^2 \\quad\\quad \\triangleleft \\quad L_2 \\text{ loss}\n\\end{aligned}\\]\n\n\n\n\n\n\n\n\n\nNote\n\n\n\\(J(\\theta)\\) will denote the total objective; \\(\\mathcal{L}\\) will denote the loss per datapoint. That is \\[\nJ(\\theta) = \\sum_{i=1}^N \\mathcal{L}(f_{\\theta}(x^{(i)}), y^{(i)}).\n\\]"
  },
  {
    "objectID": "lec6.html#linear-least-squares-regression-3",
    "href": "lec6.html#linear-least-squares-regression-3",
    "title": "Intro to learning",
    "section": "Linear Least-Squares Regression",
    "text": "Linear Least-Squares Regression\n\n\n\nProblem statement\n\n\nThe full learning problem is as follows: \\[\\begin{aligned}\n    \\theta^* = \\mathop{\\mathrm{arg\\,min}}_{\\theta} \\sum_{i=1}^N (\\theta_1 x^{(i)} + \\theta_0 - y^{(i)})^2.\n\\end{aligned}\n\\]\n\n\n\n\n\n\nRandom solution?\n\n\nA first idea might be ‚Äútry a bunch of random values for \\(\\theta\\) and return the one that maximizes the objective.‚Äù\nWill be slow!"
  },
  {
    "objectID": "lec6.html#linear-least-squares-regression-4",
    "href": "lec6.html#linear-least-squares-regression-4",
    "title": "Intro to learning",
    "section": "Linear Least-Squares Regression",
    "text": "Linear Least-Squares Regression\n\n\n\nCalculus way\n\n\nWe are trying to find the minimum of the objective \\(J(\\theta)\\):\n\\[\\begin{aligned}\n    J(\\theta) = \\sum_{i=1}^N (\\theta_1 x^{(i)} + \\theta_0 - y^{(i)})^2.\n\\end{aligned}\\]\nThis function can be rewritten as \\[\\begin{aligned}\n    J(\\theta) = (\\mathbf{y} - \\mathbf{X}\\theta)^\\mathsf{T}(\\mathbf{y} - \\mathbf{X}\\theta),\n\\end{aligned}\n\\] \\[\\begin{aligned}\n\\mathbf{X} =\n\\begin{bmatrix}\n    1 & x^{(1)}  \\\\\n    1 & x^{(2)} \\\\\n    \\vdots & \\vdots \\\\\n    1 & x^{(N)}\n\\end{bmatrix}\n\\quad\n\\mathbf{y} =\n\\begin{bmatrix}\n    y^{(1)}  \\\\\n    y^{(2)} \\\\\n    \\vdots \\\\\n    y^{(N)}\n\\end{bmatrix}\n\\quad\n\\theta =\n\\begin{bmatrix}\n    \\theta_0 \\\\\n    \\theta_1\n\\end{bmatrix}.\n\\end{aligned}\\]"
  },
  {
    "objectID": "lec6.html#linear-least-squares-regression-5",
    "href": "lec6.html#linear-least-squares-regression-5",
    "title": "Intro to learning",
    "section": "Linear Least-Squares Regression",
    "text": "Linear Least-Squares Regression\n\n\n\nCalculus way\n\n\nThe \\(J\\) is a quadratic form, which has a single global minimum where the derivative is zero, and no other points where the derivative is zero. The derivative is: \\[\\begin{aligned}\n    \\frac{\\partial J(\\theta)}{\\partial \\theta} =  2(\\mathbf{X}^\\mathsf{T}\\mathbf{X} \\theta - \\mathbf{X}^\\mathsf{T}\\mathbf{y}).\n\\end{aligned}\n\\]\nWe set this derivative to zero and solve for \\(\\theta^*\\): \\[\n\\begin{aligned}\n    &2(\\mathbf{X}^\\mathsf{T}\\mathbf{X} \\theta^* - \\mathbf{X}^\\mathsf{T}\\mathbf{y}) = 0\\\\\n&\\mathbf{X}^\\mathsf{T}\\mathbf{X} \\theta^* = \\mathbf{X}^\\mathsf{T}\\mathbf{y}\\\\\n&\\theta^* = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1}\\mathbf{X}^\\mathsf{T}\\mathbf{y}.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lec6.html#linear-least-squares-regression-6",
    "href": "lec6.html#linear-least-squares-regression-6",
    "title": "Intro to learning",
    "section": "Linear Least-Squares Regression",
    "text": "Linear Least-Squares Regression\n\n\nFigure¬†5: The \\(\\theta^*\\) defines the best fitting line to our data. A best fit line is a visualization of a function \\(f_{\\theta}\\), that predicts the \\(y\\)-value for each input \\(x\\)-value."
  },
  {
    "objectID": "lec6.html#linear-least-squares-regression-7",
    "href": "lec6.html#linear-least-squares-regression-7",
    "title": "Intro to learning",
    "section": "Linear Least-Squares Regression",
    "text": "Linear Least-Squares Regression\n\n\nFigure¬†6: Linear regression finds a line that predicts the training data‚Äôs \\(y\\)-values from its \\(x\\)-values."
  },
  {
    "objectID": "lec6.html#program-induction",
    "href": "lec6.html#program-induction",
    "title": "Intro to learning",
    "section": "Program Induction",
    "text": "Program Induction\n\n\n\nProgram induction\n\n\nProgram induction: one of the broadest classes of learning algorithm. In this setting, our hypothesis space may be all Python programs.\n\n\n\n\n\nFigure¬†7: Linear regression finds a line that predicts the training data‚Äôs \\(y\\)-values from its \\(x\\)-values."
  },
  {
    "objectID": "lec6.html#program-induction-2",
    "href": "lec6.html#program-induction-2",
    "title": "Intro to learning",
    "section": "Program Induction",
    "text": "Program Induction\n\n\nFigure¬†8: Python program induction finds a Python program that predicts the training data‚Äôs \\(y\\)-values from its \\(x\\)-values."
  },
  {
    "objectID": "lec6.html#classification-and-softmax-regression",
    "href": "lec6.html#classification-and-softmax-regression",
    "title": "Intro to learning",
    "section": "Classification and Softmax Regression",
    "text": "Classification and Softmax Regression\n\n\n\nDefinition\n\n\nA common problem in computer vision is to recognize objects. Our input is an image \\(\\mathbf{x}\\), and our target output is a class label \\(\\mathbf{y}\\)\n\n\n\n\n\nFigure¬†9: Image classification."
  },
  {
    "objectID": "lec6.html#classification-and-softmax-regression-1",
    "href": "lec6.html#classification-and-softmax-regression-1",
    "title": "Intro to learning",
    "section": "Classification and Softmax Regression",
    "text": "Classification and Softmax Regression\n\n\n\nInput\n\n\n\\[\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times 3},\n\\] where \\(H\\) is image height and \\(W\\) is image width.\n\n\n\n\n\n\nOutput\n\n\nLet \\(\\mathbf{y}\\) be a \\(K\\)-dimensional vector, for \\(K\\) possible classes, such that: \\[\ny_k = \\begin{cases}1, \\; \\text{if} \\; \\mathbf{y} \\; \\text{represents class}\\; k, \\\\\n0, \\; \\text{otherwise}\\end{cases}\n\\] This representation is called a one-hot code."
  },
  {
    "objectID": "lec6.html#classification-and-softmax-regression-2",
    "href": "lec6.html#classification-and-softmax-regression-2",
    "title": "Intro to learning",
    "section": "Classification and Softmax Regression",
    "text": "Classification and Softmax Regression\n\n\n\nGoal\n\n\nLearn a function \\(f_{\\theta}\\) that output vectors \\(\\hat{\\mathbf{y}}\\) that match the one-hot codes, thereby correctly classifying the input images.\n\n\n\n\nAn example of one-hot codes for representing \\(K\\)=5 different classes."
  },
  {
    "objectID": "lec6.html#classification-and-softmax-regression-3",
    "href": "lec6.html#classification-and-softmax-regression-3",
    "title": "Intro to learning",
    "section": "Classification and Softmax Regression",
    "text": "Classification and Softmax Regression\n\n\n\nLoss function - version 1\n\n\nPerhaps we should minimize misclassifications? That would correspond to the so called 0-1 loss:\n\\[\n\\begin{aligned}\n    \\mathcal{L}(\\hat{\\mathbf{y}},\\mathbf{y}) = \\mathbb{1}(\\hat{\\mathbf{y}}\\neq\\mathbf{y}),\n\\end{aligned}\n\\] where \\(\\mathbb{1}\\) is the indicator function that evaluates to 1 if and only if its argument is true, and 0 otherwise. Unfortunately, minimizing this loss is a discrete optimization problem, and it is NP-hard."
  },
  {
    "objectID": "lec6.html#classification-and-softmax-regression-4",
    "href": "lec6.html#classification-and-softmax-regression-4",
    "title": "Intro to learning",
    "section": "Classification and Softmax Regression",
    "text": "Classification and Softmax Regression\n\n\n\nLoss function - version 2\n\n\nInstead, people commonly use the cross-entropy loss, which is continuous and differentiable (making it easier to optimize): \\[\\begin{aligned}\n    \\mathcal{L}(\\hat{\\mathbf{y}},\\mathbf{y}) = H(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{k=1}^K y_k \\log \\hat{y}_k \\quad\\quad \\triangleleft \\quad \\text{cross-entropy loss}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lec6.html#classification-and-softmax-regression-5",
    "href": "lec6.html#classification-and-softmax-regression-5",
    "title": "Intro to learning",
    "section": "Classification and Softmax Regression",
    "text": "Classification and Softmax Regression\n\n\n\nInterpretation\n\n\n\\(\\hat{y}_k\\) should represent the probability we think the image is an image of class \\(k\\). Under that interpretation, minimizing cross-entropy maximizes the log likelihood of the ground truth observation \\(\\mathbf{y}\\) under our model‚Äôs prediction \\(\\hat{\\mathbf{y}}\\). \\(\\hat{y}\\) should represent a pmf.\n\n\n\n\n\n\nProbability mass function (pmf)\n\n\nA pmf \\(\\mathbf{p}\\), over \\(K\\) classes, is defined as a \\(K\\)-dimensional vector with elements in the range \\([0,1]\\) that sums to 1. In other words, \\(\\mathbf{p}\\) is a point on the \\((K-1)\\)-simplex, which we denote as \\(\\mathbf{p} \\in \\vartriangle^{K-1}\\).\n\n\n\n\n\nThe \\((K-1)\\)-simplex, \\(\\vartriangle^{K-1}\\), is the set of all \\(K\\)-dimensional vectors whose elements sum to 1. \\(K\\)-dimensional one-hot codes live on the vertices of \\(\\vartriangle^{K-1}\\)."
  },
  {
    "objectID": "lec6.html#classification-and-softmax-regression-6",
    "href": "lec6.html#classification-and-softmax-regression-6",
    "title": "Intro to learning",
    "section": "Classification and Softmax Regression",
    "text": "Classification and Softmax Regression\n\n\n\nProcedure\n\n\nTo ensure that the output of our learned function \\(f_{\\theta}\\) has this property, i.e., \\(f_{\\theta} \\in \\vartriangle^{K-1}\\), we can compose two steps:\n\nfirst apply a function \\(z_{\\theta}: \\mathcal{X} \\rightarrow \\mathbb{R}^K\\)\nthen squash the output into the range \\([0,1]\\) and normalize it to sum to 1."
  },
  {
    "objectID": "lec6.html#classification-and-softmax-regression-7",
    "href": "lec6.html#classification-and-softmax-regression-7",
    "title": "Intro to learning",
    "section": "Classification and Softmax Regression",
    "text": "Classification and Softmax Regression\n\n\n\nSquashing\n\n\nA popular way to squash is via the softmax function:\n\\[\\begin{aligned}\n    &\\mathbf{z} = z_{\\theta}(\\mathbf{x})\\\\\n    &\\hat{\\mathbf{y}} = \\texttt{softmax}(\\mathbf{z})\\\\\n    &\\quad \\quad \\hat{y}_j = \\frac{e^{-z_j}}{\\sum_{i=1}^K e^{-z_k}}.\n\\end{aligned}\\]\nThe values in \\(\\mathbf{z}\\) are called the logits.\n\n\n\n\n\n\n\n\n\nNote\n\n\nNow we have:\n\\[\\begin{aligned}\n    \\hat{\\mathbf{y}} = f_{\\theta}(\\mathbf{x}) = \\texttt{softmax}(z_{\\theta}(\\mathbf{x}))\n\\end{aligned}\\]"
  },
  {
    "objectID": "lec6.html#classification-and-softmax-regression-8",
    "href": "lec6.html#classification-and-softmax-regression-8",
    "title": "Intro to learning",
    "section": "Classification and Softmax Regression",
    "text": "Classification and Softmax Regression\n\n\nFigure¬†10: Softmax regression for image classification."
  },
  {
    "objectID": "lec6.html#classification-and-softmax-regression-9",
    "href": "lec6.html#classification-and-softmax-regression-9",
    "title": "Intro to learning",
    "section": "Classification and Softmax Regression",
    "text": "Classification and Softmax Regression\n\n\nFigure¬†11: Learning is a meta-algorithm, an algorithm that outputs algorithms; metalearning is just learning applied to learning, and therefore it is a meta-meta-algorithm."
  },
  {
    "objectID": "lec6.html#learning-to-learn",
    "href": "lec6.html#learning-to-learn",
    "title": "Intro to learning",
    "section": "Learning to Learn",
    "text": "Learning to Learn\n\n\n\nMetalearning\n\n\nIt‚Äôs a special case of learning where the hypothesis space is learning algorithms.\nThe goal of metalearning is to handle the case where the future problem we will encounter is itself a learning problem.\n\n\n\n\n\n\nExample\n\n\nSuppose that we are given the following {input, output} examples:\n\\[\\begin{aligned}\n    &\\{\\texttt{input:} \\big(x:[1,2], y:[1,2]\\big), &&\\texttt{output:} y = x\\}\\nonumber \\\\\n    &\\{\\texttt{input:} \\big(x:[1,2], y:[2,4]\\big), &&\\texttt{output:} y = 2x\\}\\nonumber \\\\\n    &\\{\\texttt{input:} \\big(x:[1,2], y:[0.5,1]\\big), &&\\texttt{output:} y = \\frac{x}{2}\\}\\nonumber\n\\end{aligned}\\]\nThe learner can fit these examples by learning to perform least-squares regression."
  },
  {
    "objectID": "lec6.html#learning-to-learn-1",
    "href": "lec6.html#learning-to-learn-1",
    "title": "Intro to learning",
    "section": "Learning to Learn",
    "text": "Learning to Learn\n\n\nFigure¬†12: Learning is a meta-algorithm, an algorithm that outputs algorithms; metalearning is learning applied to learning: a meta-meta-algorithm."
  },
  {
    "objectID": "lec6.html#learning-to-learn-2",
    "href": "lec6.html#learning-to-learn-2",
    "title": "Intro to learning",
    "section": "Learning to Learn",
    "text": "Learning to Learn\n\n\n\nRecursion\n\n\nNotice that you can apply this idea recursively, constructing meta-meta-...-metalearners.\nHumans perform at least three levels of this process, if not more: we have evolved to be taught in school how to learn quickly on our own.\n\n\n\n\n\nEvolution is a learning algorithm according to our present definition."
  },
  {
    "objectID": "lec6.html#gradient-based-learning-algorithms-1",
    "href": "lec6.html#gradient-based-learning-algorithms-1",
    "title": "Intro to learning",
    "section": "Gradient-Based Learning Algorithms",
    "text": "Gradient-Based Learning Algorithms\n\n\n\nSetting\n\n\nWe consider the task of minimizing a cost function \\(J: \\cdot \\rightarrow \\mathbb{R}\\), which is a function that maps some arbitrary input to a scalar cost.\nIn learning problems, the domain of \\(J\\) is the training data and the parameters \\(\\theta\\). We will often consider the training data to be fixed and only denote the objective as a function of the parameters, \\(J(\\theta)\\). Our goal is to solve: \\[\n\\theta^* = \\arg\\min_{\\theta} J(\\theta)\n\\]"
  },
  {
    "objectID": "lec6.html#gradient-based-learning-algorithms-2",
    "href": "lec6.html#gradient-based-learning-algorithms-2",
    "title": "Intro to learning",
    "section": "Gradient-Based Learning Algorithms",
    "text": "Gradient-Based Learning Algorithms\n\n\nFigure¬†13: General optimization loop."
  },
  {
    "objectID": "lec6.html#gradient-based-learning-algorithms-3",
    "href": "lec6.html#gradient-based-learning-algorithms-3",
    "title": "Intro to learning",
    "section": "Gradient-Based Learning Algorithms",
    "text": "Gradient-Based Learning Algorithms\n\n\n\nZeroth-order optimization\n\n\nThe update function only gets to observe the value \\(J(\\theta)\\). The only way, then, to find \\(\\theta\\)‚Äôs that minimize the loss is to sample different values for \\(\\theta\\) and move toward the values that are lower.\n\n\n\n\n\n\nFirst-order optimization\n\n\nAlso called gradient-based optimization: the update function takes as input the gradient of the cost with respect to the parameters at the current operating point, \\(\\nabla_{\\theta}J(\\theta)\\). This reveals the direction.\n\n\n\n\n\nHigher-order optimization methods observe higher-order derivatives of the loss, such as the Hessian \\(H\\), which tells you how the landscape is locally curving."
  },
  {
    "objectID": "lec6.html#gradient-based-learning-algorithms-4",
    "href": "lec6.html#gradient-based-learning-algorithms-4",
    "title": "Intro to learning",
    "section": "Gradient-Based Learning Algorithms",
    "text": "Gradient-Based Learning Algorithms\n\nGradient descent GD. Optimizing a cost function \\(J: \\theta \\rightarrow \\mathbb{R}\\) by descending the gradient \\(\\nabla_{\\theta} J\\)."
  },
  {
    "objectID": "lec6.html#gradient-based-learning-algorithms-5",
    "href": "lec6.html#gradient-based-learning-algorithms-5",
    "title": "Intro to learning",
    "section": "Gradient-Based Learning Algorithms",
    "text": "Gradient-Based Learning Algorithms\n\n\n\nHyperparameters\n\n\n\nlearning rate \\(\\eta\\), which controls the step size (learning rate times gradient magnitude)\nthe number of steps \\(K\\).\n\nIf the learning rate is sufficiently small and the initial parameter vector \\(\\theta^0\\) is random, then this algorithm will almost surely converge to a local minimum of \\(J\\) as \\(K \\rightarrow \\infty\\)."
  },
  {
    "objectID": "lec6.html#gradient-based-learning-algorithms-6",
    "href": "lec6.html#gradient-based-learning-algorithms-6",
    "title": "Intro to learning",
    "section": "Gradient-Based Learning Algorithms",
    "text": "Gradient-Based Learning Algorithms\n\n\n\nLearning Rate Schedules\n\n\nWe are calling some function \\(\\texttt{lr}(\\eta^0,k)\\) to get the learning rate on each iteration of descent: \\[\n\\eta^{k} = \\texttt{lr}(\\eta^0,k)\n\\] Generally, we want an update rule where \\(\\eta^{k+1} &lt; \\eta^k\\) (making smaller steps).\n\n\n\n\n\n\nExamples\n\n\n\\[\n\\begin{aligned}\n    \\texttt{lr}(\\eta^0,k) &= \\beta^{-k} \\eta^0 &\\quad\\quad \\triangleleft\\quad \\text{exponential decay}\\\\\n    \\texttt{lr}(\\eta^0,k) &= \\beta^{-\\lfloor k/M \\rfloor} \\eta^0 &\\quad\\quad \\triangleleft\\quad \\text{stepwise exponential decay}\\\\\n    \\texttt{lr}(\\eta^0,k) &= \\frac{(K - k)}{K} \\eta^0 &\\quad\\quad \\triangleleft\\quad \\text{linear decay}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lec6.html#gradient-based-learning-algorithms-7",
    "href": "lec6.html#gradient-based-learning-algorithms-7",
    "title": "Intro to learning",
    "section": "Gradient-Based Learning Algorithms",
    "text": "Gradient-Based Learning Algorithms\n\nGradient descent with learning rate decay algorithm."
  },
  {
    "objectID": "lec6.html#gradient-based-learning-algorithms-8",
    "href": "lec6.html#gradient-based-learning-algorithms-8",
    "title": "Intro to learning",
    "section": "Gradient-Based Learning Algorithms",
    "text": "Gradient-Based Learning Algorithms\n\n\n\nMomentum\n\n\nMomentum means that we set the parameter update to be a direction \\(\\mathbf{v}^{k+1}\\), given by a weighted combination of the previous update direction, \\(\\mathbf{v}^{k}\\), plus the current negative gradient: \\[\n\\mathbf{v}^{k+1} = \\mu \\mathbf{v}^{k} - \\eta\\nabla_{\\theta} J(\\theta^k)\n\\] The weight \\(\\mu\\) in this combination is a new hyperparameter, sometimes simply called the momentum."
  },
  {
    "objectID": "lec6.html#gradient-based-learning-algorithms-9",
    "href": "lec6.html#gradient-based-learning-algorithms-9",
    "title": "Intro to learning",
    "section": "Gradient-Based Learning Algorithms",
    "text": "Gradient-Based Learning Algorithms\n\nGradient descent with momentum algorithm."
  },
  {
    "objectID": "lec6.html#gradient-based-learning-algorithms-10",
    "href": "lec6.html#gradient-based-learning-algorithms-10",
    "title": "Intro to learning",
    "section": "Gradient-Based Learning Algorithms",
    "text": "Gradient-Based Learning Algorithms\n\n\nFigure¬†14: (left) A simple loss function \\(J = \\texttt{abs}(\\theta)\\). (right) Optimization trajectory for three different settings of momentum \\(\\mu\\). White line indicates value of the parameter at each iteration of optimization, starting at top and progressing to bottom. Color is value of the loss. Red dot is location where loss first reaches within \\(0.01\\) of optimal value."
  },
  {
    "objectID": "lec6.html#gradient-based-learning-algorithms-11",
    "href": "lec6.html#gradient-based-learning-algorithms-11",
    "title": "Intro to learning",
    "section": "Gradient-Based Learning Algorithms",
    "text": "Gradient-Based Learning Algorithms\n\n\nFigure¬†15"
  },
  {
    "objectID": "lec6.html#gradient-based-learning-algorithms-12",
    "href": "lec6.html#gradient-based-learning-algorithms-12",
    "title": "Intro to learning",
    "section": "Gradient-Based Learning Algorithms",
    "text": "Gradient-Based Learning Algorithms\n\n\n\nAlternatives\n\n\nWhat are some other good choices for \\(\\mathbf{v}\\)?\n\nOne common idea is to set \\(\\mathbf{v}\\) to be the gradient of a surrogate loss function, which is a function, \\(J_{\\texttt{surr}}\\), with meaningful (non-zero) gradients that approximates \\(J\\). An example might be a smoothed version of \\(J\\).\nAnother way to get \\(\\mathbf{v}\\) is to compute it by sampling perturbations of \\(\\theta\\), and seeing which perturbation leads to lower loss. In this strategy, we evaluate \\(J(\\theta+\\epsilon)\\) for a set of perturbations \\(\\epsilon\\), then move toward the \\(\\epsilon\\)‚Äôs that decreased the loss. Approaches of this kind are sometimes called evolution strategies."
  },
  {
    "objectID": "lec6.html#gradient-based-learning-algorithms-13",
    "href": "lec6.html#gradient-based-learning-algorithms-13",
    "title": "Intro to learning",
    "section": "Gradient-Based Learning Algorithms",
    "text": "Gradient-Based Learning Algorithms\n\nEvolution strategy algorithm."
  },
  {
    "objectID": "lec6.html#gradient-based-learning-algorithms-14",
    "href": "lec6.html#gradient-based-learning-algorithms-14",
    "title": "Intro to learning",
    "section": "Gradient-Based Learning Algorithms",
    "text": "Gradient-Based Learning Algorithms\n\n\nFigure¬†16: Using evolution strategies to minimize a nondifferentiable (zero-gradient) loss, using \\(\\sigma=1\\), \\(M=10\\), and \\(\\eta=0.02\\)."
  },
  {
    "objectID": "lec6.html#gradient-based-learning-algorithms-15",
    "href": "lec6.html#gradient-based-learning-algorithms-15",
    "title": "Intro to learning",
    "section": "Gradient-Based Learning Algorithms",
    "text": "Gradient-Based Learning Algorithms\n\n\n\nGradient Clipping\n\n\n\n\n\nGradient clipping algorithm.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nclip is the ‚Äúclipping‚Äù function: \\(\\texttt{clip}(v, -m, m) = \\max(\\min(v,m),-m)\\)"
  },
  {
    "objectID": "lec6.html#gradient-based-learning-algorithms-16",
    "href": "lec6.html#gradient-based-learning-algorithms-16",
    "title": "Intro to learning",
    "section": "Gradient-Based Learning Algorithms",
    "text": "Gradient-Based Learning Algorithms\n\n\n\nGradient Clipping\n\n\n\n\n\n\n\n\nFigure¬†17: Using GD with clipping to minimize a loss with exploding gradients, using \\(m=0.1\\)."
  },
  {
    "objectID": "lec6.html#stochastic-gradient-descent",
    "href": "lec6.html#stochastic-gradient-descent",
    "title": "Intro to learning",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\n\n\n\nExpensive computation\n\n\nFor typical learning problems, \\(\\nabla_{\\theta} J(\\theta, \\{\\mathbf{x}^{(i)}, \\mathbf{y}^{(i)}\\}_{i=1}^N)\\) decomposes as follows: \\[\n\\begin{align}\n    \\nabla_{\\theta} J(\\theta, \\{\\mathbf{x}^{(i)}, \\mathbf{y}^{(i)}\\}_{i=1}^N) &=\n    \\nabla_{\\theta} \\frac{1}{N}\\sum_{i=1}^N \\mathcal{L}(f_{\\theta}(\\mathbf{x}^{(i)}), \\mathbf{y}^{(i)}) = \\frac{1}{N}\\sum_{i=1}^N \\nabla_{\\theta} \\mathcal{L}(f_{\\theta}(\\mathbf{x}^{(i)}), \\mathbf{y}^{(i)})\n\\end{align}\n\\]\n\n\n\n\n\n\nBatching\n\n\nSuppose instead we randomly subsample a batch of terms from this sum, \\(\\{\\mathbf{x}^{(b)}, \\mathbf{y}^{(b)}\\}_{b=1}^B\\), where \\(B\\) is the batch size. We then compute an estimate of the total gradient as the average gradient over this batch as follows: \\[\n\\begin{align}\n    \\tilde{\\mathbf{g}} = \\frac{1}{N}\\sum_{b=1}^B \\nabla_{\\theta} \\mathcal{L}(f_{\\theta}(\\mathbf{x}^{(b)}), \\mathbf{y}^{(b)})\n\\end{align}\n\\]"
  },
  {
    "objectID": "lec6.html#stochastic-gradient-descent-1",
    "href": "lec6.html#stochastic-gradient-descent-1",
    "title": "Intro to learning",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\n\nStochastic gradient descent algorithm. Stochastic gradient descent estimates the gradient from a stochastic subset (batch) of the full training data, and makes an update on that basis."
  },
  {
    "objectID": "lec6.html#stochastic-gradient-descent-2",
    "href": "lec6.html#stochastic-gradient-descent-2",
    "title": "Intro to learning",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\n\n\n\nProperties\n\n\n\nBecause each step of descent is somewhat random, \\(\\texttt{SGD}\\) can jump over small bumps in the loss landscape, as long those bumps disappear for some randomly sampled batches\n\\(\\texttt{SGD}\\) can implicitly regularize the learning problem. For example, for linear problems (i.e., \\(f_\\theta\\) is linear), then if there are multiple parameter settings that minimize the loss, \\(\\texttt{SGD}\\) will often converge to the solution with minimum parameter norm"
  },
  {
    "objectID": "lec6.html#the-problem-of-generalization-1",
    "href": "lec6.html#the-problem-of-generalization-1",
    "title": "Intro to learning",
    "section": "The Problem of Generalization",
    "text": "The Problem of Generalization\n\n\n\nTrain vs test\n\n\nSo far, we have described learning as an optimization problem: maximize an objective over the training set. But this is not our actual goal. Our goal is to maximize the objective over the test set.\n\n\n\n\n\n\nOverfitting\n\n\nHappens when we fit to properties in the training data that do not exist in the test data.\nThis means that what we learned about the training data does not generalize to the test data.\n\n\n\n\n\n\nUnderfitting\n\n\nLearner failed to optimize the objective on the training data."
  },
  {
    "objectID": "lec6.html#the-problem-of-generalization-2",
    "href": "lec6.html#the-problem-of-generalization-2",
    "title": "Intro to learning",
    "section": "The Problem of Generalization",
    "text": "The Problem of Generalization\n\n\n\nPolynomial regression\n\n\nThe hypothesis space is polynomial functions rather than linear functions, that is, \\[\n\\begin{aligned}\ny = f_{\\theta}(x) = \\sum_{k=0}^K \\theta_k x^k\n\\end{aligned}\n\\]\nwhere \\(K\\), the degree of the polynomial, is a hyperparameter of the hypothesis space."
  },
  {
    "objectID": "lec6.html#the-problem-of-generalization-3",
    "href": "lec6.html#the-problem-of-generalization-3",
    "title": "Intro to learning",
    "section": "The Problem of Generalization",
    "text": "The Problem of Generalization\n\n\n\nEquivalence of polynomial and linear regression\n\n\nLet us consider the setting where we use the least-squares (\\(L_2\\)) loss function.\nWe can see this by rewriting the polynomial as:\n\\[\n\\sum_{k=0}^K \\theta_k x^k = \\theta^\\mathsf{T}\\phi(x), \\; \\phi(x) = \\begin{bmatrix}\n  1 \\\\ x \\\\ x^2 \\\\ \\vdots \\\\ x^K\n\\end{bmatrix}\n\\]\nNow the form of \\(f_{\\theta}\\) is \\(f_{\\theta}(x) = \\theta^\\mathsf{T}\\phi(x)\\), which is a linear function in the parameters \\(\\theta\\). Therefore, if we featurize \\(x\\), representing each datapoint \\(x\\) with a feature vector \\(\\phi(x)\\), then we have arrived at a linear regression problem in this feature space."
  },
  {
    "objectID": "lec6.html#the-problem-of-generalization-4",
    "href": "lec6.html#the-problem-of-generalization-4",
    "title": "Intro to learning",
    "section": "The Problem of Generalization",
    "text": "The Problem of Generalization\n\n\n\nEquivalence of polynomial and linear regression\n\n\nSo, the learning problem, and closed form optimizer, for \\(L_2\\) polynomial regression looks almost identical to that of \\(L_2\\) linear regression:\nwhere \\[\\mathbf{\\Phi} =\n     \\begin{bmatrix}\n        1 & x^{(1)} & x^{(1)^2} & ... & x^{(1)^K} \\\\\n        1 & x^{(2)} & x^{(2)^2} & ... & x^{(2)^K} \\\\\n        \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n        1 & x^{(N)} & x^{(N)^2} & ... & x^{(N)^K}  \\\\\n    \\end{bmatrix}\\]\nThe matrix \\(\\mathbf{\\Phi}\\) is an array of the features (columns) for each datapoint (rows). It plays the same role as data matrix \\(\\mathbf{X}\\) did earlier; in fact we often call matrices of the feature representations of each datapoint also as a data matrix."
  },
  {
    "objectID": "lec6.html#the-problem-of-generalization-5",
    "href": "lec6.html#the-problem-of-generalization-5",
    "title": "Intro to learning",
    "section": "The Problem of Generalization",
    "text": "The Problem of Generalization\n\n\nFigure¬†18: Underfitting and overfitting."
  },
  {
    "objectID": "lec6.html#the-problem-of-generalization-6",
    "href": "lec6.html#the-problem-of-generalization-6",
    "title": "Intro to learning",
    "section": "The Problem of Generalization",
    "text": "The Problem of Generalization\n\n\n\nData generating process\n\n\n\\[\n\\begin{aligned}\n    Y &= X^2 + 1 &\\triangleleft \\quad\\text{true underlying relationship}\\\\\n    \\epsilon &\\sim \\mathcal{N}(0,1) &\\triangleleft \\quad\\text{observation noise}\\\\\n    Y^\\prime &= Y + \\epsilon &\\triangleleft \\quad\\text{noisy observations}\\\\\n    x,y &\\sim p(X,Y^{\\prime}) &\\triangleleft \\quad\\text{data-generating process}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lec6.html#the-problem-of-generalization-7",
    "href": "lec6.html#the-problem-of-generalization-7",
    "title": "Intro to learning",
    "section": "The Problem of Generalization",
    "text": "The Problem of Generalization\n\n\n\nWhy does overfitting happen?\n\n\nIt‚Äôs because for \\(K=10\\) the curve can become wiggly enough to not just fit the true underlying relationship but also to fit the noise, the minor offsets \\(\\epsilon\\) around the green line.\nThis noise is a property of the training data that does not generalize to the test data; the test data will have different observation noise.\n\n\n\n\n\n\nMultiple hypotheses\n\n\nFor \\(K=10\\) there are many hypotheses (polynomial functions) that perfectly the data (true function + noise) ‚Äì there is insufficient data for the objective to uniquely identify one of the hypotheses to be the best. Because of this, the hypothesis output by the optimizer may be an arbitrary one."
  },
  {
    "objectID": "lec6.html#the-problem-of-generalization-8",
    "href": "lec6.html#the-problem-of-generalization-8",
    "title": "Intro to learning",
    "section": "The Problem of Generalization",
    "text": "The Problem of Generalization\n\n\n\nApproximation error\n\n\nLet \\(\\{x_{(\\texttt{train})}^{(i)}, y_{(\\texttt{train})}^{(i)}\\}_{i=1}^N\\) be our training data set (the black points). Then the approximation error \\(J_{\\texttt{approx}}\\) is defined as the total cost incurred on this training data:\n\\[\\begin{aligned}\n    J_{\\texttt{approx}} = \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}(f_{\\theta}(x_{(\\texttt{train})}^{(i)}), y_{(\\texttt{train})}^{(i)})\n\\end{aligned}\\] It is the gap between the black line and the training data points.\n\n\n\n\n\nNotice that approximation error is the cost function we minimize in empirical risk minimization."
  },
  {
    "objectID": "lec6.html#the-problem-of-generalization-9",
    "href": "lec6.html#the-problem-of-generalization-9",
    "title": "Intro to learning",
    "section": "The Problem of Generalization",
    "text": "The Problem of Generalization\n\n\n\nGeneralization error\n\n\nThe expected cost we would incur if we sampled a new test point at random from the true data generating process. Generalization error is often approximated by measuring performance on a heldout , \\(\\{x_{(\\texttt{val})}^{(i)}, y_{(\\texttt{val})}^{(i)}\\}_{i=1}^N\\), which can simply be a subset of the data that we don‚Äôt use for training or testing:\n\\[\\begin{aligned}\n    J_{\\texttt{gen}} &= \\mathbb{E}_{x,y \\sim p_{\\texttt{data}}} [ \\mathcal{L}(f_{\\theta}(x), y)]\\\\\n                        &\\approx \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}(f_{\\theta}(x_{(\\texttt{val})}^{(i)}), y_{(\\texttt{val})}^{(i)})\n\\end{aligned}\\] It is the gap between the black line and the green line."
  },
  {
    "objectID": "lec6.html#the-problem-of-generalization-10",
    "href": "lec6.html#the-problem-of-generalization-10",
    "title": "Intro to learning",
    "section": "The Problem of Generalization",
    "text": "The Problem of Generalization\n\n\nFigure¬†19: Approximation error approx versus generalization error gen for polynomial regression of order \\(K\\). Here we measured error as the proportion of validation points that are mispredicted (defined as having an \\(L_2\\) prediction error greater than 0.25)."
  },
  {
    "objectID": "lec6.html#regularization",
    "href": "lec6.html#regularization",
    "title": "Intro to learning",
    "section": "Regularization",
    "text": "Regularization\n\n\n\nGoldilocks principle\n\n\nWe should prefer hypotheses (functions \\(f\\)) that are sufficiently expressive to fit the data, but not so flexible that they can overfit the data."
  },
  {
    "objectID": "lec6.html#regularization-1",
    "href": "lec6.html#regularization-1",
    "title": "Intro to learning",
    "section": "Regularization",
    "text": "Regularization\n\n\n\nRegularization\n\n\nMechanisms that penalize function complexity so that we avoid learning too flexible a function that overfits. Regularizers embody the principle of Occam‚Äôs razor.\nThe general form of a regularized objective is: \\[\\begin{aligned}\n    J(\\theta) = \\overbrace{\\frac{1}{N} \\sum^N_{i=1} \\mathcal{L}(f_{\\theta}(x)^{(i)}, y^{(i)})}^\\text{data fit loss} + \\underbrace{\\lambda R(\\theta)}_\\text{regularizer} \\quad\\quad\\triangleleft \\quad\\text{regularized objective function}\n\\end{aligned} \\qquad(1)\\] where \\(\\lambda\\) is a hyperparameter that controls the strength of the regularization.\n\n\n\n\n\nBayesian Occam‚Äôs razor: more complex hypothesis spaces must cover more possible hypotheses, and therefore must assign less prior mass to any single hypothesis."
  },
  {
    "objectID": "lec6.html#regularization-3",
    "href": "lec6.html#regularization-3",
    "title": "Intro to learning",
    "section": "Regularization",
    "text": "Regularization\n\n\n\nNorm penalization\n\n\nOne of the most common regularizers is to penalize the \\(L_p\\) norm of the parameters of our model, \\(\\theta\\): \\[\nR(\\theta) = \\left\\lVert\\theta\\right\\rVert_{p}.\n\\]\nThe \\(L_p\\)-norm of \\(\\mathbf{x}\\) is \\((\\sum_i |x_i|^{p})^{\\frac{1}{p}}\\). The \\(L_2\\)-norm is the familiar least-squares objective."
  },
  {
    "objectID": "lec6.html#regularization-4",
    "href": "lec6.html#regularization-4",
    "title": "Intro to learning",
    "section": "Regularization",
    "text": "Regularization\n\n\n\nRegularizers as Probabilistic Priors\n\n\nRegularizers can be interpreted as priors that prefer, a priori (before looking at the data), some solutions over others.\n\nUnder this interpretation, the data fit loss (e.g., \\(L_2\\) loss) is a likelihood function \\(p(\\{y^{(i)}\\}^N_{i=1} \\bigm | \\{x^{(i)}\\}^N_{i=1}, \\theta)\\) and the regularizer is a prior \\(p(\\theta)\\).\nBayes‚Äô rule then states that the posterior \\(p(\\theta \\bigm | \\{x^{(i)}, y^{(i)}\\}^N_{i=1})\\) is proportional to the product of the prior and the likelihood. The log posterior is then the sum of the log likelihood and the log prior, plus a constant."
  },
  {
    "objectID": "lec6.html#regularization-5",
    "href": "lec6.html#regularization-5",
    "title": "Intro to learning",
    "section": "Regularization",
    "text": "Regularization\n\n\n\nRevisiting the \\(\\star\\) Problem\n\n\n\\[\\begin{aligned}\n    3 \\star 2 &= 36\\nonumber \\\\\n    7 \\star 1 &= 49\\nonumber \\\\\n    5 \\star 2 &= 100\\nonumber \\\\\n    2 \\star 2 &= 16\\nonumber\n\\end{aligned}\\]\n\nmaybe \\(x \\star y =  94.5x - 9.5x^2 + 4y^2 - 151\\)?\nor maybe\n\ndef star(x,y):\n    if x==2 && y==3:\n        return 36\n    elif x==7 && y==1:\n        return 49\n    elif x==5 && y==2:\n        return 100\n    elif x==2 && y==2:\n        return 16\n    else:\n        return 0"
  },
  {
    "objectID": "lec6.html#data-priors-and-hypotheses",
    "href": "lec6.html#data-priors-and-hypotheses",
    "title": "Intro to learning",
    "section": "Data, Priors, and Hypotheses",
    "text": "Data, Priors, and Hypotheses\n\n\n\nThree tools\n\n\n\ndata: observations of the world like photos and videos. Finding explanations consistent with the observed data is the centerpiece of learning-based vision.\npriors (a.k.a. regularizers): prefer some solutions over others a priori.\nSet of hypotheses under consideration for what the true function may be. The hypothesis space constrains which solutions we can possibly find."
  },
  {
    "objectID": "lec6.html#data-priors-and-hypotheses-1",
    "href": "lec6.html#data-priors-and-hypotheses-1",
    "title": "Intro to learning",
    "section": "Data, Priors, and Hypotheses",
    "text": "Data, Priors, and Hypotheses\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†20: A cartoon of the tools for honing in on the truth."
  },
  {
    "objectID": "lec6.html#data-priors-and-hypotheses-2",
    "href": "lec6.html#data-priors-and-hypotheses-2",
    "title": "Intro to learning",
    "section": "Data, Priors, and Hypotheses",
    "text": "Data, Priors, and Hypotheses\n\n\n\nExperiment 1: Effect of Data\n\n\nConsider the following empirical risk minimization problem: \\[\n\\begin{aligned}\n    J(\\theta; \\{x^{(i)}, y^{(i)}\\}^N_{i=1}) &= \\frac{1}{N}\\sum_i \\lvert f_{\\theta}(x^{(i)}) - y^{(i)}\\rvert^{0.25} \\quad\\quad \\triangleleft \\quad\\text{objective}:error_fn_1\\\\\n    f_{\\theta}(x) &= \\theta_0 x + \\theta_1 \\sin(x)  \\quad\\quad \\triangleleft \\quad\\text{hypothesis space}\n\\end{aligned}\n\\qquad(2)\\]"
  },
  {
    "objectID": "lec6.html#data-priors-and-hypotheses-3",
    "href": "lec6.html#data-priors-and-hypotheses-3",
    "title": "Intro to learning",
    "section": "Data, Priors, and Hypotheses",
    "text": "Data, Priors, and Hypotheses\n\n\nFigure¬†21: The more data you have, the less you need other modeling tools. ."
  },
  {
    "objectID": "lec6.html#data-priors-and-hypotheses-4",
    "href": "lec6.html#data-priors-and-hypotheses-4",
    "title": "Intro to learning",
    "section": "Data, Priors, and Hypotheses",
    "text": "Data, Priors, and Hypotheses\n\n\n\nExperiment 2: Effect of Priors\n\n\nWe will use a slightly different hypothesis space and objective function \\[\n\\begin{aligned}\n    J(\\theta; \\{x^{(i)}, y^{(i)}\\}^N_{i=1}) &= \\frac{1}{N}\\sum_i \\left\\lVert f_{\\theta}(x^{(i)}) - y^{(i)}\\right\\rVert_2^2 + \\lambda \\left\\lVert\\theta\\right\\rVert_2^2 \\quad\\quad \\triangleleft \\quad\\text{objective}\\\\\n    f_{\\theta}(x) &= \\theta_0 x + \\theta_1 x \\quad\\quad \\triangleleft \\quad\\text{hypothesis space}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lec6.html#data-priors-and-hypotheses-5",
    "href": "lec6.html#data-priors-and-hypotheses-5",
    "title": "Intro to learning",
    "section": "Data, Priors, and Hypotheses",
    "text": "Data, Priors, and Hypotheses\n\n\nFigure¬†22: More regularization, more (soft) constraints."
  },
  {
    "objectID": "lec6.html#data-priors-and-hypotheses-6",
    "href": "lec6.html#data-priors-and-hypotheses-6",
    "title": "Intro to learning",
    "section": "Data, Priors, and Hypotheses",
    "text": "Data, Priors, and Hypotheses\n\n\n\nExperiment 2: Effect of Priors\n\n\nYou can take away a few lessons from this example:\n\nPriors help only when they are good guesses as to the truth.\nOverreliance on the prior means ignoring the data, and this is generally a bad thing.\nFor any given prior, there is a sweet spot where the strength is optimal. Sometimes this ideal strength can be derived from modeling assumptions and other times you may need to tune it as a hyperparameter."
  },
  {
    "objectID": "lec6.html#data-priors-and-hypotheses-7",
    "href": "lec6.html#data-priors-and-hypotheses-7",
    "title": "Intro to learning",
    "section": "Data, Priors, and Hypotheses",
    "text": "Data, Priors, and Hypotheses\n\n\n\nExperiment 3: Effect of the Hypothesis Space\n\n\nConsider the following three hypothesis spaces: \\[\n\\begin{aligned}\n    f_{\\theta}(x) &= \\theta_0 x + \\theta_1 x^2 &\\triangleleft \\quad\\texttt{quadratic}\\\\\n    f_{\\theta}(x) &= \\theta_0 x &\\triangleleft \\quad\\texttt{linear}\\\\\n    f_{\\theta}(x) &= 0 &\\triangleleft \\quad\\texttt{constant}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lec6.html#data-priors-and-hypotheses-8",
    "href": "lec6.html#data-priors-and-hypotheses-8",
    "title": "Intro to learning",
    "section": "Data, Priors, and Hypotheses",
    "text": "Data, Priors, and Hypotheses\n\n\nFigure¬†23: Fewer hypotheses, more (hard) constraints"
  },
  {
    "objectID": "lec6.html#summary-of-the-experiments",
    "href": "lec6.html#summary-of-the-experiments",
    "title": "Intro to learning",
    "section": "Summary of the Experiments",
    "text": "Summary of the Experiments\n\n\n\n\n\n\nGeneral principle\n\n\n\nWhat can be achieved with any one of our tools can also be achieved with any other.\n\n\n\n\n\n\n\n\nHowever, note that the hypothesis space places hard constraints on our search; we cannot violate them. Data and priors apply soft constraints; we can violate them but we will pay a penalty."
  },
  {
    "objectID": "lec5.html#introduction",
    "href": "lec5.html#introduction",
    "title": "Filters and image derivatives",
    "section": "Introduction",
    "text": "Introduction\n\n\n\nBlur filters\n\n\nBlur filters are low-pass filters\n\nremove the high spatial-frequency content\nleave only the low-frequency spatial components.\n\nThe result is an image that has lost details and that looks blurry.\n\n\n\n\n\n\nBlur applications\n\n\n\nit can be used to reduce noise\nto reveal image structures at different scales\nupsampling and downsampling images."
  },
  {
    "objectID": "lec5.html#blur-filters-1",
    "href": "lec5.html#blur-filters-1",
    "title": "Filters and image derivatives",
    "section": "Blur filters",
    "text": "Blur filters\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†1: Original noisy image.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†2: Blurred image."
  },
  {
    "objectID": "lec5.html#blur-filters-2",
    "href": "lec5.html#blur-filters-2",
    "title": "Filters and image derivatives",
    "section": "Blur filters",
    "text": "Blur filters\n\n\n\nImplementation\n\n\nBlurring is implemented by computing local averages over small neighborhoods of input pixel values. This can be done by:\n\nconvolution (linear)\nanisotropic diffusion (non-linear)\nbilateral filtering (non-linear)"
  },
  {
    "objectID": "lec5.html#box-filter",
    "href": "lec5.html#box-filter",
    "title": "Filters and image derivatives",
    "section": "Box Filter",
    "text": "Box Filter\n\n\n\nBox filter - definition\n\n\nLet‚Äôs start with a very simple low-pass filter, the box filter. The box filter uses a box function as the convolution kernel. The box convolution kernel can be written as:\n\\[\n\\text{box}_{N,M} \\left[n,m \\right] =\n\\begin{cases}\n    1       & \\quad \\text{if } -N \\leq n \\leq N  \\; \\text{and} \\;   -M \\leq m \\leq M\\\\\n    0       & \\quad \\text{otherwise.}\n\\end{cases}\n\\]\nFiltering an input image, \\(\\ell_{\\text{in}}\\), with a box filter results in the following: \\[\\begin{aligned}\n\\ell_{\\text{out}} \\left[n,m\\right] &=\n\\text{box}_{N,M} \\left[n,m\\right] \\circ \\ell_{\\text{in}} \\left[n,m\\right]  \\\\\n&= \\sum_{k,l} \\ell_{\\text{in}} \\left[n-k,m-l \\right] \\text{box}_{N,M} \\left[k,l \\right] \\\\\n&= \\sum_{k=-N}^N \\sum_{l=-M}^M \\ell_{\\text{in}} \\left[n-k,m-l \\right]\n\\end{aligned}\\]"
  },
  {
    "objectID": "lec5.html#box-filter-1",
    "href": "lec5.html#box-filter-1",
    "title": "Filters and image derivatives",
    "section": "Box Filter",
    "text": "Box Filter\n\n\n\nNote\n\n\nThe box filter with \\(N=M=1\\) is:"
  },
  {
    "objectID": "lec5.html#box-filter-2",
    "href": "lec5.html#box-filter-2",
    "title": "Filters and image derivatives",
    "section": "Box Filter",
    "text": "Box Filter\n\n\n\nVisual examples\n\n\n\n\n\n\n\n\nFigure¬†3: Fig (a) Input image. (b) Blurring with a square, (c) a horizontal, and (d) a vertical line. Each color channel is filtered independently."
  },
  {
    "objectID": "lec5.html#box-filter-3",
    "href": "lec5.html#box-filter-3",
    "title": "Filters and image derivatives",
    "section": "Box Filter",
    "text": "Box Filter\n\n\n\nProperties\n\n\n\nLow-pass: it attenuates the high spatial-frequency content of the input image.\n2D box filter is separable and can be written as the convolution of two 1D kernels: \\[\n\\text{box}_{N,M} \\left[n,m \\right]  = \\text{box}_{N,0} \\circ \\text{box}_{0,M}.\n\\]\nDC gain: the gain it has for a constant value input (i.e., the lowest possible input frequency). If the input signal is a constant, i.e., \\(\\ell[n,m] = a\\), where \\(a\\) is a real number, the result of convolving the image with the box filter \\(h_{N,M}\\) is also a constant:\n\n\\[\n\\ell_{\\text{out}} \\left[n,m\\right] = \\sum_{k,l} a \\text{box}_{N,M} \\left[k,l \\right] = a \\sum_{k,l} \\text{box}_{N,M} \\left[k,l \\right]  = a (2N+1)(2M+1)\n\\]"
  },
  {
    "objectID": "lec5.html#box-filter-4",
    "href": "lec5.html#box-filter-4",
    "title": "Filters and image derivatives",
    "section": "Box Filter",
    "text": "Box Filter\n\n\n\n\n\n\nTip\n\n\nRemember that the DC value of a signal is its mean value. DC is an old name derived from Direct Current.\nIn general, the DC gain of an arbitrary filter \\(h [n,m]\\) is the sum of its kernel values:\n\\[\n\\text{DC gain} = \\sum_{n,m} h [n,m]\n\\] In the example of the box filter with \\(N=1\\), the DC gain is 3.\nIn the frequency domain, the DC gain of a filter refers to its gain at a frequency of 0, represented by the value \\(H[0,0]\\). The value of \\(\\left| \\text{Box}_1[0] \\right|\\) is \\(3\\). The DC gain of a filter will change the mean value of the input signal."
  },
  {
    "objectID": "lec5.html#box-filter-5",
    "href": "lec5.html#box-filter-5",
    "title": "Filters and image derivatives",
    "section": "Box Filter",
    "text": "Box Filter\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\nFigure¬†4: 1D Box Filter.\n\n\n\n\n\n\n\n\n\n\nFigure¬†5: Fourier transform of Box Filter.\n\n\n\n\nFig (a) A one-dimensional (1D) box filter (\\(\\left[1,1,1\\right]\\)), and (b) its Fourier transform over 20 samples. Note that the frequency gain is not monotonically decreasing with spatial frequency."
  },
  {
    "objectID": "lec5.html#box-filter-6",
    "href": "lec5.html#box-filter-6",
    "title": "Filters and image derivatives",
    "section": "Box Filter",
    "text": "Box Filter\n\n\n\nDC gain: normalization\n\n\n\nwe generally want to have a DC gain of 1\nthe reason is that if we have an image with grayscale levels in the range of 0 to 256 and with an average around 128, we will want to preserve the same mean value in the output.\nFor this reason, in most applications, we will normalize the kernel values so that they sum 1.\nIn the case of the box filter, this means dividing the kernel values by \\((2N+1)(2M+1)\\)."
  },
  {
    "objectID": "lec5.html#box-filter-7",
    "href": "lec5.html#box-filter-7",
    "title": "Filters and image derivatives",
    "section": "Box Filter",
    "text": "Box Filter\n\n\n\nLimitations\n\n\n\nThe box filter is not a perfect blurring filter.\n\n\n\n\n\n\n\n\n\n\nOscillating signal\n\n\n\n\n\n\n\nConvolved with \\([1,1,1]\\)\n\n\n\n\n\n\nIf you convolve two boxes, you get a triangle. For \\(N=1, M=0\\):\n\n\\[\n\\text{box}_{1,0} \\circ \\text{box}_{1,0} = \\left[1, 1, 1\\right] \\circ \\left[1, 1, 1\\right] = \\left[1,2,3,2,1\\right]\n\\]"
  },
  {
    "objectID": "lec5.html#sec-spt_gaussian",
    "href": "lec5.html#sec-spt_gaussian",
    "title": "Filters and image derivatives",
    "section": "Gaussian Filter",
    "text": "Gaussian Filter\n\n\n\nDefinition\n\n\nThe Gaussian distribution is defined in continuous variables. In one dimension:\n\\[\ng(x; \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp{ \\left( -\\frac{x^2}{2 \\sigma^2} \\right) }\n\\qquad(1)\\]\nand in two dimensions:\n\\[\ng(x,y; \\sigma) = \\frac{1}{2 \\pi \\sigma^2} \\exp{ \\left(-\\frac{x^2 + y^2}{2 \\sigma^2} \\right) }\n\\qquad(2)\\]\nThe parameter \\(\\sigma\\) adjusts the spatial extent of the Gaussian. The normalization constant is set so that the function integrates to 1. The Gaussian kernel is positive and symmetric (a zero-phase filter)."
  },
  {
    "objectID": "lec5.html#gaussian-filter",
    "href": "lec5.html#gaussian-filter",
    "title": "Filters and image derivatives",
    "section": "Gaussian Filter",
    "text": "Gaussian Filter\n\n\n\nDiscretization\n\n\nWe only need to consider samples within three standard deviations \\(x \\in (-3\\sigma, 3\\sigma)\\).\nFor a given \\(\\sigma\\):\n\\[\ng \\left[ n,m; \\sigma \\right] = \\exp{ \\left( -\\frac{n^2 + m^2}{2 \\sigma^2} \\right) }\n\\qquad(3)\\]\n\n\n\n\n\n1D Gaussian with \\(\\sigma=1\\) and its discretized version"
  },
  {
    "objectID": "lec5.html#gaussian-filter-1",
    "href": "lec5.html#gaussian-filter-1",
    "title": "Filters and image derivatives",
    "section": "Gaussian Filter",
    "text": "Gaussian Filter\n\n\n\n\n\n\n\n\n\n\n\n(a) Gaussian filter with \\(\\sigma=2\\).\n\n\n\n\n\n\n\n\n\n\n\n(b) Gaussian filter with \\(\\sigma=4\\).\n\n\n\n\n\n\n\n\n\n\n\n(c) Gaussian filter with \\(\\sigma=8\\).\n\n\n\n\n\n\n\nFigure¬†6"
  },
  {
    "objectID": "lec5.html#gaussian-filter-2",
    "href": "lec5.html#gaussian-filter-2",
    "title": "Filters and image derivatives",
    "section": "Gaussian Filter",
    "text": "Gaussian Filter\n\n\n\nMultiple dimensions\n\n\nRecall: \\[\n\\ell_{\\texttt{out}}\\left[n,m\\right] = h \\left[n,m\\right] \\circ \\ell_{\\texttt{in}}\\left[n,m\\right] =  \\sum_{k,l}h \\left[n-k,m-l \\right] \\ell_{\\texttt{in}}\\left[k,l \\right]\n\\qquad(4)\\] \nLetting \\(g^x [n]=g[n,0]\\), and \\(g^y[m]=g[0,m]\\)), we have:\n\\[\ng \\left[n,m \\right] \\circ \\ell \\left[n,m\\right]\n= \\sum_{k,l} g \\left[n-k,m-l \\right] \\ell \\left[k,l \\right]\n= \\sum_{k,l} \\exp{ \\left( -\\frac{(n-k)^2 + (m-l)^2}{2 \\sigma^2} \\right) } \\circ \\ell \\left[n,m \\right]\n\\]\n\\[\n= \\sum_{k} \\exp{ \\left( -\\frac{(n-k)^2}{2 \\sigma^2} \\right) }\n\\left( \\sum_{l} \\exp{ \\left( -\\frac{(m-l)^2}{2 \\sigma^2} \\right) } \\ell \\left[k,l \\right] \\right)\n= g^x \\circ (g^y \\circ \\ell \\left[n,m \\right])\n\\qquad(5)\\]"
  },
  {
    "objectID": "lec5.html#gaussian-filter-3",
    "href": "lec5.html#gaussian-filter-3",
    "title": "Filters and image derivatives",
    "section": "Gaussian Filter",
    "text": "Gaussian Filter\n\n\n\nRemoving distractions\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Original Image.\n\n\n\n\n\n\n\n\n\n\n\n(b) Blurred Image.\n\n\n\n\n\n\n\nFigure¬†7"
  },
  {
    "objectID": "lec5.html#gaussian-filter-4",
    "href": "lec5.html#gaussian-filter-4",
    "title": "Filters and image derivatives",
    "section": "Gaussian Filter",
    "text": "Gaussian Filter\n\n\n\nProperties of the Continuous Gaussian\n\n\n\nThe \\(n\\)-dimensional Gaussian is the only completely circularly symmetric operator that is separable.\nThe continuous Fourier transform (FT) of a Gaussian is also a Gaussian. For a 1D Gaussian, its Fourier transform is:\n\n\\[\nG (w; \\sigma) = \\exp{ \\left( -\\frac{w^2 \\sigma^2} {2} \\right) }\n\\]\nand in 2D the Fourier transform is:\n\\[\nG (w_x, w_y; \\sigma) = \\exp{ \\left(- \\frac{(w_x^2+w_y^2) \\sigma^2} {2} \\right) }\n\\qquad(6)\\]\nNote that this function is monotonically decreasing in magnitude for increasing frequencies, and it is also radially symmetric."
  },
  {
    "objectID": "lec5.html#gaussian-filter-5",
    "href": "lec5.html#gaussian-filter-5",
    "title": "Filters and image derivatives",
    "section": "Gaussian Filter",
    "text": "Gaussian Filter\n\n\n\nProperties of the Continuous Gaussian\n\n\n\nThe width of the Gaussian Fourier transform decreases with \\(\\sigma\\) (this is the opposite behavior to the Gaussian in the spatial domain).\nThe convolution of two \\(n\\)-dimensional Gaussians is an \\(n\\)-dimensional Gaussian.\n\n\\[\ng (x,y; \\sigma_1 ) \\circ g (x,y; \\sigma_2)  = g (x,y; \\sigma_3)\n\\]\nwhere the variance of the result is the sum \\(\\sigma_3^2 = \\sigma_1^2 + \\sigma_2^2\\).\n\nThe Gaussian is the solution to the heat equation.\nRepeated convolutions of any function concentrated in the origin result in a Gaussian (central limit theorem).\nIn the limit \\(\\sigma \\rightarrow 0\\) the Gaussian becomes an impulse. This property is shared by many other functions, but it is a useful thing to know."
  },
  {
    "objectID": "lec5.html#gaussian-filter-6",
    "href": "lec5.html#gaussian-filter-6",
    "title": "Filters and image derivatives",
    "section": "Gaussian Filter",
    "text": "Gaussian Filter\n\n\n\n\n\n\nLimitations\n\n\nThe convolution of discretized Gaussians is not a Gaussian anymore.\nLet‚Äôs consider a Gaussian with variance \\(\\sigma^2=1/2\\). It can be approximated by five samples. We will call this approximation \\(g_5\\) and it takes the values:\n\\[\ng_5\\left[ n \\right] = \\left[0.0183, \\,    0.3679, \\,    1.0000, \\,    0.3679, \\,    0.0183 \\right]\n\\]\nIf we compute the approximation for \\(\\sigma^2=1\\) by discretizing the Gaussian, the result obtained is not equal to doing \\(g_5 \\circ g_5\\): therefore, errors will accumulate."
  },
  {
    "objectID": "lec5.html#binomial-filters",
    "href": "lec5.html#binomial-filters",
    "title": "Filters and image derivatives",
    "section": "Binomial Filters",
    "text": "Binomial Filters\n\n\n\nDefinition\n\n\nBinomial coefficients: a common approximation of the Gaussian filter, obtained by successive convolutions of the box filter \\(\\left[1,1\\right]\\).\nThe binomial coefficients use the central limit theorem to approximate a Gaussian as successive convolutions of a very simple function. The binomial coefficients form the Pascal‚Äôs triangle as shown in figure.\n\\[\n\\begin{array}{*{20}{&gt;{\\centering\\arraybackslash}p{.2cm}}{&gt;{\\arraybackslash}p{1.5cm}}}\nb_0 &   ~ &  ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & 1 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~  & ~ & \\sigma_0^2=0\\\\\nb_1 &    ~ &  ~ & ~ & ~ & ~ & ~ & ~ & ~ & 1 & ~ & 1 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & \\sigma_1^2=1/4\\\\\nb_2 &   ~ &  ~ & ~ & ~ & ~ & ~ & ~ & 1 & ~ & 2 & ~ & 1 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & \\sigma_2^2=1/2\\\\\nb_3 &  ~ &  ~ & ~ & ~ & ~ & ~ & 1 & ~ & 3 & ~ & 3 & ~ & 1 & ~ & ~ & ~ & ~ & ~ & ~ & \\sigma_3^2=3/4\\\\\nb_4 &  ~ &  ~ & ~ & ~ & ~ & 1 & ~ & 4 & ~ &  6 & ~  & 4 & ~ & 1 & ~ & ~ & ~ & ~ & ~ & \\sigma_4^2=1\\\\\nb_5 &  ~ &  ~ & ~  & ~ & 1 & ~   & 5   & ~   &10  &  ~  & 10 & ~   & 5   & ~   & 1 & ~ & ~ & ~ & ~ & \\sigma_5^2=5/4\\\\\nb_6 &  ~ &  ~ & ~  & 1 & ~ & 6   & ~   & 15 & ~   & 20 & ~   & 15 & ~   & 6   & ~ & 1 & ~ & ~ & ~ & \\sigma_6^2=3/2\\\\\nb_7 &  ~ &  ~ &  1 & ~ & 7 & ~   & 21 & ~   & 35 & ~   & 35 & ~   & 21 & ~   & 7 & ~ & 1 & ~ & ~ & \\sigma_7^2=7/4\\\\\nb_8 &  ~ &  1 & ~  & 8 & ~ & 28 & ~   & 56 & ~   & 70 & ~   & 56 & ~   & 28 & ~ & 8 & ~ & 1 & ~ & \\sigma_8^2=2\n\\end{array}\n\\]\nBinomial coefficients. To build the Pascal‚Äôs triangle, each number is the sum of the number above to the left and the one above to the right."
  },
  {
    "objectID": "lec5.html#binomial-filters-1",
    "href": "lec5.html#binomial-filters-1",
    "title": "Filters and image derivatives",
    "section": "Binomial Filters",
    "text": "Binomial Filters\n\n\n\nProperties\n\n\n\nBinomial coefficients provide a compact approximation of the Gaussian coefficients using only integers. Note that the values of \\(b_2\\) are different from \\(g_5\\) despite that both will be used as approximations to a Gaussian with the same variance \\(\\sigma^2 = 1/2\\). The thing to note is that the variance of \\(g_5\\) is not really \\(\\sigma^2 = 1/2\\) despite being obtained by discretizing a Gaussian with that variance.\nThe sum of all the coefficients (DC gain) for each binomial filter \\(b_n\\) is \\(2^n\\), and their spatial variance is \\(\\sigma^2 = n/4\\).\nOne remarkable property of the binomial filters is that \\(b_n \\circ b_m = b_{n+m}\\), and, therefore, \\(\\sigma_n^2 + \\sigma_m^2  = \\sigma_{n+m}^2\\), which is analogous to the Gaussian property in the continuous domain. That is, the convolution of two binomial filters is another binomial filter."
  },
  {
    "objectID": "lec5.html#binomial-filters-2",
    "href": "lec5.html#binomial-filters-2",
    "title": "Filters and image derivatives",
    "section": "Binomial Filters",
    "text": "Binomial Filters\n\n\n\nProperties\n\n\n\nThe simplest approximation to the Gaussian filter is the 3-tap binomial kernel:\n\n\\[\nb_2 = \\left[1, 2, 1\\right]\n\\]\nIts DFT is:\n\\[\nB_2 \\left[u\\right] = 2+2 \\cos (2 \\pi u/N)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) 1D Binomial Filter.\n\n\n\n\n\n\n\n\n\n\n\n(b) Fourier Transform of Binomial Filter.\n\n\n\n\n\n\n\nFigure¬†8: Fig (a) A one-dimensional three-tap approximation to the Gaussian filter (\\(\\left[1,2,1\\right]\\)) and, (b) its Fourier transform for \\(N=20\\) samples."
  },
  {
    "objectID": "lec5.html#binomial-filters-3",
    "href": "lec5.html#binomial-filters-3",
    "title": "Filters and image derivatives",
    "section": "Binomial Filters",
    "text": "Binomial Filters\n\n\n\nProperties\n\n\n\nAll the even binomial filters can be written as successive convolutions with the kernel \\(\\left[1,2,1\\right]\\). Therefore, their Fourier transform is a power of the Fourier transform of the filter \\(\\left[1,2,1\\right]\\) and therefore they are also monotonic:\n\n\\[\nB_{2n} \\left[u\\right] = (2+2 \\cos (2 \\pi u/N))^n\n\\]\nThe filter transfer function, \\(B_{2n}\\), is real and positive. It is a zero-phase filter. \n\nFor all the binomial filters \\(b_n\\), when they are convolved with the wave \\(\\left[1,-1,1,-1,...\\right]\\), the result is the zero signal \\(\\left[0,0,0,0,...\\right]\\). This is a very nice property of binomial filters and will become very useful later when talking about downsampling an image."
  },
  {
    "objectID": "lec5.html#binomial-filters-4",
    "href": "lec5.html#binomial-filters-4",
    "title": "Filters and image derivatives",
    "section": "Binomial Filters",
    "text": "Binomial Filters\n\n\n\n2D Binomial Filters\n\n\nThe Gaussian in 2D can be approximated, as the convolution of two binomial filters:\n\\[\nb_{2,2} = b_{2,0} \\circ b_{0,2} =  \\begin{bmatrix}\n  1 & 2 & 1 \\\\\n\\end{bmatrix}\\circ \\begin{bmatrix}\n  1 \\\\\n  2 \\\\\n  1\n\\end{bmatrix}=\n\\begin{bmatrix}\n  1 & 2 & 1 \\\\\n  2 & 4 & 2\\\\\n  1 & 2 & 1\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lec5.html#binomial-filters-5",
    "href": "lec5.html#binomial-filters-5",
    "title": "Filters and image derivatives",
    "section": "Binomial Filters",
    "text": "Binomial Filters\n\n\n\nExample\n\n\n\n\n\n\n\n\n\n\nFigure¬†9: Noisy Image.\n\n\n\n\n\n\n\n\n\n\nFigure¬†10: Box Filtered Image.\n\n\n\n\n\n\n\n\n\n\nFigure¬†11: Binomial Filtered Image.\n\n\n\n\nImage corrupted by a checkerboard-pattern noise (right) and its output to two different blur kernels: (middle) \\(3 \\times 3\\) box filter. (right) Binomial filter \\(b_{2,2}\\)."
  },
  {
    "objectID": "lec5.html#concluding-remarks",
    "href": "lec5.html#concluding-remarks",
    "title": "Filters and image derivatives",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\n\n\n\n\n\n\nNote\n\n\n\nthe Gaussian and the binomial filters are widely used in computer vision.\nthe binomial filter \\([1,2,1]/4\\) (here normalized so that its DC gain is 1), and its 2D extension, are very useful kernels that you can use in many situations like when you need to remove high-frequency noise, or downsample an image by a factor of 2.\nBlur kernels are useful when building image pyramids, or in neural networks when performing different pooling operations or when resizing feature maps."
  },
  {
    "objectID": "lec5.html#introduction-1",
    "href": "lec5.html#introduction-1",
    "title": "Filters and image derivatives",
    "section": "Introduction",
    "text": "Introduction\n\n\n\nDerivatives\n\n\nComputing image derivatives is an essential operator for extracting useful information from images:\n\nboundaries computation\nfiguring out where changes are happening in the image.\n\nThe derivative operator is linear and translation invariant \\(\\Rightarrow\\) can be written as a convolution.\n\n\n\n\n\nFigure¬†12: Computing image derivatives along \\(x\\)- and \\(y\\)-dimensions."
  },
  {
    "objectID": "lec5.html#image-derivatives",
    "href": "lec5.html#image-derivatives",
    "title": "Filters and image derivatives",
    "section": "Image Derivatives",
    "text": "Image Derivatives\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\nFigure¬†13: (left) Input image, (middle) its \\(x\\)-derivative, and (right) its \\(y\\)-derivative."
  },
  {
    "objectID": "lec5.html#image-derivatives-1",
    "href": "lec5.html#image-derivatives-1",
    "title": "Filters and image derivatives",
    "section": "Image Derivatives",
    "text": "Image Derivatives\n\n\n\nDiscretizing\n\n\nIf we had access to the continuous image, then image derivatives could be computed as: \\(\\partial \\ell (x,y) / \\partial x\\), which is defined as:\n\\[\n\\frac{\\partial \\ell(x,y)} {\\partial x} = \\lim_{\\epsilon \\to 0} \\frac{ \\ell(x+\\epsilon,y) -\\ell(x,y)} {\\epsilon}\n\\]\nHowever, there are several reasons why we might not be able to apply this definition:\n\nWe only have access to a sampled version of the input image, \\(\\ell \\left[n,m\\right]\\), and we cannot compute the limit when \\(\\epsilon\\) goes to zero.\nThe image could contain many non-derivable points, and the gradient would not be defined. We will see how to address this issue later when we study Gaussian derivatives.\nIn the presence of noise, the image derivative might not be meaningful as it might just be dominated by the noise and not by the image content."
  },
  {
    "objectID": "lec5.html#image-derivatives-2",
    "href": "lec5.html#image-derivatives-2",
    "title": "Filters and image derivatives",
    "section": "Image Derivatives",
    "text": "Image Derivatives\n\n\n\nApproximation\n\n\nAs the derivative is a linear operator, it can be approximated by a discrete linear filter.\nLet‚Äôs start with \\(d_0  = \\left[1, -1 \\right]\\). In 1D, convolving a signal \\(\\ell \\left[n \\right]\\) with this filter results in: \\[\n\\ell \\circ d_0 = \\ell \\left[n \\right] - \\ell \\left[n-1 \\right]\n\\] This is due to the fact that \\(d_0 \\left[n\\right]\\) is not centered around the origin.\nThis can be addressed with a different approximation to the spatial derivative \\(d_1  = \\left[1, 0, -1 \\right]/2\\). In one dimension, convolving a signal \\(\\ell \\left[n \\right]\\) with \\(d_1 \\left[n\\right]\\) results in: \\[\n\\ell \\circ d_1 = \\frac{\\ell \\left[n+1 \\right] - \\ell \\left[n-1 \\right]}{2}\n\\]"
  },
  {
    "objectID": "lec5.html#image-derivatives-3",
    "href": "lec5.html#image-derivatives-3",
    "title": "Filters and image derivatives",
    "section": "Image Derivatives",
    "text": "Image Derivatives\n\n\nFigure¬†14: (a) Input signal, \\(\\ell [n]\\). (b) Convolutional kernel \\(d_0 [n]\\), defined as \\(d_0 [0]=1\\) and \\(d_0 [1]=-1\\) and zero everywhere else. (c) Output of the convolution between \\(\\ell [n]\\) and \\(d_0 [n]\\). (d) Kernel \\(d_1 [n]\\), defined as \\(d_1 [-1]=1\\) and \\(d_1 [1]=-1\\) and zero everywhere else. (e) Output of the convolution between \\(\\ell [n]\\) and \\(d_1 [n]\\)."
  },
  {
    "objectID": "lec5.html#image-derivatives-4",
    "href": "lec5.html#image-derivatives-4",
    "title": "Filters and image derivatives",
    "section": "Image Derivatives",
    "text": "Image Derivatives\n\n\n\nApproximation: continuous Fourier domain\n\n\nIn the continuous domain, the relationship between the Fourier transform on a function and the Fourier transform of its derivative is: \\[\n\\frac{\\partial \\ell (x)}{\\partial x}  \n\\xrightarrow{\\mathscr{F}}\nj w \\mathscr{L} (w)\n\\] In the continuous Fourier domain, derivation is equivalent to multiplying by \\(jw\\)."
  },
  {
    "objectID": "lec5.html#image-derivatives-5",
    "href": "lec5.html#image-derivatives-5",
    "title": "Filters and image derivatives",
    "section": "Image Derivatives",
    "text": "Image Derivatives\n\n\n\nApproximation: discrete Fourier domain\n\n\nThe DFT of \\(d_0 \\left[n\\right]\\) is: \\[\n\\begin{split}\nD_0 \\left[u \\right] & = 1 - \\exp \\left( -2 \\pi j \\frac{u}{N} \\right)  \\\\\n& = \\exp \\left( - \\pi j \\frac{u}{N} \\right) \\left(  \\exp \\left( \\pi j \\frac{u}{N} \\right) - \\exp \\left( -\\pi j \\frac{u}{N} \\right)  \\right) \\\\\n& = \\exp \\left( - \\pi j \\frac{u}{N} \\right) 2 j \\sin (\\pi u /N)\n\\end{split}\n\\] The first term is a pure phase shift, the second term is the amplitude gain.\nThe DFT of \\(d_1 \\left[n\\right]\\) is: \\[\n\\begin{split}\nD_1 \\left[u \\right] & =  1/2\\exp \\left( 2 \\pi j \\frac{u}{N} \\right) - 1/2 \\exp \\left( -2 \\pi j \\frac{u}{N} \\right)  \\\\\n& =  j \\sin (2 \\pi u /N)\n\\end{split}\n\\]"
  },
  {
    "objectID": "lec5.html#image-derivatives-6",
    "href": "lec5.html#image-derivatives-6",
    "title": "Filters and image derivatives",
    "section": "Image Derivatives",
    "text": "Image Derivatives\n\n\n\nFigure¬†15: Magnitude of (a) \\(D_0\\left[u \\right]\\) and (b) \\(D_1\\left[u \\right]\\) and comparison with \\(\\left| 2 \\pi u/N \\right|\\), shown as a thin black line. Both DFTs are computed over 20 samples."
  },
  {
    "objectID": "lec5.html#image-derivatives-7",
    "href": "lec5.html#image-derivatives-7",
    "title": "Filters and image derivatives",
    "section": "Image Derivatives",
    "text": "Image Derivatives\n\n\n\nApproximation in 2D\n\n\n\nWe can compute derivatives along the \\(n\\) and \\(m\\) components:\n\n\\[\\begin{bmatrix}\n  1 \\\\\n  -1\n\\end{bmatrix}\\]\n\\[\\begin{bmatrix}\n  1 & -1\n\\end{bmatrix}\\]\nProblem: outputs are spatially misaligned because a filter of length 2 shifts the output image by half a pixel as we discussed earlier.\n\nWe can use a rotated reference frame as it is done in the Roberts cross operator (1963):\n\n\\[\\begin{bmatrix}\n  1 & ~0\\\\\n  0 & -1\n\\end{bmatrix}\\]\n\\[\\begin{bmatrix}\n  ~0 & 1 \\\\\n  -1 & 0\n\\end{bmatrix}\\]\nNow, both outputs are spatially aligned because they are shifted in the same way."
  },
  {
    "objectID": "lec5.html#gradient-based-image-representation",
    "href": "lec5.html#gradient-based-image-representation",
    "title": "Filters and image derivatives",
    "section": "Gradient-Based Image Representation",
    "text": "Gradient-Based Image Representation\n\n\n\nRecovering original image\n\n\nPerhaps by integrating?\nThis is the matrix that corresponds to the convolution with the kernel \\(\\left[1, -1 \\right]\\) that we will call \\(\\mathbf{D_0}\\). The next two matrices show the matrix \\(\\mathbf{D_0}\\) and its inverse \\(\\mathbf{D_0}^{-1}\\) for a 1D image of length five pixels using zero boundary conditions:\n\\[\n\\mathbf{D_0} =\n\\begin{bmatrix}\n  1 ~& 0 ~& 0 ~& 0~& 0 \\\\\n  -1 ~& 1 ~& 0 ~& 0~& 0 \\\\\n  0 ~& -1 ~& 1 ~& 0 ~& 0\\\\\n  0~& 0 ~& -1 ~& 1 ~& 0\\\\\n  0~& 0 ~& 0 ~& -1 ~& 1\n\\end{bmatrix}\n\\]\n\\[\n\\mathbf{D_0}^{-1} =\n\\begin{bmatrix}\n  1 ~&~ 0 ~&~ 0 ~&~ 0~&~ 0 \\\\\n  1 ~&~ 1 ~&~ 0 ~&~ 0~&~ 0 \\\\\n  1 ~&~ 1 ~&~ 1 ~&~ 0 ~&~ 0\\\\\n  1~&~ 1 ~&~ 1 ~&~ 1 ~&~ 0\\\\\n  1~&~ 1 ~&~ 1 ~&~ 1 ~&~ 1\n\\end{bmatrix}\n\\] First sample of the derivative gets to see the actual value of the input signal, and then we can integrate back the entire signal."
  },
  {
    "objectID": "lec5.html#gradient-based-image-representation-1",
    "href": "lec5.html#gradient-based-image-representation-1",
    "title": "Filters and image derivatives",
    "section": "Gradient-Based Image Representation",
    "text": "Gradient-Based Image Representation\n\n\n\nRecovering original image\n\n\nBut what happens if you only get to see valid differences and you remove any pixel that was affected by the boundary? In this case, the derivative operator in matrix form is:\n\\[\n\\mathbf{D_0} =\n\\begin{bmatrix}\n  -1 ~& 1 ~& 0 ~& 0~& 0 \\\\\n  0 ~& -1 ~& 1 ~& 0 ~& 0\\\\\n  0~& 0 ~& -1 ~& 1 ~& 0\\\\\n  0~& 0 ~& 0 ~& -1 ~& 1\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "lec5.html#gradient-based-image-representation-2",
    "href": "lec5.html#gradient-based-image-representation-2",
    "title": "Filters and image derivatives",
    "section": "Gradient-Based Image Representation",
    "text": "Gradient-Based Image Representation\n\n\n\nRecovering original image\n\n\nLet‚Äôs consider the next 1D input signal:\n\\[\n\\boldsymbol\\ell = \\left[1, 1, 2, 2, 0\\right]\n\\]\nThen, the output of the derivative operator is:\n\\[\n\\mathbf{r}=\\mathbf{D_0} \\boldsymbol\\ell=\\left[0, -1, 0, 2\\right]\n\\]\n\\[\n\\mathbf{D_0}^{+} = \\frac{1}{5}\n\\begin{bmatrix}\n  -4 ~& -3 ~& -2~& -1 \\\\\n  1 ~& -3 ~& -2 ~&-1 \\\\\n  1~& 2 ~& -2 ~& -1\\\\\n  1~& 2 ~& 3 ~& -1\\\\\n  1~& 2 ~& 3 ~& 4\n\\end{bmatrix}\n\\]\nIn this example, the reconstructed input is:\n\\[\n\\hat{\\boldsymbol\\ell} = \\mathbf{D_0}^{+} \\mathbf{r} =\n\\left[-0.2, -0.2, 0.8, 0.8, -1.2 \\right]\n\\] Note that \\(\\hat{\\boldsymbol\\ell}\\) is a zero mean vector. In fact, the recovered input is a shifted version of the original input, \\(\\hat{\\boldsymbol\\ell} = \\boldsymbol\\ell - 1.2\\), where 1.2 is the mean value of samples on \\(\\boldsymbol\\ell\\)."
  },
  {
    "objectID": "lec5.html#sec-editinggradientdomain",
    "href": "lec5.html#sec-editinggradientdomain",
    "title": "Filters and image derivatives",
    "section": "Image Editing in the Gradient Domain",
    "text": "Image Editing in the Gradient Domain\n\n\nFigure¬†16: Image inpainting: Using image derivatives, we delete the word ‚Äústop‚Äù by setting to zero the gradients indicated by the mask. The resulting decoded image propagates the red color inside the region that contained the word."
  },
  {
    "objectID": "lec5.html#image-editing-in-the-gradient-domain",
    "href": "lec5.html#image-editing-in-the-gradient-domain",
    "title": "Filters and image derivatives",
    "section": "Image Editing in the Gradient Domain",
    "text": "Image Editing in the Gradient Domain\n\n\n\nEncoding/Decoding\n\n\n\nFirst, the image is encoded using derivatives along the \\(x\\) and \\(y\\) dimensions: \\[\n\\mathbf{r} =\n\\left[\n\\begin{array}{c}\n\\mathbf{D_x}  \\\\\n\\mathbf{D_y}\n\\end{array}\n\\right]\n\\boldsymbol\\ell\n\\]\nThe resulting representation, \\(\\mathbf{r}\\), contains the concatenation of the output of both derivative operators. \\(\\mathbf{r}\\) will have high values in the image regions that contain changes in pixel intensities and colors and will be near zero in regions with small variations.\nThis representation can be decoded back into the input image by using the pseudoinverse as we did in the 1D case. The pseudoinverse can be efficiently computed in the Fourier domain."
  },
  {
    "objectID": "lec5.html#gaussian-derivatives",
    "href": "lec5.html#gaussian-derivatives",
    "title": "Filters and image derivatives",
    "section": "Gaussian Derivatives",
    "text": "Gaussian Derivatives\n\n\n\nNoise sensitivity\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\nFigure¬†17: Derivatives of a noisy image. (a) Input image with noise, (b) its \\(x\\)-derivative obtained by convolving with a kernel \\([1, -1]\\), and (c) its \\(x\\)-derivative obtained using a Gaussian derivative kernel."
  },
  {
    "objectID": "lec5.html#gaussian-derivatives-1",
    "href": "lec5.html#gaussian-derivatives-1",
    "title": "Filters and image derivatives",
    "section": "Gaussian Derivatives",
    "text": "Gaussian Derivatives\n\n\n\nUndefined derivative\n\n\nConsider an image in the continuous domain with the form \\(\\ell (x,y) = 0\\) if \\(x&lt;0\\) and 1 otherwise.\n\nIf we try to compute \\(\\partial \\ell (x,y) / \\partial x\\), we will get 0 everywhere, but around \\(x=0\\) the value of the derivative is not defined.\nWe avoided this issue in the previous section because for discrete images the approximation of the derivative is always defined.\n\n\n\n\n\n\n\nSolution\n\n\nGaussian derivatives address these two issues. They were popularized by Koenderink and Van Doorm @Koenderink87 as a model of neurons in the visual system."
  },
  {
    "objectID": "lec5.html#gaussian-derivatives-2",
    "href": "lec5.html#gaussian-derivatives-2",
    "title": "Filters and image derivatives",
    "section": "Gaussian Derivatives",
    "text": "Gaussian Derivatives\n\n\n\nObservation: commutativity\n\n\nLet‚Äôs start with the following observation. For two functions defined in the continuous domain \\(\\ell(x,y)\\) and \\(g(x,y)\\), the convolution and the derivative are commutative: \\[\n\\frac {\\partial \\ell(x,y)}{\\partial x} \\circ g(x,y) = \\ell(x,y) \\circ \\frac {\\partial g(x,y)}{\\partial x}\n\\]\nInstead of computing the derivative of the image we can compute the derivatives of the filter kernel and convolve it with the image."
  },
  {
    "objectID": "lec5.html#gaussian-derivatives-3",
    "href": "lec5.html#gaussian-derivatives-3",
    "title": "Filters and image derivatives",
    "section": "Gaussian Derivatives",
    "text": "Gaussian Derivatives\n\n\n\nSmoothing\n\n\nIf \\(g(x,y)\\) is a blurring kernel, it will smooth the derivatives, reducing the output noise at the expense of a loss in spatial resolution. A common smoothing kernel for computing derivatives is the Gaussian kernel.\nIf \\(g\\) is a Gaussian, then the first-order derivative is \\[\n\\begin{split}\ng_x(x,y; \\sigma) & = \\frac {\\partial g(x,y; \\sigma)}{\\partial x} \\\\\n& = \\frac{-x}{2 \\pi \\sigma^4} \\exp{-\\frac{x^2 +\n   y^2}{2 \\sigma^2}} \\\\\n& = \\frac{-x}{\\sigma^2} g(x,y; \\sigma)\n\\end{split}\n\\qquad(7)\\]"
  },
  {
    "objectID": "lec5.html#gaussian-derivatives-4",
    "href": "lec5.html#gaussian-derivatives-4",
    "title": "Filters and image derivatives",
    "section": "Gaussian Derivatives",
    "text": "Gaussian Derivatives\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure¬†18: Fig (a) 2D Gaussian with \\(\\sigma=1\\), and (b) its \\(x\\)-derivative."
  },
  {
    "objectID": "lec5.html#gaussian-derivatives-5",
    "href": "lec5.html#gaussian-derivatives-5",
    "title": "Filters and image derivatives",
    "section": "Gaussian Derivatives",
    "text": "Gaussian Derivatives\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\nFigure¬†19: An image filtered with three Gaussian derivatives: Fig (a) \\(\\sigma=2\\); (b) \\(\\sigma=4\\); and (c) \\(\\sigma=8\\). Plots (d-f) show the three Gaussian derivatives over the same spatial support as the image. The discrete functions are approximated by sampling the continuous Gaussian derivatives. The convolutions are performed with mirror boundary extension."
  },
  {
    "objectID": "lec5.html#high-order-gaussian-derivatives",
    "href": "lec5.html#high-order-gaussian-derivatives",
    "title": "Filters and image derivatives",
    "section": "High-Order Gaussian Derivatives",
    "text": "High-Order Gaussian Derivatives\n\n\n\nComputation\n\n\nThe second-order derivative of a Gaussian is: \\[\ng_{x^2}(x,y; \\sigma) = \\frac{x^2-\\sigma^2}{\\sigma^4} g(x,y; \\sigma)\n\\]\nThe general expression for the \\(n\\) derivative of a Gaussian is: \\[\ng_{x^n}(x; \\sigma) =  \\frac{\\partial^{n} g(x)}{\\partial x^n} =\n\\left( \\frac{-1}{\\sigma \\sqrt{2}} \\right)^n\nH_n\\left( \\frac{x}{\\sigma \\sqrt {2}} \\right)\ng(x; \\sigma)\n\\]\nThe first Hermite polynomial is \\(H_0(x)=1\\), the second is \\(H_1(x) = 2x\\), the third is \\(H_2(x)=4x^2-2\\), and they can be computed recursively as: \\[\nH_n(x) = 2x H_{n-1}(x) - 2(n-1)H_{n-2}(x)\n\\]"
  },
  {
    "objectID": "lec5.html#high-order-gaussian-derivatives-1",
    "href": "lec5.html#high-order-gaussian-derivatives-1",
    "title": "Filters and image derivatives",
    "section": "High-Order Gaussian Derivatives",
    "text": "High-Order Gaussian Derivatives\n\n\nFigure¬†20: Fig (a) 1D Gaussian with \\(\\sigma=1\\). (b‚Äìd) Gaussian derivatives up to order 3."
  },
  {
    "objectID": "lec5.html#high-order-gaussian-derivatives-2",
    "href": "lec5.html#high-order-gaussian-derivatives-2",
    "title": "Filters and image derivatives",
    "section": "High-Order Gaussian Derivatives",
    "text": "High-Order Gaussian Derivatives\n\n\n\nPartial derivatives\n\n\nIn two dimensions, as the Gaussian is separable, the partial derivatives result on the product of two Hermite polynomial, one for each spatial dimension:\n\\[\n\\begin{split}\ng_{x^n,y^m}(x,y; \\sigma) & =  \n\\frac{\\partial^{n+m} g(x,y)}{\\partial x^n \\partial y^m} \\\\\n& = \\left( \\frac{-1}{\\sigma \\sqrt{2}} \\right)^{n+m}\nH_n\\left( \\frac{x}{\\sigma \\sqrt {2}} \\right)\nH_m\\left( \\frac{y}{\\sigma \\sqrt {2}} \\right)\ng(x,y; \\sigma)\n\\end{split}\n\\qquad(8)\\]"
  },
  {
    "objectID": "lec5.html#high-order-gaussian-derivatives-3",
    "href": "lec5.html#high-order-gaussian-derivatives-3",
    "title": "Filters and image derivatives",
    "section": "High-Order Gaussian Derivatives",
    "text": "High-Order Gaussian Derivatives\n\n\nFigure¬†21: Gaussian derivatives up to order 6. All the kernels are separable. They seem similar to Fourier basis multiplied with a Gaussian window.\nFigure¬†22 shows that they are different from sine and cosine waves; instead, they look more like products of cosine and sine waves."
  },
  {
    "objectID": "lec5.html#high-order-gaussian-derivatives-4",
    "href": "lec5.html#high-order-gaussian-derivatives-4",
    "title": "Filters and image derivatives",
    "section": "High-Order Gaussian Derivatives",
    "text": "High-Order Gaussian Derivatives\n\n\nFigure¬†22: Fourier transform of the Gaussian derivatives shown in Figure¬†21."
  },
  {
    "objectID": "lec5.html#high-order-gaussian-derivatives-5",
    "href": "lec5.html#high-order-gaussian-derivatives-5",
    "title": "Filters and image derivatives",
    "section": "High-Order Gaussian Derivatives",
    "text": "High-Order Gaussian Derivatives\n\nAn image containing a square and a circle and its output to the Gaussian derivatives up to order 3."
  },
  {
    "objectID": "lec5.html#sec-derivatives_binomial_filters",
    "href": "lec5.html#sec-derivatives_binomial_filters",
    "title": "Filters and image derivatives",
    "section": "Derivatives using Binomial Filters",
    "text": "Derivatives using Binomial Filters\n\n\n\nDiscrete approximations\n\n\nWhen processing images we have to use discrete approximations for the Gaussian derivatives. After discretization, many of the properties of the continuous Gaussian will not hold exactly.\nThere are many discrete approximations. For instance, we can take samples of the continuous functions. In practice it is common to use the discrete approximation given by the binomial filters. Below is the result of convolving the binomial coefficients, \\(b_n\\), with \\(\\left[1, -1\\right]\\).\n\\[\\begin{array}{ccccccccccccccccccccl}\nd_0 &    ~ &  ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~~1 & ~~~ & -1 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ &\\\\\nd_1 &    ~ &  ~ & ~ & ~ & ~ & ~ & ~ & ~~1 & ~ & ~~0 & ~ & -1 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & \\\\\nd_2 &    ~ &  ~ & ~ & ~ & ~ & ~ & ~~~1 & ~ & ~~~1 & ~~~ & -1 & ~ & -1 & ~ & ~ & ~ & ~ & ~ & ~ & \\\\\nd_3 &    ~ &  ~ & ~ & ~ & ~ & ~~~1 & ~ & ~~~2 & ~ &  ~~0 & ~~~  & -2 & ~ & -1 & ~ & ~ & ~ & ~ & ~ &\\\\\nd_4 &    ~ &  ~ & ~  & ~ & ~~~1 & ~   & ~~~3   & ~   & ~~~2  &  ~~~  & -2 & ~   & -3   & ~   & -1 & ~ & ~ & ~ & ~ &\\\\\nd_5 &    ~ &  ~ & ~  & ~~~1 & ~ & ~~~4   & ~   & ~~~5 & ~   & ~~0 & ~   & -5 & ~   & -4   & ~ & -1 & ~ & ~ & ~ &\n\\end{array}\\]\nDerivative of binomial coefficients resulting from the convolution \\(b_n \\circ \\left[1, -1\\right]\\). The filters, \\(d_0\\) and \\(d_1\\), are the ones we have studied in the previous section."
  },
  {
    "objectID": "lec5.html#derivatives-using-binomial-filters",
    "href": "lec5.html#derivatives-using-binomial-filters",
    "title": "Filters and image derivatives",
    "section": "Derivatives using Binomial Filters",
    "text": "Derivatives using Binomial Filters\n\n\n\nSobel operator\n\n\nIn two dimensions, we can use separable filters and build a partial derivative as:\n\\[\nSobel_x =  \\begin{bmatrix}\n  1 & 0 & -1 \\\\\n\\end{bmatrix} \\circ \\begin{bmatrix}\n  1 \\\\\n  2 \\\\\n  1\n\\end{bmatrix} =\n\\begin{bmatrix}\n  1 & 0 & -1 \\\\\n  2 & 0 & -2 \\\\\n  1 & 0 & -1\n\\end{bmatrix}\n\\]\n\\[\nSobel_y =  \\begin{bmatrix}\n  -1 & -2 & -1 \\\\\n  0 & 0 & 0 \\\\\n  1 & 2 & 1\n\\end{bmatrix}\n\\qquad(9)\\]\nThis particular filter is called the Sobel-Feldman operator.\nThe goal of this operator was to be compact and as isotropic as possible. The Sobel-Feldman operator can be implemented very efficiently as it can be written as the convolution with four small kernels, \\[Sobel_x = b_1 \\circ d_0 \\circ b_1^T \\circ b_1^T.\\]\n\n\n\n\n\nRemember that (b_1=[1, 1]), and (b_2=b_1 b_1 = [1,2,1])."
  },
  {
    "objectID": "lec5.html#derivatives-using-binomial-filters-1",
    "href": "lec5.html#derivatives-using-binomial-filters-1",
    "title": "Filters and image derivatives",
    "section": "Derivatives using Binomial Filters",
    "text": "Derivatives using Binomial Filters\n\n\n\nDFT of the Sobel-Feldman operator\n\n\nThe DFT of the Sobel-Feldman operator is:\n\\[\nSobel_x \\left[u,v \\right] = D_1\\left[u\\right] B_2 \\left[v \\right] = j \\sin \\left( 2 \\pi u /N \\right) \\left( 2+2 \\cos \\left(2 \\pi v/N \\right) \\right)\n\\]\n\n\n\n\n\nFigure¬†23: Magnitude of the DFT of four different discretizations of Gaussian derivatives: (a) (d_0); (b) (d_1); (c) Robert cross operator; and (d) Sobel-Feldman operator."
  },
  {
    "objectID": "lec5.html#derivatives-using-binomial-filters-2",
    "href": "lec5.html#derivatives-using-binomial-filters-2",
    "title": "Filters and image derivatives",
    "section": "Derivatives using Binomial Filters",
    "text": "Derivatives using Binomial Filters\n\n\nFigure¬†24: Derivatives of a circle along the directions \\(n\\), \\(m\\), and 45 degrees. The angle is shown only where the magnitude is \\(&gt;0\\). The derivative output along 45 degrees is obtained as a linear combination of the derivatives outputs along \\(n\\) and \\(m\\). Check the differences among the different kernels. The Sobel operator gives the most rotationally invariant gradient magnitude, but it is blurrier."
  },
  {
    "objectID": "lec5.html#directional-derivatives",
    "href": "lec5.html#directional-derivatives",
    "title": "Filters and image derivatives",
    "section": "Directional Derivatives",
    "text": "Directional Derivatives\n\n\n\nImage gradient\n\n\nFrom the image derivatives, we can also define the image gradient as the vector: \\[\n\\nabla \\ell (x,y) = \\left( \\frac{\\partial \\ell(x,y)}{\\partial x}, \\frac{\\partial \\ell(x,y)}{\\partial y} \\right)\n\\] For each pixel, the output is a 2D vector. In the case of using Gaussian derivatives, we can write: \\[\n\\nabla \\ell \\circ g = \\nabla g \\circ \\ell = \\left( g_x(x,y), g_y(x,y) \\right) \\circ \\ell\n\\]\nAlthough we have mostly computed derivatives along the \\(x\\) and \\(y\\) variables, we can obtain the derivative in any orientation as a linear combination of the two derivatives along the main axes. With \\({\\bf t}=\\left( \\cos (\\theta), \\sin(\\theta) \\right)\\), we can write the directional derivative along the vector \\({\\bf t}\\) as: \\[\n\\frac{\\partial \\ell (x,y)}{\\partial {\\bf t}} =  \\nabla \\ell \\cdot {\\bf t} = \\cos(\\theta) \\frac{\\partial \\ell}{\\partial x} + \\sin(\\theta) \\frac{\\partial \\ell}{\\partial y}\n\\]"
  },
  {
    "objectID": "lec5.html#directional-derivatives-1",
    "href": "lec5.html#directional-derivatives-1",
    "title": "Filters and image derivatives",
    "section": "Directional Derivatives",
    "text": "Directional Derivatives\n\n\n\nImage gradient\n\n\nIn the Gaussian case: \\[\n\\begin{split}\n\\frac{\\partial \\ell}{\\partial {\\bf t}} \\circ g & = \\left( \\cos(\\theta) g_x(x,y) + \\sin(\\theta) g_y(x,y) \\right) \\circ \\ell \\\\\n& = \\left( \\nabla g  \\cdot {\\bf t} \\right) \\circ \\ell \\\\\n& = g_{\\theta} (x,y)  \\circ \\ell(x,y)\n\\end{split}\n\\qquad(10)\\]\nwith \\(g_{\\theta} (x,y) = \\cos(\\theta) g_x(x,y) + \\sin(\\theta) g_y(x,y)\\).\nHowever, to compute the derivative along any arbitrary angle \\(\\theta\\) does not require doing new convolutions. Instead, we can compute any derivative as a linear combination of the output of convolving the image with \\(g_x(x,y)\\) and \\(g_y(x,y)\\): \\[\n\\frac{\\partial \\ell}{\\partial {\\bf t}} \\circ g =  \\cos(\\theta) g_x(x,y) \\circ \\ell + \\sin(\\theta) g_y(x,y) \\circ \\ell(x,y)\n\\]"
  },
  {
    "objectID": "lec5.html#directional-derivatives-2",
    "href": "lec5.html#directional-derivatives-2",
    "title": "Filters and image derivatives",
    "section": "Directional Derivatives",
    "text": "Directional Derivatives\n\n\n\nImage gradient\n\n\nWhen using discrete convolutional kernels \\(d_n\\left[n,m\\right]\\) and \\(d_m\\left[n,m\\right]\\) to approximate the derivatives along \\(n\\) and \\(m\\), it can be written as: \\[\n\\nabla \\ell = \\left( d_n\\left[n,m\\right], d_m\\left[n,m\\right] \\right) \\circ \\ell \\left[n,m\\right]\n\\] and \\[\n\\frac{\\partial \\ell}{\\partial {\\bf t}} \\circ g = d_{\\theta} \\left[n,m\\right] \\circ \\ell \\left[n,m\\right]\n\\]\nwith \\(d_{\\theta} \\left[n,m\\right]  = \\cos(\\theta) d_n\\left[n,m\\right] + \\sin(\\theta) d_m\\left[n,m\\right]\\). We expect that the linear combination of these two kernels should approximate the derivative along the direction \\(\\theta\\)."
  },
  {
    "objectID": "lec5.html#image-laplacian",
    "href": "lec5.html#image-laplacian",
    "title": "Filters and image derivatives",
    "section": "Image Laplacian",
    "text": "Image Laplacian\n\n\n\nDefinition\n\n\nThe Laplacian filter was made popular by Marr and Hildreth in 1980 in the search for operators that locate the boundaries between objects.\n\\[\n\\nabla^2 \\ell = \\frac{\\partial^2 \\ell}{\\partial x^2} + \\frac{\\partial^2 \\ell}{\\partial y^2}\n\\]\n\n\n\n\n\n\n\n\n\nImportant\n\n\nThe Laplacian is more sensitive to noise than the first order derivative.\n\n\n\n\n\nOne example of application of the Laplacian is in the paper Can One Hear the Shape of a Drum, where the Laplacian is used for modeling vibrations in a drum and the sounds it produces as a function of its shape."
  },
  {
    "objectID": "lec5.html#image-laplacian-1",
    "href": "lec5.html#image-laplacian-1",
    "title": "Filters and image derivatives",
    "section": "Image Laplacian",
    "text": "Image Laplacian\n\n\n\nSmoothing\n\n\nIt is useful to smooth the output with a Gaussian kernel, \\(g(x,y)\\). We can write,\n\\[\n\\nabla^2 \\ell \\circ g = \\nabla^2 g \\circ \\ell\n\\]\nTherefore, the same result can be obtained if we first compute the Laplacian of the Gaussian kernel, \\(g(x,y)\\) and then convolve it with the input image. The Laplacian of the Gaussian is\n\\[\n\\nabla^2 g = \\frac{x^2 + y^2 -2\\sigma^2}{\\sigma^4} g(x,y)\n\\]"
  },
  {
    "objectID": "lec5.html#image-laplacian-2",
    "href": "lec5.html#image-laplacian-2",
    "title": "Filters and image derivatives",
    "section": "Image Laplacian",
    "text": "Image Laplacian\n\n\nFigure¬†25: The Gaussian Laplacian (\\(\\sigma = 1\\)) is also called the inverted mexican hat wavelet. (a) 2D plot. (b) 1D section at \\(y=0\\)."
  },
  {
    "objectID": "lec5.html#image-laplacian-3",
    "href": "lec5.html#image-laplacian-3",
    "title": "Filters and image derivatives",
    "section": "Image Laplacian",
    "text": "Image Laplacian\n\n\n\nDiscrete approximation\n\n\nIn one dimension, the Laplacian can be approximated by \\([1,-2,1]\\), which is the result of the convolution of two 2-tap discrete approximations of the derivative \\([1,-1] \\circ [1,-1]\\). In two dimensions, the most popular approximation is the five-point formula, which consists of convolving the image with the kernel:\n\\[\n\\nabla_5^2 =\n\\begin{bmatrix}\n  0 & 1 & 0 \\\\\n  1 & -4 & 1\\\\\n  0 & 1 & 0\n\\end{bmatrix}\n\\qquad(11)\\]\n\nThe Laplacian of the image, using the five-point formula, is: \\[\n\\begin{split}\n\\nabla_5^2 \\ell[n,m] = & - 4 \\ell[n,m] \\\\\n                     &  + \\ell[n+1,m] \\\\\n                     &  + \\ell[n-1,m] \\\\\n                     &  + \\ell[n,m+1] \\\\\n                     &  + \\ell[n,m-1]\n\\end{split}\n\\]"
  },
  {
    "objectID": "lec5.html#image-laplacian-4",
    "href": "lec5.html#image-laplacian-4",
    "title": "Filters and image derivatives",
    "section": "Image Laplacian",
    "text": "Image Laplacian\n\n\nFigure¬†26: Magnitude of the DFT of the Gaussian Laplacian with (a) \\(\\sigma=1/2\\); (b) \\(\\sigma=1\\); (c) \\(\\sigma=2\\); and (d) DFT of the five-point discrete approximation, Equation¬†11."
  },
  {
    "objectID": "lec5.html#image-laplacian-5",
    "href": "lec5.html#image-laplacian-5",
    "title": "Filters and image derivatives",
    "section": "Image Laplacian",
    "text": "Image Laplacian\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\nFigure¬†27: Fig (a) Input image. (b) Second order derivative along \\(x\\). (c) Second-order derivative along \\(y\\). (d) The sum of (b) + (c), which gives the Laplacian."
  },
  {
    "objectID": "lec5.html#image-laplacian-6",
    "href": "lec5.html#image-laplacian-6",
    "title": "Filters and image derivatives",
    "section": "Image Laplacian",
    "text": "Image Laplacian\n\n\n\nAdvantages\n\n\n\nIt is rotationally invariant. It is a linear operator that responds equally to edges in any orientation (this is only approximate in the discrete case).\nIt measures curvature. If the image contains a linear trend the derivative will be non-zero despite having no boundaries, while the Laplacian will be zero.\nEdges can be located as the zero-crossings in the Laplacian output. However, this way of detecting edges is not very reliable.\nZero crossings of an image form closed contours."
  },
  {
    "objectID": "lec5.html#image-laplacian-7",
    "href": "lec5.html#image-laplacian-7",
    "title": "Filters and image derivatives",
    "section": "Image Laplacian",
    "text": "Image Laplacian\n\n\nFigure¬†28: Comparison between the output of a first-order derivative and the Laplacian of 1D signal. (a) Input signal. (b) Kernel \\(d_1\\). (c) Output of the derivative, that is, convolution of (a) and (b). (d) Discrete approximation of the Laplacian. (e) Output of convolving the signal (a) with the Laplacian kernel."
  },
  {
    "objectID": "lec5.html#early-visual-system-model",
    "href": "lec5.html#early-visual-system-model",
    "title": "Filters and image derivatives",
    "section": "Early Visual System Model",
    "text": "Early Visual System Model\n\n\n\nApproximation of the visual system\n\n\nWe can better approximate Campbell and Robson chart with\n\\[\nh = -\\nabla^2 g  + \\lambda g\n\\qquad(12)\\]\nThe kernel \\(h\\) is the approximate impulse response of the human visual system, \\(\\lambda\\) is a small constant that is equal to the DC gain of the visual filter (here \\(\\lambda = 2\\) and \\(\\sigma=5\\)).\n\n\n\n\n\n\nFigure¬†29: Kernel corresponding to Equation¬†12 with \\(\\lambda = 2\\) and \\(\\sigma=5\\)."
  },
  {
    "objectID": "lec5.html#early-visual-system-model-1",
    "href": "lec5.html#early-visual-system-model-1",
    "title": "Filters and image derivatives",
    "section": "Early Visual System Model",
    "text": "Early Visual System Model\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\nFigure¬†30: Campbell and Robson chart, and a radial section of the Fourier transform of \\(h\\) from Equation¬†12."
  },
  {
    "objectID": "lec5.html#early-visual-system-model-2",
    "href": "lec5.html#early-visual-system-model-2",
    "title": "Filters and image derivatives",
    "section": "Early Visual System Model",
    "text": "Early Visual System Model\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\nFigure¬†31: Vasarely visual illusion. Images (a) and (d), formed by nested-squares, appear as having bright diagonals in (a) and dark in (d). Images (b) and (e) show the output of the human model given by the filter from Equation¬†12. Plot (c) displays the intensity profiles of images (a) and (b) as horizontal sections, with image (a) represented in red and image (b) in blue. Similarly, Plot (f) represents the intensity profiles of images (d) and (e)."
  },
  {
    "objectID": "lec5.html#sharpening-filter",
    "href": "lec5.html#sharpening-filter",
    "title": "Filters and image derivatives",
    "section": "Sharpening Filter",
    "text": "Sharpening Filter\n\n\n\nDefinition\n\n\nSharpening can be achieved by amplifying the amplitude of the high-spatial frequency content of the image. We start with twice the original image (sharp plus blurred parts), then subtract away the blurred components of the image:\n\\[\n\\text{sharpening filter} =\n\\begin{bmatrix}\n  0 & 0 & 0 \\\\\n  0 & 2 & 0\\\\\n  0 & 0 & 0\n\\end{bmatrix}\n-\n\\frac{1}{16}\n\\begin{bmatrix}\n  1 & 2 & 1 \\\\\n  2 & 4 & 2\\\\\n  1 & 2 & 1\n\\end{bmatrix}\n\\qquad(13)\\]\nNote that the DC gain of this sharpening filter is 1."
  },
  {
    "objectID": "lec5.html#sharpening-filter-1",
    "href": "lec5.html#sharpening-filter-1",
    "title": "Filters and image derivatives",
    "section": "Sharpening Filter",
    "text": "Sharpening Filter\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\n\n\n\n\n\n(f)\n\n\n\n\n\n\n\nFigure¬†32: Sharpening achieved by subtraction of blurred components. (a) Original image. (b) Sharpened once by filtering with kernel from Equation¬†13. Each color channel is filtered independently. (c-f) The same filter is applied successively to the previous output. In the last image, the sharpening filter has been applied five times to the input image."
  },
  {
    "objectID": "lec5.html#retinex",
    "href": "lec5.html#retinex",
    "title": "Filters and image derivatives",
    "section": "Retinex",
    "text": "Retinex\n\n\n\nRetinex\n\n\nHow do you tell gray from white? The amount of light that reaches the eye from a painted piece of paper is the result of two quantities:\n\nthe amount of light reaching the piece of paper\nthe reflectance of the surface\n\n\n\n\n\n\nFigure¬†33: Simultaneous contrast illusion. What happens if we see two patches of unknown reflectance, and each is illuminated with two different light sources of unknown identity?"
  },
  {
    "objectID": "lec5.html#retinex-2",
    "href": "lec5.html#retinex-2",
    "title": "Filters and image derivatives",
    "section": "Retinex",
    "text": "Retinex\n\n\n\nImage formation\n\n\nLet‚Äôs think of the image formation process. The surface is made of patches of different reflectances \\(r(x,y) \\in (0,1)\\). Each location receives an illumination \\(l(x,y)\\). The observed brightness is the product:\n\\[\n\\ell(x,y) = r(x,y) \\times l(x,y)\n\\]\nDespite what reaches the eye is the signal \\(\\ell(x,y)\\), our perception is not the value of \\(\\ell(x,y)\\). In fact, the squares 1 and 2 in Figure¬†33 have the exact same values of intensity, but we see them differently, which is generally explained by saying that we discount (at least partially) the effects of the illumination, \\(l(x,y)\\)."
  },
  {
    "objectID": "lec5.html#retinex-3",
    "href": "lec5.html#retinex-3",
    "title": "Filters and image derivatives",
    "section": "Retinex",
    "text": "Retinex\n\nThe Retinex algorithm, by Land and McCann [@Land1971], is based on modeling images as if they were part of a Mondrian world (images that look like the paintings of Piet Mondrian)."
  },
  {
    "objectID": "lec5.html#retinex-4",
    "href": "lec5.html#retinex-4",
    "title": "Filters and image derivatives",
    "section": "Retinex",
    "text": "Retinex\n\n\n\nAlgorithm\n\n\nBut how can we estimate \\(r(x,y)\\) and \\(l(x,y)\\) by only observing \\(\\ell(x,y)\\)?\n\nThe Retinex algorithm works by first extracting \\(x\\) and \\(y\\) spatial derivatives of the image \\(\\ell(x,y)\\) and then thresholding the gradients.\n\nFirst, we transform the product into a sum using the \\(\\log\\):\n\n\\[\n\\log \\ell(x,y) = \\log r(x,y) + \\log l(x,y)\n\\]\n\nTaking derivatives along \\(x\\) and \\(y\\) is now simple:\n\n\\[\n\\frac{\\partial \\log \\ell(x,y)}{\\partial x} = \\frac{\\partial \\log r(x,y)}{\\partial x} + \\frac{\\partial \\log l(x,y)}{\\partial x}\n\\] And the same thing is done for the derivative along \\(y\\)."
  },
  {
    "objectID": "lec5.html#retinex-5",
    "href": "lec5.html#retinex-5",
    "title": "Filters and image derivatives",
    "section": "Retinex",
    "text": "Retinex\n\n\n\nAlgorithm\n\n\n\nAny derivative larger than the threshold is assigned to the derivative of the reflectance image \\(r(x,y)\\), and the ones smaller than a threshold are assigned to the illumination image \\(l(x,y)\\):\n\n\\[\n\\frac{\\partial \\log r(x,y)}{\\partial x} =  \\begin{cases}\n\\frac{\\partial \\log \\ell(x,y)}{\\partial x} & \\text{if} ~  \\left| \\frac{\\partial \\log \\ell(x,y)}{\\partial x} \\right|&gt;T\\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\n\nThen, the image \\(\\log r(x,y)\\) is obtained by integrating the gradients, and exponentiating the result. Finally, the illumination can be obtained as \\(l(x,y) = \\ell(x,y)/r(x,y)\\)."
  },
  {
    "objectID": "lec5.html#retinex-6",
    "href": "lec5.html#retinex-6",
    "title": "Filters and image derivatives",
    "section": "Retinex",
    "text": "Retinex\n\n\nFigure¬†34: Derivatives classified into reflectance or luminance components."
  },
  {
    "objectID": "lec5.html#retinex-7",
    "href": "lec5.html#retinex-7",
    "title": "Filters and image derivatives",
    "section": "Retinex",
    "text": "Retinex\n\n\nFigure¬†35: Recovered components. The estimated reflectance, \\(r(x,y)\\), is close to what we perceive. It seems that what we perceive contains part of \\(l(x,y)\\)."
  },
  {
    "objectID": "lec5.html#retinex-8",
    "href": "lec5.html#retinex-8",
    "title": "Filters and image derivatives",
    "section": "Retinex",
    "text": "Retinex\n\n\n\nAssumptions\n\n\n\nthe illumination image, \\(l(x,y)\\), varies smoothly\nthe reflectance image, \\(r(x,y)\\), is composed of uniform regions separated by sharp boundaries\n\n\n\n\n\n\nDecomposing an image into different physical causes is known as intrinsic images decomposition. The intrinsic image decomposition, proposed by Barrow and Tenenbaum, aims to recover intrinsic scene characteristics from images, such as occlusions, depth, surface normals, shading, reflectance, reflections, and so on."
  },
  {
    "objectID": "lec5.html#concluding-remarks-1",
    "href": "lec5.html#concluding-remarks-1",
    "title": "Filters and image derivatives",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nIn this chapter we have covered a very powerful image representation: image derivatives (first order, second order, and the Laplacian) and their discrete approximations. Despite the simplicity of this representation, we have seen that it can be used in a number of applications such as image inpaining, separation of illumination and reflectance, and it can be used to explain simple visual illusions. It is not surprising that similar filters like the ones we have seen in this chapter emerge in convolutional neural networks when trained to solve visual tasks."
  },
  {
    "objectID": "lec4.html#introduction",
    "href": "lec4.html#introduction",
    "title": "Fourier Filtering",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\nImportant\n\n\nWe need a more precise language to talk about the effect of linear filters, and the different image components, than to say ‚Äúsharp‚Äù and ‚Äúblurry‚Äù parts of the image.\nThe Fourier transform provides that precision."
  },
  {
    "objectID": "lec4.html#fourier-transform",
    "href": "lec4.html#fourier-transform",
    "title": "Fourier Filtering",
    "section": "Fourier transform",
    "text": "Fourier transform\n\n\n\nFrequencies\n\n\nBy analogy with temporal frequencies, which describe how quickly signals vary over time, a spatial frequency describes how quickly a signal varies over space.\nThe Fourier transform lets us describe a signal as:\n\na sum of complex exponentials,\neach of a different spatial frequency.\n\n\n\n\n\n\nFourier transforms are important to understand modern representations such as positional encoding popularized by transformers."
  },
  {
    "objectID": "lec4.html#fourier-transform-1",
    "href": "lec4.html#fourier-transform-1",
    "title": "Fourier Filtering",
    "section": "Fourier transform",
    "text": "Fourier transform\n\n\n\nLinear filtering re-cap\n\n\nLinear image transforms with the form \\[\n\\mathbf{x} =  \\mathbf{H} \\boldsymbol\\ell_{\\texttt{in}}\n\\] can be thought of as a way of changing the initial pixels representation of \\(\\boldsymbol\\ell_{\\texttt{in}}\\) into a different representation in \\(\\mathbf{x}\\).\n\n\n\n\n\n\n\n\n\nNote\n\n\nWe use \\(\\mathbf{x}\\) to denote an intermediate representation.\n\nthe representation might not be an image.\na useful representation \\(\\mathbf{x}\\) should have a number of interesting properties not immediately available in the original pixels of \\(\\boldsymbol\\ell_{\\texttt{in}}\\)."
  },
  {
    "objectID": "lec4.html#image-transforms",
    "href": "lec4.html#image-transforms",
    "title": "Fourier Filtering",
    "section": "Image Transforms",
    "text": "Image Transforms\n\n\n\nInversion\n\n\nThis representation is specially interesting when it can be inverted so that the original pixels can be recovered:\n\\[\n\\boldsymbol\\ell_{\\texttt{in}}=  \\mathbf{H}^{-1} \\mathbf{x}.\n\\]\nThe Fourier transform is invertible."
  },
  {
    "objectID": "lec4.html#fourier-series",
    "href": "lec4.html#fourier-series",
    "title": "Fourier Filtering",
    "section": "Fourier Series",
    "text": "Fourier Series\n\n\n\nHistory\n\n\nIn 1822, French mathematician and engineer Joseph Fourier, as part of his work on the study on heat propagation, showed that any periodic signal could be written as an infinite sum of trigonometric functions (cosine and sine functions)."
  },
  {
    "objectID": "lec4.html#fourier-series-1",
    "href": "lec4.html#fourier-series-1",
    "title": "Fourier Filtering",
    "section": "Fourier Series",
    "text": "Fourier Series\n\nFourier‚Äôs birthplace"
  },
  {
    "objectID": "lec4.html#fourier-series-2",
    "href": "lec4.html#fourier-series-2",
    "title": "Fourier Filtering",
    "section": "Fourier Series",
    "text": "Fourier Series\n\n\n\nFourier representation\n\n\n\nAny function, \\(\\ell(t)\\) defined in the interval \\(t \\in (0,\\pi)\\), could be expressed as: \\[\n\\ell(t) = a_1 \\sin (t) + a_2 \\sin (2t) + a_3 \\sin (3t) + ...\\]\nThe value of the coefficients \\(a_n\\) is computed as: \\[\na_n = \\frac{2}{\\pi} \\int_0^\\pi  \\ell(t) \\sin (nt) dt.\n\\]\nThe sum is only guaranteed to converge to the function \\(\\ell(t)\\) for \\(t \\in (0,\\pi)\\).\n\n\n\n\n\n\n\nProperties\n\n\nThe resulting sum, for any values \\(a_n\\), is a periodic function with period \\(2\\pi\\) and is anti-symmetric with respect to the origin, \\(t=0\\)."
  },
  {
    "objectID": "lec4.html#fourier-series-3",
    "href": "lec4.html#fourier-series-3",
    "title": "Fourier Filtering",
    "section": "Fourier Series",
    "text": "Fourier Series\n\n\n\nRamp signal expansion\n\n\nOne of Fourier‚Äôs original examples of sine series is the expansion of the ramp signal \\[\n\\ell(t)=t/2.\n\\] This series was first introduced by Euler. Fourier showed that his theory explained why a ramp could be written as the following infinite sum: \\[\n\\frac{1}{2} t =  \\sin (t) - \\frac{1}{2}  \\sin (2t) + \\frac{1}{3} \\sin (3t) - \\frac{1}{4}  \\sin (4t) + ...\n\\]"
  },
  {
    "objectID": "lec4.html#fourier-series-4",
    "href": "lec4.html#fourier-series-4",
    "title": "Fourier Filtering",
    "section": "Fourier Series",
    "text": "Fourier Series\n\n\nFigure¬†1: Reconstruction of a ramp with the first five sine functions."
  },
  {
    "objectID": "lec4.html#fourier-series-5",
    "href": "lec4.html#fourier-series-5",
    "title": "Fourier Filtering",
    "section": "Fourier Series",
    "text": "Fourier Series\n\n\n\nFourier series as a change of representation\n\n\nIt is useful to think of the Fourier series of a signal as a change of representation."
  },
  {
    "objectID": "lec4.html#fourier-series-6",
    "href": "lec4.html#fourier-series-6",
    "title": "Fourier Filtering",
    "section": "Fourier Series",
    "text": "Fourier Series\n\n\n\nAs sums\n\n\nFourier series can also be written as sums of different sets of harmonic functions. For instance, using cosine functions we can describe the ramp function also as: \\[\\frac{1}{2} t =  \\frac{\\pi}{4}  - \\frac{2}{\\pi} \\cos (t) - \\frac{2}{3^2 \\pi}  \\cos (3t) - \\frac{2}{5^2 \\pi}  \\cos (5t) - ...\\] The cosine and sine series of the same function are only equal in the interval \\(t \\in (0, \\pi)\\), and result in different periodic extensions outside that interval."
  },
  {
    "objectID": "lec4.html#continuous-and-discrete-waves",
    "href": "lec4.html#continuous-and-discrete-waves",
    "title": "Fourier Filtering",
    "section": "Continuous and Discrete Waves",
    "text": "Continuous and Discrete Waves\n\n\n\nContinuous time sine wave\n\n\n\\[\ns\\left(t\\right) = A \\sin\\left(w ~t - \\theta \\right)\n\\]\n\n\\(A\\) is the amplitude\n\\(w\\) is the frequency\n\\(\\theta\\) is the phase."
  },
  {
    "objectID": "lec4.html#continuous-and-discrete-waves-1",
    "href": "lec4.html#continuous-and-discrete-waves-1",
    "title": "Fourier Filtering",
    "section": "Continuous and Discrete Waves",
    "text": "Continuous and Discrete Waves\n\n\n\nDiscrete time sine wave\n\n\n\\[\ns\\left[n\\right] = A \\sin\\left(w ~n  - \\theta \\right)\n\\]\n\n\n\n\n\nAlthough \\(\\theta\\) can have any value, here we will consider only the values \\(\\theta=0\\) and \\(\\theta = \\pi/2\\), which correspond to the sine and cosine waves, respectively."
  },
  {
    "objectID": "lec4.html#continuous-and-discrete-waves-2",
    "href": "lec4.html#continuous-and-discrete-waves-2",
    "title": "Fourier Filtering",
    "section": "Continuous and Discrete Waves",
    "text": "Continuous and Discrete Waves\n\n\n\n\n\n\nPeriodicity\n\n\nThe discrete sine wave will not be periodic for any arbitrary value of \\(w\\).\nA discrete signal \\(\\ell\\left[n\\right]\\) is periodic, if there exists \\(T \\in \\mathbb{N}\\) such that \\(\\ell\\left[n\\right] = \\ell\\left[n+mT\\right]\\) for all \\(m \\in \\mathbb{Z}\\).\nFor the discrete sine (and cosine) wave to be periodic the frequency has to be \\(w = 2 \\pi K / N\\) for \\(K,N \\in \\mathbb{N}\\).\nIf \\(K/N\\) is an irreducible fraction, then the period of the wave will be \\(T = N\\) samples."
  },
  {
    "objectID": "lec4.html#continuous-and-discrete-waves-3",
    "href": "lec4.html#continuous-and-discrete-waves-3",
    "title": "Fourier Filtering",
    "section": "Continuous and Discrete Waves",
    "text": "Continuous and Discrete Waves\n\n\n\nAlternative notation\n\n\nIn general, to make explicit the periodicity of the wave we will use the form: \\[s_k\\left[n\\right] = \\sin\\left( \\frac{2 \\pi}{N} \\, k \\, n \\right)\\]\nThe same applies for the cosine: \\[\nc_k\\left[n\\right] = \\cos\\left(\\frac{2 \\pi}{N} \\,k\\,n \\right)\n\\] When considering the set of periodic signals with period \\(N\\), or the set of signals with finite support signals of length \\(N\\) with \\(n \\in \\left[0, N-1\\right]\\), \\(k \\in \\left[1, N/2\\right]\\) denotes the frequency (i.e., the number of wave cycles that will occur within the region of support)."
  },
  {
    "objectID": "lec4.html#continuous-and-discrete-waves-4",
    "href": "lec4.html#continuous-and-discrete-waves-4",
    "title": "Fourier Filtering",
    "section": "Continuous and Discrete Waves",
    "text": "Continuous and Discrete Waves\n\n\nFigure¬†2: \\(A=1\\) and \\(N=20\\), rows corresponding to \\(k=1,2,3\\). For \\(k=3\\) the samples for each oscillation are not identical, because \\(3/20\\) is an irreducible fraction."
  },
  {
    "objectID": "lec4.html#continuous-and-discrete-waves-5",
    "href": "lec4.html#continuous-and-discrete-waves-5",
    "title": "Fourier Filtering",
    "section": "Continuous and Discrete Waves",
    "text": "Continuous and Discrete Waves\n\n\n\nSines and Cosines in 2D\n\n\n\\[\ns_{u,v}\\left[n,m\\right] = A \\sin \\left(2 \\pi \\left( \\frac{u\\,n}{N}  + \\frac{v\\,m}{M}  \\right) \\right)\n\\] \\[\nc_{u,v}\\left[n,m\\right] = A \\cos \\left(2 \\pi \\left( \\frac{u\\,n}{N}  + \\frac{v\\,m}{M}  \\right) \\right)\n\\] where \\(A\\) is the amplitude and \\(u\\) and \\(v\\) are the two spatial frequencies and define how fast or slow the waves change along the spatial dimensions \\(n\\) and \\(m\\)."
  },
  {
    "objectID": "lec4.html#continuous-and-discrete-waves-6",
    "href": "lec4.html#continuous-and-discrete-waves-6",
    "title": "Fourier Filtering",
    "section": "Continuous and Discrete Waves",
    "text": "Continuous and Discrete Waves\n\n\nFigure¬†3: 2D sine waves with \\(N=M=20\\). The frequency values are (a) \\(u=2, v=0\\); (b) \\(u=3, v=1\\); (c) \\(u=7,v=-5\\)."
  },
  {
    "objectID": "lec4.html#continuous-and-discrete-waves-7",
    "href": "lec4.html#continuous-and-discrete-waves-7",
    "title": "Fourier Filtering",
    "section": "Continuous and Discrete Waves",
    "text": "Continuous and Discrete Waves\n\n\n\nComplex exponential waves\n\n\n\\[\ne_{u}\\left[n\\right] = \\exp \\left(2 \\pi j   \\frac{u\\, n}{N}   \\right)\n\\]\nComplex exponentials are related to cosine and sine waves by Euler‚Äôs formula: \\[\n\\exp \\left(j a\\right) = \\cos (a) + j \\sin (a)\n\\qquad(1)\\]"
  },
  {
    "objectID": "lec4.html#continuous-and-discrete-waves-8",
    "href": "lec4.html#continuous-and-discrete-waves-8",
    "title": "Fourier Filtering",
    "section": "Continuous and Discrete Waves",
    "text": "Continuous and Discrete Waves\n\n\nFigure¬†4: Complex exponential wave with (a) \\(N=40\\), \\(k=1\\), \\(A=1\\); and (b) \\(N=40\\), \\(k=3\\), \\(A=1\\). The red and green curves show the real and imaginary waves. The black line is the complex exponential. The dots correspond to the discrete samples."
  },
  {
    "objectID": "lec4.html#continuous-and-discrete-waves-9",
    "href": "lec4.html#continuous-and-discrete-waves-9",
    "title": "Fourier Filtering",
    "section": "Continuous and Discrete Waves",
    "text": "Continuous and Discrete Waves\n\n\n\nComplex wave in 2D\n\n\n\\[\ne_{u,v}\\left[n,m\\right] = \\exp \\left(2 \\pi j \\left(  \\frac{u\\, n}{N}  + \\frac{v\\,m}{M}  \\right) \\right)\n\\] where \\(u\\) and \\(v\\) are the two spatial frequencies.\n\n\n\n\n\n\nSeparability\n\n\nComplex exponentials can be written as the product of two 1D signals: \\[\ne_{u,v}\\left[n,m\\right]   = e_{u}\\left[n\\right] e_{v}\\left[m\\right]\n\\]"
  },
  {
    "objectID": "lec4.html#continuous-and-discrete-waves-10",
    "href": "lec4.html#continuous-and-discrete-waves-10",
    "title": "Fourier Filtering",
    "section": "Continuous and Discrete Waves",
    "text": "Continuous and Discrete Waves\n\n\n\nOrthogonal basis\n\n\nComplex exponentials form an orthogonal basis for discrete signals and images of finite length. For images of size \\(N \\times M\\), \\[\n\\left&lt;e_{u,v}, e_{u',v'} \\right&gt; = \\sum_{n=0}^{N-1} \\sum_{m=0}^{M-1} e_{u,v}\\left[n,m\\right] e^*_{u',v'}\\left[n,m\\right] = MN \\delta \\left[u-u'\\right]\\delta \\left[v-v'\\right]\n\\] Therefore, any finite length discrete image can be decomposed as a linear combination of complex exponentials."
  },
  {
    "objectID": "lec4.html#the-discrete-fourier-transform",
    "href": "lec4.html#the-discrete-fourier-transform",
    "title": "Fourier Filtering",
    "section": "The Discrete Fourier Transform",
    "text": "The Discrete Fourier Transform\n\n\n\nDiscrete Fourier Transform\n\n\nThe Discrete Fourier Transform (DFT) transforms an image \\(\\ell\\left[n,m \\right]\\), of finite size \\(N \\times M\\), into the complex image Fourier transform \\(\\mathscr{L}\\left[u,v \\right]\\) as:\n\\[\n\\mathscr{L}\\left[u,v \\right] =  \n\\mathcal{F} \\left\\{ \\ell\\left[n,m \\right] \\right\\}\n=\n\\sum_{n=0}^{N-1} \\sum_{m=0}^{M-1} \\, \\ell\\left[n,m \\right]\n\\exp{ \\left( -2\\pi j \\left( \\frac{u\\, n}{N} + \\frac{v\\, m}{M} \\right) \\right)}\n\\qquad(2)\\]\n\n\n\n\n\n\nRelationship\n\n\nWe will call \\(\\mathscr{L}\\left[u,v \\right]\\) the Fourier transform of \\(\\ell\\left[m,n \\right]\\). We will often represent the relationship between the signal as its transform as: \\[\\ell\\left[n,m \\right] \\xrightarrow{\\mathscr{F}} \\mathscr{L}\\left[u,v \\right]\\]"
  },
  {
    "objectID": "lec4.html#the-discrete-fourier-transform-1",
    "href": "lec4.html#the-discrete-fourier-transform-1",
    "title": "Fourier Filtering",
    "section": "The Discrete Fourier Transform",
    "text": "The Discrete Fourier Transform\n\n\n\nInverse transform\n\n\nBy applying \\(\\frac{1}{MN} \\sum\\limits_{u=0}^{M-1} \\sum\\limits_{v=0}^{N-1}\\) to both sides of equation (Equation¬†2) and exploiting the orthogonality between distinct Fourier basis elements, we find the inverse Fourier transform relation\n\\[\n\\ell\\left[n,m \\right] =\n\\mathcal{F}^{-1} \\left\\{ \\mathscr{L}\\left[u,v \\right] \\right\\}\n=\n\\frac{1}{NM} \\sum_{u=0}^{N-1} \\sum_{v=0}^{M-1} \\mathscr{L}\\left[u,v \\right]\n\\exp{ \\left(+2\\pi j \\left(\\frac{u\\, n}{N} + \\frac{v\\, m}{M} \\right) \\right) }\n\\qquad(3)\\]"
  },
  {
    "objectID": "lec4.html#the-discrete-fourier-transform-2",
    "href": "lec4.html#the-discrete-fourier-transform-2",
    "title": "Fourier Filtering",
    "section": "The Discrete Fourier Transform",
    "text": "The Discrete Fourier Transform\n\n\n\nImage rewrite\n\n\n\nwe rewrite the image, instead of as a sum of offset pixel values, as a sum of complex exponentials, each at a different frequency, called a spatial frequency for images because they describe how quickly things vary across space.\nfrom the inverse transform formula, we see that to construct an image from a Fourier transform, \\(\\mathscr{L}\\left[u,v \\right]\\), we just add in the corresponding amount of that particular complex exponential (conjugated)."
  },
  {
    "objectID": "lec4.html#the-discrete-fourier-transform-3",
    "href": "lec4.html#the-discrete-fourier-transform-3",
    "title": "Fourier Filtering",
    "section": "The Discrete Fourier Transform",
    "text": "The Discrete Fourier Transform\n\n\n\nPeriodicity\n\n\nAs \\(\\mathscr{L}\\left[u,v \\right]\\) is obtained as a sum of complex exponential with a common period of \\(N,M\\) samples, the function \\(\\mathscr{L}\\left[u,v \\right]\\) is also periodic: \\[\n\\mathscr{L}\\left[u+aN,v+bM \\right] = \\mathscr{L}\\left[u,v \\right]\n\\] for any \\(a,b \\in \\mathbb{Z}\\).\nAlso the result of the inverse DFT is a periodic image. Indeed you can verify from equation (Equation¬†3) that \\[\n\\ell\\left[n+aN,m+bM \\right] = \\ell\\left[n,m \\right]\n\\] for any \\(a,b \\in \\mathbb{Z}\\)."
  },
  {
    "objectID": "lec4.html#the-discrete-fourier-transform-4",
    "href": "lec4.html#the-discrete-fourier-transform-4",
    "title": "Fourier Filtering",
    "section": "The Discrete Fourier Transform",
    "text": "The Discrete Fourier Transform\n\n\n\nEquivalent representation\n\n\nUsing the fact that \\(e_{N-u, M-v} = e_{-u,-v}\\), another equivalent way to write for the Fourier transform is to sum over the frequency interval \\(\\left[-N/2, N/2\\right]\\) and \\(\\left[-M/2, M/2\\right]\\). This is especially useful for the inverse that can be written as: \\[\n\\ell\\left[n,m \\right] = \\frac{1}{NM} \\sum_{u=-N/2}^{N/2} \\sum_{v=-M/2}^{M/2} \\mathscr{L}\\left[u,v \\right]\\exp{ \\left(+2\\pi j \\left(\\frac{u\\, n}{N} + \\frac{v\\, m}{M} \\right) \\right) }\n\\qquad(4)\\]"
  },
  {
    "objectID": "lec4.html#the-discrete-fourier-transform-5",
    "href": "lec4.html#the-discrete-fourier-transform-5",
    "title": "Fourier Filtering",
    "section": "The Discrete Fourier Transform",
    "text": "The Discrete Fourier Transform\n\n\n\nProperties\n\n\n\nthis formulation allows us to arrange the coefficients in the complex plane so that the zero frequency, or DC, coefficient is at the center.\nslow, large variations correspond to complex exponentials of frequencies near the origin\nfrequencies further away from the origin represent faster variation with movement across space\nif the amplitudes of the complex conjugate exponentials are the same, then their sum will represent a cosine wave; otherwise, a sine wave\n\n\n\n\n\n\n\nUniqueness\n\n\nOne very important property is that the decomposition of a signal into a sum of complex exponentials is unique: there is a unique linear combination of the exponentials that will result in a given signal."
  },
  {
    "objectID": "lec4.html#the-discrete-fourier-transform-6",
    "href": "lec4.html#the-discrete-fourier-transform-6",
    "title": "Fourier Filtering",
    "section": "The Discrete Fourier Transform",
    "text": "The Discrete Fourier Transform\n\n\n\nMatrix Form of the Fourier Transform\n\n\nAs the DFT is a linear transform we can also write the DFT in matrix form, with one basis per row. In 1D, the matrix for the DFT is as follows:\n\\[\\mathbf{F} = \\begin{bmatrix}1 & 1 & 1 & \\dots & 1\\\\ %u=0\n1 & \\exp{ \\left(-2\\pi j \\frac{1}{N} \\right)} & \\exp{ \\left(-2\\pi j \\frac{2}{N} \\right)} & \\dots & \\exp{ \\left(-2\\pi j \\frac{N-1}{N} \\right)}\\\\ %u=1\n1 & \\exp{ \\left(-2\\pi j \\frac{2}{N} \\right)} & \\exp{ \\left(-2\\pi j \\frac{4}{N} \\right)} & \\dots & \\exp{ \\left(-2\\pi j \\frac{2\\, (N-1)}{N} \\right)}\\\\ %u=2\n\\vdots & \\vdots & \\vdots & ~ & \\vdots \\\\\n1 & \\exp{ \\left(-2\\pi j \\frac{(N-1)}{N} \\right)} & \\exp{ \\left(-2\\pi j \\frac{(N-1)\\, 2}{N} \\right)} & \\dots & \\exp{ \\left(-2\\pi j \\frac{(N-1)\\, (N-1)}{N} \\right)}\\\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "lec4.html#the-discrete-fourier-transform-7",
    "href": "lec4.html#the-discrete-fourier-transform-7",
    "title": "Fourier Filtering",
    "section": "The Discrete Fourier Transform",
    "text": "The Discrete Fourier Transform\n\n\n\nMatrix Form\n\n\n\n\\(\\mathbf{F}\\) is a symmetric matrix\nthe inverse of the DFT is the complex conjugate: \\(\\mathbf{F}^{-1} = \\mathbf{F}^{*}\\)."
  },
  {
    "objectID": "lec4.html#the-discrete-fourier-transform-8",
    "href": "lec4.html#the-discrete-fourier-transform-8",
    "title": "Fourier Filtering",
    "section": "The Discrete Fourier Transform",
    "text": "The Discrete Fourier Transform\n\n\nFigure¬†5: Visualization of the discrete Fourier transform as a matrix. The signal to be transformed forms the entries of the column vector at right."
  },
  {
    "objectID": "lec4.html#the-discrete-fourier-transform-9",
    "href": "lec4.html#the-discrete-fourier-transform-9",
    "title": "Fourier Filtering",
    "section": "The Discrete Fourier Transform",
    "text": "The Discrete Fourier Transform\n\n\n\nDFT Decomposition\n\n\n\nusing the real and imaginary components: \\[\n\\mathscr{L}\\left[u,v \\right] = Re \\left\\{\\mathscr{L}\\left[u,v \\right] \\right\\}  + j \\, Imag \\left\\{\\mathscr{L}\\left[u,v \\right] \\right\\}\n\\]\nusing a polar decomposition: \\[\n\\mathscr{L}\\left[u,v \\right]  = A \\left[u,v \\right] \\, \\exp{\\left( j \\, \\theta\\left[u,v \\right]  \\right)}\n\\]"
  },
  {
    "objectID": "lec4.html#the-discrete-fourier-transform-10",
    "href": "lec4.html#the-discrete-fourier-transform-10",
    "title": "Fourier Filtering",
    "section": "The Discrete Fourier Transform",
    "text": "The Discrete Fourier Transform\n\n\nFigure¬†6: DFT of an image and visualization of (top) the real and imaginary components, and (bottom) the amplitude and phase of the Fourier transform."
  },
  {
    "objectID": "lec4.html#the-discrete-fourier-transform-11",
    "href": "lec4.html#the-discrete-fourier-transform-11",
    "title": "Fourier Filtering",
    "section": "The Discrete Fourier Transform",
    "text": "The Discrete Fourier Transform\n\n\nFigure¬†7: Reconstructing an image from the \\(N\\) Fourier coefficients of the largest amplitude. The right frame shows the location, in the Fourier domain, of the \\(N\\) Fourier coefficients, which when inverted, give the image at the left."
  },
  {
    "objectID": "lec4.html#useful-transforms",
    "href": "lec4.html#useful-transforms",
    "title": "Fourier Filtering",
    "section": "Useful Transforms",
    "text": "Useful Transforms\n\n\n\nDelta Distribution\n\n\nThe Fourier transform of the delta function \\(\\delta \\left[n,m \\right]\\) is \\[\n\\mathcal{F} \\left\\{ \\delta \\left[n,m \\right] \\right\\} =\n\\sum_{n=0}^{N-1} \\sum_{m=0}^{M-1} \\, \\delta \\left[n,m \\right]\n\\exp{ \\left( -2\\pi j \\left( \\frac{u\\, n}{N} + \\frac{v\\, m}{M} \\right) \\right)} = 1\n\\] where the Fourier transform of the delta signal is 1: \\[\n\\delta \\left[n,m \\right] \\xrightarrow{\\mathscr{F}} 1.\n\\]"
  },
  {
    "objectID": "lec4.html#useful-transforms-1",
    "href": "lec4.html#useful-transforms-1",
    "title": "Fourier Filtering",
    "section": "Useful Transforms",
    "text": "Useful Transforms\n\n\n\nInverse for delta\n\n\nIf we think in terms of the inverse Fourier transform, this means that if we sum all the complex exponentials with a coefficient of 1, then all the values will cancel but the one at the origin, which results in a delta function: \\[\n\\delta \\left[n,m \\right] = \\frac{1}{NM} \\sum_{u=-N/2}^{N/2} \\sum_{v=-M/2}^{M/2}  \n\\exp{ \\left(2\\pi j \\left(\\frac{u\\, n}{N} + \\frac{v\\, m}{M} \\right) \\right) }\n\\]"
  },
  {
    "objectID": "lec4.html#useful-transforms-2",
    "href": "lec4.html#useful-transforms-2",
    "title": "Fourier Filtering",
    "section": "Useful Transforms",
    "text": "Useful Transforms\n\n\n\nCosine and Sine Waves\n\n\nThe Fourier transform of the cosine wave, \\(\\cos{ \\left( 2\\pi \\left( \\frac{u_0\\, n}{N} + \\frac{v_0\\, m}{M} \\right) \\right) }\\), is: \\[\n\\begin{aligned}\n\\sum_{n=0}^{N-1} \\sum_{m=0}^{M-1} \\, \\cos{ \\left( 2\\pi \\left( \\frac{u_0 \\, n}{N} + \\frac{v_0 \\, m}{M} \\right) \\right) }\n\\exp{ \\left( -2\\pi j \\left( \\frac{u\\, n}{N} + \\frac{v\\, m}{M} \\right) \\right)} = \\\\\n=\\frac{1}{2} \\left( \\delta \\left[u-u_0, v-v_0 \\right] +  \\delta \\left[u+u_0, v+v_0 \\right] \\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "lec4.html#useful-transforms-3",
    "href": "lec4.html#useful-transforms-3",
    "title": "Fourier Filtering",
    "section": "Useful Transforms",
    "text": "Useful Transforms\n\n\n\nFourier transform relationship\n\n\n\\[\n\\cos{ \\left( 2\\pi \\left( \\frac{u_0\\, n}{N} + \\frac{v_0\\, m}{M} \\right) \\right) }\n\\xrightarrow{\\mathscr{F}}\n\\frac{1}{2} \\left( \\delta \\left[u-u_0, v-v_0 \\right] +  \\delta \\left[u+u_0, v+v_0 \\right] \\right)\n\\]\nAnd for the sine wave: \\(\\sin{ \\left( 2\\pi \\left( \\frac{u_0\\, n}{N} + \\frac{v_0 m}{M} \\right) \\right) }\\): \\[\n\\sin{ \\left( 2\\pi \\left( \\frac{u_0\\, n}{N} + \\frac{v_0 m}{M} \\right) \\right) }\n\\xrightarrow{\\mathscr{F}}\n\\frac{1}{2j} \\left( \\delta \\left[u-u_0, v-v_0 \\right] - \\delta \\left[u+u_0, v+v_0 \\right]\\right)\n\\]"
  },
  {
    "objectID": "lec4.html#useful-transforms-4",
    "href": "lec4.html#useful-transforms-4",
    "title": "Fourier Filtering",
    "section": "Useful Transforms",
    "text": "Useful Transforms\n\n\nFigure¬†8: Some 2D Fourier transform pairs. Images are \\(64 \\times 64\\) pixels. The waves are cosine with frequencies \\((1,2)\\), \\((5,0)\\), \\((10,7)\\), \\((11,-15)\\). The last two examples show the sum of two waves and the product."
  },
  {
    "objectID": "lec4.html#useful-transforms-5",
    "href": "lec4.html#useful-transforms-5",
    "title": "Fourier Filtering",
    "section": "Useful Transforms",
    "text": "Useful Transforms\n\n\nFigure¬†9: Some two-dimensional Fourier transform pairs. Note the trends visible in the collection of transform pairs: As the support of the image in one domain gets larger, the magnitude in the other domain becomes more localized. A line transforms to a line oriented perpendicularly to the first. Images are \\(64 \\times 64\\) pixels, origin is in the center. Depicted signals are symmetric. All these images except for the last are separable."
  },
  {
    "objectID": "lec4.html#useful-transforms-6",
    "href": "lec4.html#useful-transforms-6",
    "title": "Fourier Filtering",
    "section": "Useful Transforms",
    "text": "Useful Transforms\n\n\n\nBox Function\n\n\nThe box function is defined as follows: \\[\n\\text{box}_{L} \\left[n \\right] =\n\\begin{cases}\n    1       & \\quad \\text{if } -L \\leq n \\leq L  \\\\\n    0       & \\quad \\text{otherwise.}\n  \\end{cases}\n\\] The duration of the box is \\(2L+1\\).\n\n\n\n\nBox function for \\(L=5\\) and \\(N=32\\)."
  },
  {
    "objectID": "lec4.html#useful-transforms-7",
    "href": "lec4.html#useful-transforms-7",
    "title": "Fourier Filtering",
    "section": "Useful Transforms",
    "text": "Useful Transforms\n\n\n\nDFT for finite length box function\n\n\n\\[\n\\begin{aligned}\n\\mathcal{F} \\left\\{ \\text{box}_{L} \\left[n \\right] \\right\\} &=&\n\\sum_{n=0}^{N-1} \\, \\text{box}_{L} \\left[n \\right]\n\\exp{ \\left( -2\\pi j \\frac{u\\, n}{N} \\right)} \\\\\n&=& \\sum_{n=-L}^{L} \\,\n\\exp{ \\left( -2\\pi j \\frac{u\\, n}{N} \\right)}\n\\end{aligned} \\qquad(5)\\]\nWe can use the equation of the sum of a geometric series: \\[\n\\sum_{n=-L}^{L} a^n = a^{-L} \\sum_{n=0}^{2L} a^n = a^{-L} \\frac{1-a^{2L+1}}{1-a} = \\frac{a^{-(2L+1)/2}-a^{(2L+1)/2}}{a^{-1/2}-a^{1/2}}\n\\] where \\(a\\) is a constant. With \\(a = \\exp{ \\left( -2\\pi j \\frac{u}{N} \\right)}\\) we can write the sum in equation (Equation¬†5) as\n\\[\n\\begin{aligned}\n\\sum_{n=-L}^{L} \\,\n\\exp{ \\left( -2\\pi j \\frac{u\\, n}{N} \\right)} &=\n\\frac{\\exp{ \\left( \\pi j \\frac{u(2L+1)}{N} \\right)} - \\exp{ \\left( -\\pi j \\frac{u(2L+1)}{N} \\right)}}\n{\\exp{ \\left( \\pi j \\frac{u}{N} \\right)} - \\exp{ \\left( - \\pi j \\frac{u}{N} \\right)} } \\\\\n&= \\frac{\\sin \\left( \\pi u(2L+1)/N \\right)}{\\sin \\left( \\pi u/N \\right)}\n\\end{aligned}\n\\qquad(6)\\]"
  },
  {
    "objectID": "lec4.html#useful-transforms-8",
    "href": "lec4.html#useful-transforms-8",
    "title": "Fourier Filtering",
    "section": "Useful Transforms",
    "text": "Useful Transforms\n\n\n\nDFT for finite length box function\n\n\n\\[\n\\text{box}_{L} \\left[n \\right]\\xrightarrow{\\mathscr{F}}\n\\frac{\\sin \\pi u (2L+1)/N}{\\sin \\pi u/N}\n\\]\n\n\n\n\n\n\nDiscrete sinc function\n\n\nThe discrete sinc function sincd is defined as follows: \\[\n\\text{sincd}(x;a) = \\frac{\\sin(\\pi x)}{a \\sin (\\pi x/a)}\n\\] Where \\(a\\) is a constant. This is a symmetrical periodic function with a maximum value of 1."
  },
  {
    "objectID": "lec4.html#useful-transforms-9",
    "href": "lec4.html#useful-transforms-9",
    "title": "Fourier Filtering",
    "section": "Useful Transforms",
    "text": "Useful Transforms\n\n\nFigure¬†10: DFT of the box filter with \\(L=5\\), and \\(N=32\\)."
  },
  {
    "objectID": "lec4.html#useful-transforms-10",
    "href": "lec4.html#useful-transforms-10",
    "title": "Fourier Filtering",
    "section": "Useful Transforms",
    "text": "Useful Transforms\n\n\n\n2D case\n\n\nA 2D box is a separable function: \\[\\text{box}_{L_n, L_m} \\left[n,m\\right] = \\text{box}_{L_n} \\left[n\\right] \\text{box}_{L_m}\\left[m\\right]\n\\] The DFT is the product of the two DFTs: \\[\n\\text{Box}_{L_n, L_m} \\left[ u,v \\right] = \\text{Box}_{L_n} \\left[ u\\right] \\text{Box}_{L_m} \\left[ v\\right].\n\\]"
  },
  {
    "objectID": "lec4.html#sec-DFTproperties",
    "href": "lec4.html#sec-DFTproperties",
    "title": "Fourier Filtering",
    "section": "DFT Properties",
    "text": "DFT Properties\n\n\n\nTable¬†1: Table of basic DFT transforms and properties\n\n\n\n\n\n\n\n\n\n\\(\\ell[n]\\)\n\\(\\mathscr{L}[u]\\)\n\n\n\n\n\\(\\ell_1[n] \\circ \\ell_2[n]\\)\n\\(\\mathscr{L}_1[u] \\mathscr{L}_2[u]\\)\n\n\n\\(\\ell_1[n] \\ell_2[n]\\)\n\\(\\frac{1}{N} \\mathscr{L}_1[u] \\circ \\mathscr{L}_2[u]\\)\n\n\n\\(\\ell\\left[n-n_0\\right]\\)\n\\(\\mathscr{L}[u] \\exp \\left(-2 \\pi j \\frac{u n_0}{N}\\right)\\)\n\n\n\\(\\delta[n]\\)\n1\n\n\n\\(\\exp \\left(2 \\pi u_0 \\frac{n}{N}\\right)\\)\n\\(\\delta\\left[u-u_0\\right]\\)\n\n\n\\(\\cos \\left(2 \\pi u_0 \\frac{n}{N}\\right)\\)\n\\(\\frac{1}{2}\\left(\\delta\\left[u-u_0\\right]+\\delta\\left[u+u_0\\right]\\right)\\)\n\n\n\\(\\sin \\left(2 \\pi u_0 \\frac{n}{N}\\right)\\)\n\\(\\frac{1}{2 j}\\left(\\delta\\left[u-u_0\\right]-\\delta\\left[u+u_0\\right]\\right)\\)\n\n\n\\(\\operatorname{box}_L[n]\\)\n\\(\\frac{\\sin \\pi u(2 L+1) / N}{\\sin \\pi u / N}\\)"
  },
  {
    "objectID": "lec4.html#discrete-fourier-transform-properties",
    "href": "lec4.html#discrete-fourier-transform-properties",
    "title": "Fourier Filtering",
    "section": "Discrete Fourier Transform Properties",
    "text": "Discrete Fourier Transform Properties\n\n\n\nLinearity\n\n\nThe Fourier transform and its inverse are linear transformations: \\[\n\\alpha \\ell_1 \\left[n,m \\right] + \\beta \\ell_2 \\left[ n,m \\right]  \n\\xrightarrow{\\mathscr{F}}\n\\alpha \\mathscr{L}_1 \\left[u,v \\right] + \\beta \\mathscr{L}_2 \\left[ u,v \\right]\n\\] where \\(\\alpha\\) and \\(\\beta\\) are complex numbers.\n\n\n\n\n\n\nSeparability\n\n\nAn image is separable if it can be written as the product of two 1D signals, \\[\n\\ell\\left[n,m \\right] = \\ell_1\\left[n \\right] \\ell_2\\left[m \\right].\n\\] If an image is separable, then its Fourier transform is separable: \\[\n\\mathscr{L}\\left[u,v \\right] = \\mathscr{L}_1 \\left[u \\right] \\mathscr{L}_2 \\left[v \\right].\n\\]"
  },
  {
    "objectID": "lec4.html#dft-properties",
    "href": "lec4.html#dft-properties",
    "title": "Fourier Filtering",
    "section": "DFT Properties",
    "text": "DFT Properties\n\n\n\n\n\nOriginal image\n\n\n\n\n\n\nSeparable approximation"
  },
  {
    "objectID": "lec4.html#dft-properties-1",
    "href": "lec4.html#dft-properties-1",
    "title": "Fourier Filtering",
    "section": "DFT Properties",
    "text": "DFT Properties\n\n\n\nParseval‚Äôs Theorem\n\n\nPreservation of the dot product between two signals and the norm of a vector after the basis change (up to a constant factor): \\[\\sum_{n=0}^{N-1} \\sum_{m=0}^{M-1} \\,  \\ell_1 \\left[n,m \\right] \\ell_2^* \\left[n,m \\right] = \\frac{1}{NM}\\sum_{u=0}^{N-1} \\sum_{v=0}^{M-1} \\,  \\mathscr{L}_1 \\left[u,v \\right] \\mathscr{L}_2^* \\left[u,v \\right]\\]\n\n\n\n\n\n\nPlancherel theorem\n\n\nIf \\(\\ell_1=\\ell_2\\), this reduces to: \\[\n\\sum_{n=0}^{N-1} \\sum_{m=0}^{M-1} \\,  \\| \\ell\\left[n,m \\right] \\|^2 = \\frac{1}{NM}\\sum_{u=0}^{N-1} \\sum_{v=0}^{M-1} \\,  \\| \\mathscr{L}\\left[u,v \\right] \\|^2\n\\]"
  },
  {
    "objectID": "lec4.html#dft-properties-2",
    "href": "lec4.html#dft-properties-2",
    "title": "Fourier Filtering",
    "section": "DFT Properties",
    "text": "DFT Properties\n\n\n\nConvolution\n\n\nConsider: \\[\n\\ell_{\\texttt{out}}= \\ell_1 \\circ \\ell_2\n\\]\n\\[\n\\begin{split}\n\\mathscr{L}_{\\texttt{out}}\\left[u,v \\right] & =  \\mathcal{F}  \\left\\{ \\ell_1 \\circ_{N,M} \\ell_2 \\right\\} \\\\\n& =  \n\\sum_{n=0}^{N-1}  \\sum_{m=0}^{M-1}\n\\left\\{\n\\sum_{k=0}^{N-1} \\sum_{l=0}^{M-1}\n\\ell_1 \\left[n-k, m-l \\right] \\ell_2 \\left[k,l \\right]\n\\right\\}\n\\exp \\left(-2 \\pi j \\left(\\frac{nu}{N} + \\frac{mv}{M} \\right) \\right)\n\\end{split}\n\\] Introducing \\(n' = n - k\\) and \\(m' = m - l\\), we have: \\[\n\\mathscr{L}_{\\texttt{out}}\\left[u,v \\right] =\n\\sum_{k=0}^{N-1} \\sum_{l=0}^{M-1}\n\\ell_2 \\left[k,l \\right]\n\\sum_{n'=-k}^{N-k-1}  \\sum_{m'=-l}^{M-l-1}\n\\ell_1 \\left[n', m' \\right]\\times \\\\\n\\times \\exp{ \\left(-2 \\pi j \\left(\\frac{(n'+k)u}{N} + \\frac{(m'+l)v}{M} \\right) \\right)}\n\\]"
  },
  {
    "objectID": "lec4.html#dft-properties-3",
    "href": "lec4.html#dft-properties-3",
    "title": "Fourier Filtering",
    "section": "DFT Properties",
    "text": "DFT Properties\n\n\n\nConvolution\n\n\nRecognizing that the last two summations give the DFT of \\(x\\left[n,m\\right]\\), using circular boundary conditions, gives \\[\n\\mathscr{L}_{\\texttt{out}}\\left[u,v \\right] = \\sum_{k=0}^{N-1} \\sum_{l=0}^{M-1}  \\mathscr{L}_1 \\left[u,v\\right] \\exp{ \\left(-2 \\pi j \\left(\\frac{ku}{N}+\\frac{lv}{M} \\right ) \\right)} \\ell_2 \\left[k,l\\right].\n\\] Performing the DFT indicated by the second two summations gives: \\[\n\\mathscr{L}_{\\texttt{out}}\\left[u,v\\right] = \\mathscr{L}_1 \\left[u,v\\right] \\mathscr{L}_2 \\left[u,v\\right]\n\\]\nTherefore: \\[\n\\ell_1 \\left[n,m\\right] \\circ_{N,M} \\ell_2 \\left[n,m\\right]\n\\xrightarrow{\\mathscr{F}}\n\\mathscr{L}_1 \\left[u,v\\right] \\mathscr{L}_2 \\left[u,v\\right]\n\\]"
  },
  {
    "objectID": "lec4.html#discrete-fourier-transform-properties-1",
    "href": "lec4.html#discrete-fourier-transform-properties-1",
    "title": "Fourier Filtering",
    "section": "Discrete Fourier Transform Properties",
    "text": "Discrete Fourier Transform Properties\n\n\n\nConvolution\n\n\nBecause the Fourier bases are the eigenfunctions of all space invariant linear operators, if you start with a complex exponential, and apply any linear, space invariant operator to it, you always come out with a complex exponential of that same frequency, but, in general, with some different amplitude and phase.\nThis property lets us examine the operation of a filter on any image by examining how it modulates the Fourier coefficients of any image.\n\n\n\n\n\nNote that the autocorrelation function does not have this property."
  },
  {
    "objectID": "lec4.html#discrete-fourier-transform-properties-2",
    "href": "lec4.html#discrete-fourier-transform-properties-2",
    "title": "Fourier Filtering",
    "section": "Discrete Fourier Transform Properties",
    "text": "Discrete Fourier Transform Properties\n\n\n\nDual Convolution\n\n\nThe Fourier transform of the product of two images is the (circular) convolution of their DFTs:\n\\[\n\\ell_1 \\left[n,m\\right] \\ell_2 \\left[n,m\\right]\n\\xrightarrow{\\mathscr{F}}\n\\frac{1}{NM} \\mathscr{L}_1 \\left[u,v\\right] \\circ \\mathscr{L}_2 \\left[u,v\\right]\n\\]"
  },
  {
    "objectID": "lec4.html#discrete-fourier-transform-properties-3",
    "href": "lec4.html#discrete-fourier-transform-properties-3",
    "title": "Fourier Filtering",
    "section": "Discrete Fourier Transform Properties",
    "text": "Discrete Fourier Transform Properties\n\n\n\nShift (Translation in space)\n\n\nWhen displacing the image by \\((n_0, m_0)\\) pixels, we get \\(\\ell\\left[n-n_0,m-m_0 \\right]\\) and its Fourier transform is: \\[\n\\begin{split}\n\\mathcal{F} \\left\\{ \\ell\\left[n-n_0,m-m_0 \\right] \\right\\} &=  \\sum_{n=0}^{N-1} \\sum_{m=0}^{M-1} \\, \\ell\\left[n-n_0,m-m_0 \\right] \\exp{ \\left( -2\\pi j \\left( \\frac{u\\, n}{N} + \\frac{v\\, m}{M} \\right) \\right)} = \\\\\n& =  \\sum_{n=0}^{N-1} \\sum_{m=0}^{M-1} \\, \\ell\\left[n,m \\right] \\exp{ \\left( -2\\pi j \\left( \\frac{u\\, (n+n_0)}{N} + \\frac{v\\, (m+m_0)}{M} \\right) \\right)}  = \\\\\n& =   \\mathscr{L}\\left[u,v \\right]  \\exp{ \\left( -2\\pi j \\left( \\frac{u\\, n_0}{N} + \\frac{v\\, m_0}{M} \\right) \\right)}\n\\end{split} \\qquad(7)\\]\n\n\n\n\n\nIn practice, if we have an image and we apply a translation there will be some boundary artifacts. So, in general, this property is only true if we apply a circular shift."
  },
  {
    "objectID": "lec4.html#dft-properties-4",
    "href": "lec4.html#dft-properties-4",
    "title": "Fourier Filtering",
    "section": "DFT Properties",
    "text": "DFT Properties\n\n\n\nShift (Translation in space)\n\n\n\n\n\n\n\n\nFigure¬†11: Translation in space. Image (c) corresponds to image (a) after a translation of 16 pixels to the right and four pixels down. Images (b) and (d) show the real parts of their corresponding DFTs (with \\(N=128\\)). Image (f) shows the real part of the ratio between the two DFTs, and (e) is the inverse transform of the ratio between DFTs."
  },
  {
    "objectID": "lec4.html#dft-properties-5",
    "href": "lec4.html#dft-properties-5",
    "title": "Fourier Filtering",
    "section": "DFT Properties",
    "text": "DFT Properties\n\n\n\nModulation (Translation in frequency)\n\n\nIf we multiply an image with a complex exponential, its FT is translated:\n\\[\n\\ell\\left[n,m \\right]  \\exp{ \\left( -2\\pi j \\left( \\frac{u_0\\, n}{N} + \\frac{v_0\\, m}{M} \\right) \\right)}\\xrightarrow{\\mathscr{F}}\\mathscr{L}\\left[u-u_0,v-v_0 \\right]\n\\qquad(8)\\]\nA related relationship is as follows:\n\\[\n\\ell\\left[n,m \\right]  \\cos{ \\left( 2\\pi j \\left( \\frac{u_0\\, n}{N} + \\frac{v_0\\, m}{M} \\right) \\right)}\\xrightarrow{\\mathscr{F}}\n\\mathscr{L}\\left[u-u_0,v-v_0 \\right] + \\mathscr{L}\\left[u+u_0,v+v_0 \\right]\n\\]"
  },
  {
    "objectID": "lec4.html#dft-properties-6",
    "href": "lec4.html#dft-properties-6",
    "title": "Fourier Filtering",
    "section": "DFT Properties",
    "text": "DFT Properties\n\n\n\nModulation (Translation in frequency)\n\n\n\n\n\n\n\n\nFigure¬†12: Modulation in space. Multiplying an image by a cosine wave results in a new image with a Fourier transform with two copies of the Fourier transform of the original image. Only the magnitude of the Fourier transforms are shown."
  },
  {
    "objectID": "lec4.html#a-family-of-fourier-transforms",
    "href": "lec4.html#a-family-of-fourier-transforms",
    "title": "Fourier Filtering",
    "section": "A Family of Fourier Transforms",
    "text": "A Family of Fourier Transforms\n\n\n\nFourier Transform for Continuous Signals\n\n\n\\[\n\\mathscr{L}(w) =  \\int_{-\\infty}^{\\infty} \\ell(t) \\exp{ \\left( - j w t \\right)} \\, dt\n\\qquad(9)\\]\n\n\n\n\n\n\nInverse Fourier Transform for Continuous Signals\n\n\n\\[\n\\ell(t) = \\frac{1}{2 \\pi} \\int_{-\\infty}^{\\infty} \\mathscr{L}(w) \\exp{ \\left( j w t \\right) } \\, dw\n\\qquad(10)\\]\n\n\n\n\n\n\nConvolution between Continuous Signals\n\n\n\\[\n\\ell_{\\texttt{out}}(t) =  \\ell_1 \\circ \\ell_2 = \\int_{-\\infty}^{\\infty} \\ell_1 (t-t') \\ell_2(t') \\, d t'\n\\qquad(11)\\]"
  },
  {
    "objectID": "lec4.html#a-family-of-fourier-transforms-1",
    "href": "lec4.html#a-family-of-fourier-transforms-1",
    "title": "Fourier Filtering",
    "section": "A Family of Fourier Transforms",
    "text": "A Family of Fourier Transforms\n\n\n\nFourier Transform for Infinite Length Discrete Signals\n\n\nBy replacing \\(w = 2 \\pi u / N\\) in equation (Equation¬†2), we can write: \\[\n\\mathscr{L}(w) = \\sum_{n=-\\infty}^{\\infty} \\ell\\left[n \\right] \\exp{(- j w n)}\n\\] The frequency \\(w\\) is now a continuous variable. The Fourier transform \\(\\mathscr{L}(w)\\) is a periodic function with period \\(2 \\pi\\).\n\n\n\n\n\n\nInverse Fourier Transform for Infinite Length Discrete Signals\n\n\n\\[\n\\ell\\left[n \\right] = \\frac{1}{2\\pi} \\int_{2 \\pi} \\mathscr{L}(w) \\exp{(j w n)} d w\n\\] where the integral is done only in one of the periods."
  },
  {
    "objectID": "lec4.html#a-family-of-fourier-transforms-2",
    "href": "lec4.html#a-family-of-fourier-transforms-2",
    "title": "Fourier Filtering",
    "section": "A Family of Fourier Transforms",
    "text": "A Family of Fourier Transforms\n\n\n\nTable¬†2: Fourier transforms.\n\n\n\n\n\n\n\n\n\n\n\nTime Domain\nFT\nFT\\(^{-1}\\)\nFrequency Domain\n\n\n\n\nDiscrete time, Finite length (\\(N\\))\n\\(\\mathscr{L} \\left[u \\right] = \\sum_{n=0}^{N-1} \\ell \\left[n \\right] e^{-2 \\pi j \\frac{un}{N}}\\)\n\\(\\ell \\left[n \\right] = \\frac{1}{N} \\sum_{u=0}^{N-1} \\mathscr{L} \\left[u \\right] e^{2 \\pi j \\frac{un}{N} }\\)\nDiscrete frequency, Finite length (\\(N\\))\n\n\nContinuous time, Infinite length\n\\(\\mathscr{L} (w) = \\int_{- \\infty}^{\\infty} \\ell (t) e^{- j w t} dt\\)\n\\(\\ell (t) = \\frac{1}{2\\pi} \\int_{- \\infty}^{\\infty} \\mathscr{L} (w) e^{j w t} d w\\)\nContinuous frequency, Infinite length\n\n\nDiscrete time, Infinite length\n\\(\\mathscr{L} (w) = \\sum_{n=-\\infty}^{\\infty} \\ell \\left[n \\right] e^{- j w n}\\)\n\\(\\ell \\left[n \\right] = \\frac{1}{2\\pi} \\int_{2 \\pi} \\mathscr{L} (w) e^{j w n} d w\\)\nContinuous frequency, Finite length (\\(2 \\pi\\))"
  },
  {
    "objectID": "lec4.html#ft-as-an-image-representation",
    "href": "lec4.html#ft-as-an-image-representation",
    "title": "Fourier Filtering",
    "section": "FT as an Image Representation",
    "text": "FT as an Image Representation\n\n\n\nRepresentation\n\n\nThe Fourier transform of an image can be written in polar form: \\[\\mathscr{L}\\left[u,v \\right]  = A \\left[u,v \\right] \\, \\exp{\\left( j\\, \\theta\\left[u,v \\right]  \\right)}\\] where \\(A \\left[u,v \\right] = \\left| \\mathscr{L}\\left[u,v \\right]  \\right|\\) and \\(\\theta \\left[u,v \\right] = \\angle  \\mathscr{L}\\left[u,v \\right]\\).\n\n\\(A \\left[u,v \\right]\\) gives the strength of the weight for each complex exponential: intensity scaling.\nthe phase \\(\\theta \\left[u,v \\right]\\) translates the complex exponential: location information."
  },
  {
    "objectID": "lec4.html#ft-as-an-image-representation-1",
    "href": "lec4.html#ft-as-an-image-representation-1",
    "title": "Fourier Filtering",
    "section": "FT as an Image Representation",
    "text": "FT as an Image Representation\n\n\nFigure¬†13: Swapping the amplitude and the phase of the Fourier Transform of two images. Each color channel is processed in the same way."
  },
  {
    "objectID": "lec4.html#ft-as-an-image-representation-2",
    "href": "lec4.html#ft-as-an-image-representation-2",
    "title": "Fourier Filtering",
    "section": "FT as an Image Representation",
    "text": "FT as an Image Representation\n\n\n\n\n\n\nImportant\n\n\nMagnitude of the DFT of natural images is quite similar and can be approximated by \\[\nA \\left[u,v \\right] = a/ (u^2+v^2)^b\n\\] with \\(a\\) and \\(b\\) being two constants."
  },
  {
    "objectID": "lec4.html#ft-as-an-image-representation-3",
    "href": "lec4.html#ft-as-an-image-representation-3",
    "title": "Fourier Filtering",
    "section": "FT as an Image Representation",
    "text": "FT as an Image Representation\n\n\n\nAmplitude intuition\n\n\nThe amplitude is great for capturing images that contain strong periodic patterns. Experiment:\n\ncompute the Fourier transform of an image\ncreate two images by applying the inverse FT when removing one of the components while keeping the other original component\nfor the amplitude image, we will randomize the phase\nfor the phase image, we will replace the amplitude by a noninformative \\(A \\left[u,v \\right] = 1/(u^2+v^2)^{1/2}\\) for all images (better than random)."
  },
  {
    "objectID": "lec4.html#ft-as-an-image-representation-4",
    "href": "lec4.html#ft-as-an-image-representation-4",
    "title": "Fourier Filtering",
    "section": "FT as an Image Representation",
    "text": "FT as an Image Representation\n\n\nFigure¬†14: The relative importance of phase and amplitude depends on the image."
  },
  {
    "objectID": "lec4.html#ft-as-an-image-representation-5",
    "href": "lec4.html#ft-as-an-image-representation-5",
    "title": "Fourier Filtering",
    "section": "FT as an Image Representation",
    "text": "FT as an Image Representation\n\n\nFigure¬†15: The Fourier transform matching game: Match each image (a-h) with its corresponding Fourier transform magnitude (1-8)."
  },
  {
    "objectID": "lec4.html#sec-transfer_function",
    "href": "lec4.html#sec-transfer_function",
    "title": "Fourier Filtering",
    "section": "Fourier Analysis of Linear Filters",
    "text": "Fourier Analysis of Linear Filters\n\n\n\nLinear filters as convolutions:\n\n\n\n\n\nConvolutional linear filter.\n\n\nwhere \\(h \\left[n,m \\right]\\) is the impulse response of the system, or convolution kernel.\n\n\n\n\n\n\nLinear filters as products in Fourier domain\n\n\n\n\n\nTransfer function of a linear filter.\n\n\nThe function \\(H \\left[u, v \\right]\\) is called the transfer function of the filter."
  },
  {
    "objectID": "lec4.html#fourier-analysis-of-linear-filters",
    "href": "lec4.html#fourier-analysis-of-linear-filters",
    "title": "Fourier Filtering",
    "section": "Fourier Analysis of Linear Filters",
    "text": "Fourier Analysis of Linear Filters\n\n\n\nPolar form\n\n\n\\[\nH \\left[u,v \\right] = \\left|H \\left[u, v \\right] \\right| \\exp \\left( {j \\, \\angle H \\left[u, v \\right]} \\right)\n\\qquad(12)\\]\n\nthe magnitude \\(\\left| H \\left[u, v \\right] \\right|\\) is the amplitude gain\nthe phase \\(\\angle H \\left[u, v \\right]\\) is the phase shift\nthe magnitude at the origin, \\(\\left| H \\left[0, 0 \\right] \\right|\\), is the DC gain of the filter\nthe average value of the output signal is equal to the average value of the input times the DC gain."
  },
  {
    "objectID": "lec4.html#fourier-analysis-of-linear-filters-1",
    "href": "lec4.html#fourier-analysis-of-linear-filters-1",
    "title": "Fourier Filtering",
    "section": "Fourier Analysis of Linear Filters",
    "text": "Fourier Analysis of Linear Filters\n\n\n\nFilters as frequency blockers\n\n\nFilters are many times classified according to the frequencies that they let pass through the filter.\n\n\n\n\n\n\nFigure¬†16: Sketch of the frequency responses of low-pass, band-pass, and high-pass filters."
  },
  {
    "objectID": "lec4.html#fourier-analysis-of-linear-filters-2",
    "href": "lec4.html#fourier-analysis-of-linear-filters-2",
    "title": "Fourier Filtering",
    "section": "Fourier Analysis of Linear Filters",
    "text": "Fourier Analysis of Linear Filters\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\nFigure¬†17: An image filtered by (left) low-pass, (middle) band-pass, and (right) high-pass filters."
  },
  {
    "objectID": "lec4.html#fourier-analysis-of-linear-filters-3",
    "href": "lec4.html#fourier-analysis-of-linear-filters-3",
    "title": "Fourier Filtering",
    "section": "Fourier Analysis of Linear Filters",
    "text": "Fourier Analysis of Linear Filters\n\n\n\nExample 1: Removing the Columns from the MIT Building\n\n\n\n\n\n\n\n\nFigure¬†18: Simple filtering in the Fourier domain. (a) The repeated columns of the building of the MIT dome generate harmonics along a horizontal line in the Fourier domain shown in (b). By zeroing out those Fourier components, as done in (d), the columns of the building are substantially removed (c). We can also the complementary operation keeping only those harmonics, shown in (f), which results in keeping only the columns (e)."
  },
  {
    "objectID": "lec4.html#fourier-analysis-of-linear-filters-4",
    "href": "lec4.html#fourier-analysis-of-linear-filters-4",
    "title": "Fourier Filtering",
    "section": "Fourier Analysis of Linear Filters",
    "text": "Fourier Analysis of Linear Filters\n\n\n\nExample 2: The Human Visual System and the Contrast Sensitivity Function\n\n\nVisual psychophysics is an experimental science that studies the relationship between real world stimuli and our perception.\n\n\n\n\nThe concept of psychophysics was introduced by Gustav Theodor Fechner (1801‚Äì1887)."
  },
  {
    "objectID": "lec4.html#fourier-analysis-of-linear-filters-5",
    "href": "lec4.html#fourier-analysis-of-linear-filters-5",
    "title": "Fourier Filtering",
    "section": "Fourier Analysis of Linear Filters",
    "text": "Fourier Analysis of Linear Filters\n\n\n\nExample 2: The Human Visual System and the Contrast Sensitivity Function\n\n\nWhen the input to a linear system is a wave of frequency \\(u_0\\) and amplitude 1, the output is another wave of the same frequency as the input but with an amplitude equal to \\(|H \\left[ u_0 \\right]|\\):\n\n\n\nOutput to an exponential wave.\n\n\nThis means that one way of identifying the transfer function of a system is by using as input a wave and measuring the output amplitude as a function of the input frequency \\(u_0\\). This inspired a generation of psychophysicists to study how the human visual system behaved when presented with periodic signals."
  },
  {
    "objectID": "lec4.html#fourier-analysis-of-linear-filters-6",
    "href": "lec4.html#fourier-analysis-of-linear-filters-6",
    "title": "Fourier Filtering",
    "section": "Fourier Analysis of Linear Filters",
    "text": "Fourier Analysis of Linear Filters\n\n\n\nExample 2: The Human Visual System and the Contrast Sensitivity Function\n\n\nTo experience the transfer function of our own visual system, let‚Äôs build the following \\(N \\times M\\) image: \\[\n\\ell\\left[n,m\\right] = A\\left[m\\right] \\sin(2 \\pi f\\left[n\\right] n/N)\n\\] with \\[\nA\\left[m\\right] = A_{min} \\left(\\frac{A_{max}}{A_{min}}\\right)^{m/M}\n\\] and \\[\nf\\left[n\\right] = f_{min} \\left(\\frac{f_{max}}{f_{min}}\\right)^{n/N}\n\\]"
  },
  {
    "objectID": "lec4.html#fourier-analysis-of-linear-filters-7",
    "href": "lec4.html#fourier-analysis-of-linear-filters-7",
    "title": "Fourier Filtering",
    "section": "Fourier Analysis of Linear Filters",
    "text": "Fourier Analysis of Linear Filters\n\n\n\nExample 2: The Human Visual System and the Contrast Sensitivity Function\n\n\n\nan amplitude \\(A\\left[m\\right]\\), varies along the vertical dimension \\(m\\);\na wave with a frequency, \\(f\\left[n\\right]\\), that varies along the horizontal component, \\(n\\);\nthe frequency function \\(f\\left[n\\right]\\), is defined as an increasing function that starts from \\(f_{min}=1\\) and grows up to \\(f_{max}=60\\).\n\n\n\n\n\n\n\nFigure¬†19: Contrast sensitivity function shown by the Campbell and Robson chart."
  },
  {
    "objectID": "lec4.html#fourier-analysis-of-linear-filters-8",
    "href": "lec4.html#fourier-analysis-of-linear-filters-8",
    "title": "Fourier Filtering",
    "section": "Fourier Analysis of Linear Filters",
    "text": "Fourier Analysis of Linear Filters\n\n\n\nExample 2: The Human Visual System and the Contrast Sensitivity Function\n\n\n\nour visual system is nonlinear: photo-receptors compute the \\(\\log\\) of the incoming intensity (approximately).\nacts like a band-pass filter: you are sensitive to middle spatial frequencies (peaking around 6 cycles/degree) and less sensitive to very low spatial frequencies (on the left of the image) and high spatial frequencies (on the right of the image).\n\n\n\n\nCSF (contrast sensitify function) example"
  },
  {
    "objectID": "lec4.html#concluding-remarks",
    "href": "lec4.html#concluding-remarks",
    "title": "Fourier Filtering",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\n\n\n\nFeatures\n\n\n\neasy to analyze images according to spatial frequency representation\nFT is too global! Every sinusoidal component covers the entire image.\nFT tells us a little about what is happening in the image (based on the spatial frequency content)\nFT tells us nothing about where it is happening."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Computer vision course"
  },
  {
    "objectID": "nb/pset2_2024/pset2_2024.html",
    "href": "nb/pset2_2024/pset2_2024.html",
    "title": "Problem 1",
    "section": "",
    "text": "%matplotlib inline\nimport numpy as np\nfrom numpy.fft import fft2, ifft2, fftshift, ifftshift\nfrom numpy import angle, real\nfrom numpy import exp, abs, pi, sqrt\nimport matplotlib.pyplot as plt\nimport cv2\nimport scipy.ndimage as ndimage\n\ndef imshow(im, cmap='gray'):\n    # clip image from 0-1\n    im = np.clip(im, 0, 1)\n    plt.imshow(im, cmap=cmap)\n! curl http://6.869.csail.mit.edu/sp21/pset3_data/einsteinandwho.jpg &gt; einsteinandwho.jpg\n! curl http://6.869.csail.mit.edu/sp21/pset3_data/bill.avi &gt; bill.avi\n### TODO: ENTER YOUR CODE BELOW"
  },
  {
    "objectID": "nb/pset2_2024/pset2_2024.html#problem-2",
    "href": "nb/pset2_2024/pset2_2024.html#problem-2",
    "title": "Problem 1",
    "section": "Problem 2",
    "text": "Problem 2\n\n# scale image's intensity to [0,1] with mean value of 0.5 for better visualization.\ndef intensityscale(raw_img):\n\n    # scale an image's intensity from [min, max] to [0, 1].\n    v_min, v_max = raw_img.min(), raw_img.max()\n    scaled_im = (raw_img * 1.0 - v_min) / (v_max - v_min)\n\n    # keep the mean to be 0.5.\n    meangray = np.mean(scaled_im)\n    scaled_im = scaled_im - meangray + 0.5\n\n    # clip to [0, 1]\n    scaled_im = np.clip(scaled_im, 0, 1)\n\n    return scaled_im\n\n\n### ENTER YOUR CODE BELOW"
  },
  {
    "objectID": "nb/pset2_2024/pset2_2024.html#problem-3.a",
    "href": "nb/pset2_2024/pset2_2024.html#problem-3.a",
    "title": "Problem 1",
    "section": "Problem 3.a",
    "text": "Problem 3.a\n\n# 9x9 images\nimSize = 9\n\n# we would like to magnify the change between im1 and im2 by 4x\nmagnificationFactor = 4;\n\n# horizontal movement from (0, 0) to (0, 1)\nim1 = np.zeros([imSize, imSize])\nim2 = np.zeros([imSize, imSize])\nim1[0,0] = 1\nim2[0,1] = 1\n\nff1 = fftshift(fft2(im1))\nff2 = fftshift(fft2(im2))\n\nplt.figure()\nplt.subplot(121)\nimshow(im1)\nplt.subplot(122)\nimshow(im2)\n\nplt.figure()\nplt.subplot(121)\nimshow(angle(ff1))\nplt.subplot(122)\nimshow(angle(ff2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMagnify Change\n\ndef magnifyChange(im1, im2, magnificationFactor):\n\n    # find phase shift in frequency domain\n    im1Dft = fft2(im1)\n    im2Dft = fft2(im2)\n    phaseShift = # TODO\n\n    # magnify the phase change in frequency domain\n    magnifiedDft = # TODO\n\n    # what does the magnified phase change cause in image space?\n    magnified = ifft2(magnifiedDft).real;\n\n    return magnified\n\nHINT: If you‚Äôre not familiar with complex number in python, here‚Äôs a quickstart.\n\n# create a complex number\nx = 1 + 1j\nprint(\"x =\", x)\nprint(\"x.real\", x.real, \"x.imag\", x.imag)\n\n# magnitude and phase of complex number\nmag = abs(x)\nphase = angle(x)\n\nprint(\"Magnitude\", mag)\nprint(\"Phase\", phase)\n\n# Euler's formula\ny = mag * exp(phase * 1j)\nprint(\"y =\", y)\n\n\n# magnify position change\nmagnified = magnifyChange(im1, im2, magnificationFactor);\n\nplt.figure(figsize=(12,36))\nplt.subplot(131)\nimshow(im1); plt.title('im1');\n\nplt.subplot(132)\nimshow(im2); plt.title('im2');\n\nplt.subplot(133)\nimshow(magnified); plt.title('magnified');\nplt.savefig(\"problem_3a.png\", bbox=\"tight\")"
  },
  {
    "objectID": "nb/pset2_2024/pset2_2024.html#problem-3.b",
    "href": "nb/pset2_2024/pset2_2024.html#problem-3.b",
    "title": "Problem 1",
    "section": "Problem 3.b",
    "text": "Problem 3.b\n\n# 9x9 images\nimSize = 9\n\n# we would like to magnify the change between im1 and im2 by 4x\nmagnificationFactor = 4\n\n# horizontal movement from (1, 1) to (1, 2)\n# additional vertical movement from (9, 9) to (8, 9)\nim1 = np.zeros([imSize, imSize])\nim2 = np.zeros([imSize, imSize])\nim1[0,0] = 1\nim2[0,1] = 1\nim1[8,8] = 1\nim2[7,8] = 1\n\n\n### TODO: ENTER YOUR CODE BELOW\n### manually edit the expected matrix (currently set as zeros) by creating 1s to show the expected output\nexpected = np.zeros([imSize, imSize])\n\n\n\n# magnify position change\nmagnified = magnifyChange(im1, im2, magnificationFactor)\n\n\nplt.figure(figsize=(12,36))\nplt.subplot(141)\nimshow(im1); plt.title('im1');\n\nplt.subplot(142)\nimshow(im2); plt.title('im2');\n\nplt.subplot(143)\nimshow(expected); plt.title('expected');\n\nplt.subplot(144)\nimshow(magnified); plt.title('magnified');\nplt.savefig(\"problem_3b.png\", bbox=\"tight\")"
  },
  {
    "objectID": "nb/pset2_2024/pset2_2024.html#problem-3.c",
    "href": "nb/pset2_2024/pset2_2024.html#problem-3.c",
    "title": "Problem 1",
    "section": "Problem 3.c",
    "text": "Problem 3.c\n\n# 9x9 images\nimSize = 9\n\n# we would like to magnify the change between im1 and im2 by 4x\nmagnificationFactor = 4\n\n# width of our Gaussian window\nsigma = 2\n\n# horizontal movement from (1, 1) to (1, 2)\n# additional vertical movement from (9, 9) to (8, 9)\nim1 = np.zeros([imSize, imSize])\nim2 = np.zeros([imSize, imSize])\nim1[0,0] = 1\nim2[0,1] = 1\nim1[8,8] = 1\nim2[7,8] = 1\n\n# we will magnify windows of the image and aggregate the results\nmagnified = np.zeros([imSize, imSize])\n\n# meshgrid for computing Gaussian window\nX, Y = np.meshgrid(np.arange(imSize), np.arange(imSize))\n\nfor y in range(0, imSize, 2*sigma):\n    for x in range(0, imSize, 2*sigma):\n        gaussianMask = # TODO\n        windowMagnified = magnifyChange(# TODO,\\\n            magnificationFactor)\n        magnified = magnified + windowMagnified\n\nplt.figure(figsize=(12,36))\nplt.subplot(131)\nimshow(im1); plt.title('im1');\n\nplt.subplot(132)\nimshow(im2); plt.title('im2');\n\nplt.subplot(133)\nimshow(magnified); plt.title('magnified');\nplt.savefig(\"problem_3c.png\", bbox=\"tight\")"
  },
  {
    "objectID": "nb/pset2_2024/pset2_2024.html#problem-3.d",
    "href": "nb/pset2_2024/pset2_2024.html#problem-3.d",
    "title": "Problem 1",
    "section": "Problem 3.d",
    "text": "Problem 3.d\n\nimport numpy as np\nimport cv2\n\ncap = cv2.VideoCapture('bill.avi')\n\n# list of video frames\nframes = []\n\nwhile(cap.isOpened()):\n    # read frame from the video\n    ret, frame = cap.read()\n\n    if ret is False:\n        break\n\n    frames.append(frame)\n\ncap.release()\n\n# scale frame to 0-1\nframes = np.array(frames) / 255.\nprint(\"frames size:\", frames.shape, \"# (nb_frames, height, width, channel)\")\n\n# get height, width\nnumFrames = frames.shape[0]\nheight = frames.shape[1]\nwidth = frames.shape[2]\n\n\nMotion magnification\nFill out code here\n\n# 10x magnification of motion\nmagnificationFactor = 10\n\n# width of Gaussian window\nsigma = 13\n\n# alpha for moving average\nalpha = 0.5\n\n# we will magnify windows of the video and aggregate the results\nmagnified = np.zeros_like(frames)\n\n# meshgrid for computing Gaussian window\nX, Y = np.meshgrid(np.arange(width), np.arange(height))\n\n# iterate over windows of the frames\nxRange = list(range(0, width, 2*sigma))\nyRange = list(range(0, height, 2*sigma))\nnumWindows = len(xRange) * len(yRange)\nwindowIndex = 1\n\nfor y in yRange:\n    for x in xRange:\n        for channelIndex in range(3): # RGB channels\n            for frameIndex in range(numFrames):\n\n                # create windowed frames\n                gaussianMask = # TODO\n                windowedFrames = gaussianMask * frames[frameIndex,:,:,channelIndex]\n\n                # initialize moving average of phase for current window/channel\n                if frameIndex == 0:\n                    windowAveragePhase = angle(fft2(windowedFrames))\n\n                windowDft = fft2(windowedFrames)\n\n                # compute phase shift and constrain to [-pi, pi] since\n                # angle space wraps around\n                windowPhaseShift = angle(windowDft) - windowAveragePhase\n                windowPhaseShift[windowPhaseShift &gt; pi] = windowPhaseShift[windowPhaseShift &gt; pi] - 2 * pi\n                windowPhaseShift[windowPhaseShift &lt; -pi] = windowPhaseShift[windowPhaseShift &lt; -pi] + 2 * pi\n\n                # magnify phase shift\n                windowMagnifiedPhase = # TODO\n\n                # go back to image space\n                windowMagnifiedDft = # TODO\n                windowMagnified = abs(ifft2(windowMagnifiedDft))\n\n                # update moving average\n                windowPhaseUnwrapped = windowAveragePhase + windowPhaseShift\n                windowAveragePhase = alpha * windowAveragePhase + (1 - alpha) * windowPhaseUnwrapped\n\n                # aggregate\n                magnified[frameIndex,:,:,channelIndex] = magnified[frameIndex,:,:,channelIndex] + windowMagnified\n\n        # print progress\n        print('{}/{}'.format(windowIndex, numWindows), end='\\r')\n        windowIndex += 1\n\n\noutputs = magnified / np.max(magnified)\nfor channelIndex in range(3):\n    originalFrame = frames[0,:,:,channelIndex]\n    magnifiedFrame = outputs[0,:,:,channelIndex]\n    scale = np.std(originalFrame[:]) / np.std(magnifiedFrame[:])\n    originalMean = np.mean(originalFrame[:])\n    magnifiedMean = np.mean(magnifiedFrame[:])\n    outputs[:,:,:,channelIndex] = magnifiedMean + scale * (outputs[:,:,:,channelIndex] - magnifiedMean)\n\noutputs = np.clip(outputs, 0, 1)\n\n\n# create output video\nfourcc = cv2.VideoWriter_fourcc('M','J','P','G')\n# fourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter('bill_magnified.avi',fourcc, 30.0, (height, width))\n\nfor i in range(frames.shape[0]):\n    # scale the frame back to 0-255\n    frame = (np.clip(outputs[i], 0, 1) * 255).astype(np.uint8)\n\n    # write frame to output video\n    out.write(frame)\n\nout.release()\n\n\n# Only for colab downloading videos\ntry:\n    from google.colab import files\n    files.download('bill_magnified.avi')\nexcept:\n    print(\"Only for google colab\")"
  }
]