---
title: "Filters and image derivatives"
author: 
  - name: Vitaly Vlasov
    affiliation: Lviv University
code-fold: false
execute:
  enabled: false
  cache: true
diagram:
  cache: true
  cache-dir: ./cache
  engine:
    tikz:
      execpath: lualatex
      additional-packages: |
        \usepackage{neuralnetwork}
        \usepackage{mathtools}
        \usepackage{amsmath}
        \pgfplotsset{compat=1.16}
        \usepackage{pgfplots}
        \newcommand\mybox[2][]{\tikz[overlay]\node[fill=blue!20,inner sep=2pt, anchor=text, rectangle, rounded corners=1mm,#1] {#2};\phantom{#2}}
        \usetikzlibrary{arrows.meta}
        \usetikzlibrary{positioning}
        \usetikzlibrary{shapes.misc}
        \usetikzlibrary{decorations.pathreplacing}
filters:
  - diagram
format: 
  revealjs:
    preview-links: auto
    slide-number: true
    theme: default
    multiplex:
      url: 'https://mplex.vitv.ly'
      secret: '701983790cb49f0305d621abfd1c06bf'
      id: '0623eaeb78cd62d74a293dd87ad65affd248201173509008578f7fd7bfa7ad87'
---

# Blur Filters {#sec-blur_filters}

## Introduction

:::{.callout-tip icon=false}
## Blur filters
Blur filters are low-pass filters

- remove the high spatial-frequency content
- leave only the low-frequency spatial components.

The result is an image that has lost details and that looks **blurry**. 
:::

:::{.callout-tip icon=false}
## Blur
Image blur has many applications in computer graphics and computer vision:

- it can be used to reduce noise
- to reveal image structures at different scales
- upsampling and downsampling images.
:::

## Blur filters

:::: {.columns}
::: {.column width="50%"}
![Original noisy image.](img/blur_filters/stop_256_noise_3.jpg){#fig-stop_256_noise_3-a}
:::

::: {.column width="50%"}
![Blurred image.](img/blur_filters/stop_256_blur_3.jpg){#fig-stop_256_noise_3-b}
:::
::::

## Blur filters
:::{.callout-note icon=false}
## Implementation
Blurring is implemented by computing local averages over small neighborhoods of input pixel values. This can be done by:

- convolution (**linear**)
- anisotropic diffusion (**non-linear**)
- bilateral filtering (**non-linear**).
:::


## Box Filter

:::{.callout-tip icon=false}
## Box filter - definition
Let's start with a very simple low-pass filter, the box filter. The box filter uses a box function as the convolution kernel. 
The box convolution kernel can be written as:

$$
\text{box}_{N,M} \left[n,m \right] = 
\begin{cases}
    1       & \quad \text{if } -N \leq n \leq N  \text{~and~}  -M \leq m \leq M\\
    0       & \quad \text{otherwise.} 
\end{cases}
$$
:::

:::{.callout-note icon=false}
The box filter with $N=M=1$ is:

![](img/blur_filters/box.png){width="90%"}
:::

## Box Filter
:::{.callout-tip icon=false}
## Filtering with a box filter
Filtering an input image, $\ell_{\text{in}}$, with a box filter results in the following:
$$\begin{aligned}
\ell_{\text{out}} \left[n,m\right] &= 
\text{box}_{N,M} \left[n,m\right] \circ \ell_{\text{in}} \left[n,m\right]  \\
&= \sum_{k,l} \ell_{\text{in}} \left[n-k,m-l \right] \text{box}_{N,M} \left[k,l \right] \\
&= \sum_{k=-N}^N \sum_{l=-M}^M \ell_{\text{in}} \left[n-k,m-l \right]
\end{aligned}$$

That is, the output value on each location $(n,m)$ is the sum of the input pixels within the rectangle around that location.  
:::


## Box Filter
:::{.callout-tip icon=false}
## Visual examples

![Fig (a) Input image. (b) Blurring with a square, (c) a horizontal, and (d) a vertical line. Each color channel is filtered independently.](img/blur_filters/convexamps.png){#fig-convExamps2}
:::


## Box Filter
:::{.callout-tip icon=false}
## Properties

1. Low-pass: it attenuates the high spatial-frequency content of the input image. 

2. 2D box filter is separable and can be written as the convolution of two 1D kernels: 
   $$
   \text{box}_{N,M} \left[n,m \right]  = \text{box}_{N,0} \circ \text{box}_{0,M}
   $$.

3. **DC gain**: the gain it has for a constant value input (i.e., the lowest possible input frequency). If the input signal is a constant, i.e., $\ell[n,m] = a$, where $a$ is a real number, the result of convolving the image with the box filter $h_{N,M}$ is also a constant:

$$
\ell_{\text{out}} \left[n,m\right] = \sum_{k,l} a \text{box}_{N,M} \left[k,l \right] = a \sum_{k,l} \text{box}_{N,M} \left[k,l \right]  = a (2N+1)(2M+1)
$$
:::

:::{.callout-tip icon=false}
Remember that the DC value of a signal is its mean value. DC is an old name derived from Direct Current.

In general, the DC gain of an arbitrary filter $h [n,m]$ is the sum of its kernel values:

$$
\text{DC~gain} = \sum_{n,m} h [n,m]
$$
:::

## Box Filter
:::{.callout-tip icon=false}
## DC gain
In the example of the box filter with $N=1$, the DC gain is 3. 

In the frequency domain, the DC gain of a filter refers to its gain at a frequency of 0, represented by the value $H[0,0]$. The value of $\left| \text{Box}_1[0] \right|$ is $3$. The DC gain of a filter will change the mean value of the input signal. 
:::

## Box Filter
:::{.callout-tip icon=false}
## DC gain

:::: {.columns}
::: {.column width="50%"}
![1D Box Filter.](img/blur_filters/boxfilter_1d.png){#fig-boxfilter-a}
:::

::: {.column width="50%"}
![Fourier transform of Box Filter.](img/blur_filters/boxfilter_fourier.png){#fig-boxfilter-b}
:::
::::

Fig (a) A one-dimensional (1D) box filter ($\left[1,1,1\right]$), and (b) its Fourier transform over 20 samples. Note that the frequency gain is not monotonically decreasing with spatial frequency.
:::

## Box Filter
:::{.callout-tip icon=false}
## DC gain
When designing blur kernels (low-pass filters), we generally want to have a DC gain of 1. The reason is that if we have an image with grayscale levels in the range of 0 to 256 and with an average around 128, we will want to preserve the same mean value in the output. For this reason, in most applications, we will normalize the kernel values so that they sum 1. In the case of the box filter, this means dividing the kernel values by $(2N+1)(2M+1)$. 
:::

## Box Filter
:::{.callout-tip icon=false}
## Limitations

- **The box filter is not a perfect blurring filter.** A blur filter should attenuate high spatial frequencies with stronger attenuation for higher spatial frequencies. However, if you consider the highest spatial frequency, which will be an oscillating signal that takes successively on the values 1 and $-1$: $\left[..., 1, -1, 1, -1, 1, -1, ... \right]$ when filtered with the box filter $\text{box}_{1}$ the result is the same oscillating signal! However, if you filter a wave with lower frequency such as $\left[..., 0.5, 0.5, -1, 0.5, 0.5, -1, ... \right]$ then the result is $\left[..., 0,0,0,0, ...\right]$. Therefore, the attenuation is not monotonic with spatial frequency. This is not a desirable behavior for a blurring filter, and it can cause artifacts to appear. This could be addressed using an even size box filter $\left[1,1 \right]$. However, an even box filter is not centered around the origin and the output image will have a half-pixel translation. Therefore, odd filter sizes are preferred.
:::

## Box Filter
:::{.callout-tip icon=false}
## Limitations

The oscillating signal:

![](img/blur_filters/osci.png){width="80%"}

convolved with the kernel $[1,1,1]$ results in:

![](img/blur_filters/kern.png){width="80%"}
:::

## Box Filter
:::{.callout-tip icon=false}
## Limitations

- **If you convolve two boxes, you do not get another box.** Instead, you get a triangle. You can easily check this by convolving two box filters. For instance, in the simple case where $N=1, M=0$:

$$
\text{box}_{1,0} \circ \text{box}_{1,0} = \left[1, 1, 1\right] \circ \left[1, 1, 1\right] = \left[1,2,3,2,1\right]
$$

The output is a **triangular filter** \index{Filter!Triangular filter} with length $2 \times (L-1)$, with $L=3$ the length of the box filter. When convolving two box filters of different lengths, the result will be a truncated triangle. Although that is not a problem at first sight, it means that if you blur an image twice with box filters, what you get is not the equivalent to blurring only once with a larger box filter.

:::

## Gaussian Filter {#sec-spt_gaussian}
:::{.callout-tip icon=false}
## Definition

One of the important blurring (low-pass) filters in computer vision is the Gaussian filter. 
The Gaussian filter is important because it is a good model for many naturally occurring filters. It also has several properties, as we will discuss here, that make it unique. 

The Gaussian distribution is defined in continuous variables. In one dimension:
\index{Filter!Gaussian filter}

$$
g(x; \sigma) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp{ \left( -\frac{x^2}{2 \sigma^2} \right) }
$$ {#eq-gauss1dcont}

and in two dimensions:

$$
g(x,y; \sigma) = \frac{1}{2 \pi \sigma^2} \exp{ \left(-\frac{x^2 + y^2}{2 \sigma^2} \right) }
$$ {#eq-gauss2dcont}

The parameter $\sigma$ adjusts the spatial extent of the Gaussian. The normalization constant is set so that the function integrates to 1. The Gaussian kernel is positive and symmetric (a zero-phase filter).
:::


## Gaussian Filter
:::{.callout-tip icon=false}
## Discretization

In order to use this filter in practice, we need to consider discrete locations and also approximate the function by a finite support function. In practice, we only need to consider samples within three standard deviations $x \in (-3\sigma, 3\sigma)$. At $3\sigma$, the amplitude of the Gaussian filter is around 1 percent of its central value. Unfortunately, many of the properties of the Gaussian filter that we will discuss later are only true in the continuous domain and are only approximated when using its discrete form. 

For a given standard deviation parameter, $\sigma$, the discretized Gaussian kernel is $g \left[n, m; \sigma \right]$:
\index{Filter!Gaussian filter!Discrete}

$$
g \left[ n,m; \sigma \right] = \exp{ \left( -\frac{n^2 + m^2}{2 \sigma^2} \right) }
$$ {#eq-gauss2d}
:::


## Gaussian Filter
:::{.callout-tip icon=false}
## Example
1D Gaussian with $\sigma=1$ and its discretized version:

![](img/blur_filters/gaus.png){width="90%"}
:::

<!-- We have removed the normalization constant as the sum of the discrete Gaussian will be different from the integral of the continuous function. So here we prefer to define the form in which the value at the origin is 1. In practice, we should normalize the discrete Gaussian by the sum of its values to make sure that the DC gain is 1. -->



## Gaussian Filter
:::{.callout-tip icon=false}
## Adjustment
By adjusting the standard deviation, $\sigma$, of the Gaussian, it is possible to adjust the level of image detail
that appears in the blurred image. 

:::: {.columns}
::: {.column width="33%"}
![Gaussian filter with $\sigma=2$.](img/spatial_filters/gausian_zebra_c_2.jpg){#fig-zebragaussian-a}
:::

::: {.column width="33%"}
![Gaussian filter with $\sigma=4$.](img/spatial_filters/gausian_zebra_c_4.jpg){#fig-zebragaussian-b}
:::

::: {.column width="33%"}
![Gaussian filter with $\sigma=8$.](img/spatial_filters/gausian_zebra_c_8.jpg){#fig-zebragaussian-c}
:::
::::

An image filtered with three Gaussians with standard deviations: (a) $\sigma=2$, (b) $\sigma=4$, and (c) $\sigma=8$. Plots (d--f) show the three Gaussians over the same spatial support as the image. The discrete Gaussians are approximated by sampling the continuous Gaussian. The convolutions are performed with mirror boundary conditions.
:::

## Gaussian Filter

:::{.callout-tip icon=false}
## Multiple dimensions
$$
\ell_{\texttt{out}}\left[n,m\right] = h \left[n,m\right] \circ \ell_{\texttt{in}}\left[n,m\right] =  \sum_{k,l}h \left[n-k,m-l \right] \ell_{\texttt{in}}\left[k,l \right]
$${#eq-2dconv} 
:
The $n$-dimensional Gaussian filter has the additional computational advantage that it can be applied as a concatenation of $n$ 1D Gaussian filters. This can be seen by writing the 2D Gaussian, @eq-gauss2dcont, in the convolution equation, @eq-2dconv. Letting $g^x$ and $g^y$ be the 1D Gaussian convolution kernels in the horizontal and vertical directions (i.e., $g^x [n]=g[n,0]$, and $g^y[m]=g[0,m]$), we have:


$$
g \left[n,m \right] \circ \ell \left[n,m\right] 
= \sum_{k,l} g \left[n-k,m-l \right] \ell \left[k,l \right] 
= \sum_{k,l} \exp{ \left( -\frac{(n-k)^2 + (m-l)^2}{2 \sigma^2} \right) } \circ \ell \left[n,m \right]
$$

$$
= \sum_{k} \exp{ \left( -\frac{(n-k)^2}{2 \sigma^2} \right) }
\left( \sum_{l} \exp{ \left( -\frac{(m-l)^2}{2 \sigma^2} \right) } \ell \left[k,l \right] \right)
= g^x \circ (g^y \circ \ell \left[n,m \right])
$$ {#eq-2dgauss}
:::

This can save quite a bit in computation time when applying the convolution of @eq-2dgauss. If the 2D convolution kernel is $N \times N$ samples, then a direct convolution of that 2D kernel scales in proportion to $N^2$, since @eq-2dgauss requires one multiplication per image position per kernel sample. Using the cascade of two 1D kernels, resulting in an equivalent 2D filter of the same size, scales in proportion to $2N$.

## Gaussian Filter
:::{.callout-tip icon=false}
## Removing distractions
Another application of blurring is to remove distracting high-resolution image details. Figure shows a Gaussian low-pass filter applied to remove unwanted image details (the blocky artifacts) from an image [@Harmon_1973].

:::: {.columns}
::: {.column width="50%"}
![Original Image.](img/blur_filters/Jules_Lincoln_1971.jpg){#fig-lincoln-a}
:::

::: {.column width="50%"}
![Blurred Image.](img/blur_filters/Jules_Lincoln_1971_blur.jpg){#fig-lincoln-b}
:::
::::


(Left) Input image. (Right) Blurred version. The left version has many spurious details introduced by the blocky style of the image. The right image has been blurred by a large Gaussian filter. **Source**: Image by Bela Julesz and Leon Harmon, 1971 [@Harmon_1973].
:::

## Gaussian Filter
:::{.callout-tip icon=false}
## Properties of the Continuous Gaussian

Here are some key properties of the Gaussian filter:

- **The $n$-dimensional Gaussian is the only completely circularly symmetric operator that is separable.**
- **The continuous Fourier transform (FT) of a Gaussian is also a Gaussian.** For a 1D Gaussian, its Fourier transform is:

$$
G (w; \sigma) = \exp{ \left( -\frac{w^2 \sigma^2} {2} \right) }
$$

and in 2D the Fourier transform is:

$$
G (w_x, w_y; \sigma) = \exp{ \left(- \frac{(w_x^2+w_y^2) \sigma^2} {2} \right) }
$$ {#eq-FTgauss2d}

Note that this function is monotonically decreasing in magnitude for increasing frequencies, and it is also radially symmetric. 
:::

## Gaussian Filter
:::{.callout-tip icon=false}
## Properties of the Continuous Gaussian
- **The width of the Gaussian Fourier transform decreases with $\sigma$** (this is the opposite behavior to the Gaussian in the spatial domain).
- **The convolution of two $n$-dimensional Gaussians is an $n$-dimensional Gaussian.**

$$
g (x,y; \sigma_1 ) \circ g (x,y; \sigma_2)  = g (x,y; \sigma_3)
$$

where the variance of the result is the sum $\sigma_3^2 = \sigma_1^2 + \sigma_2^2$. This is a remarkable property of Gaussian filters and is the basis of the Gaussian pyramid that we will see later. To prove this property, one can use the Fourier transform of the Gaussian and the fact that the convolution is the product of Fourier transforms.

- **The Gaussian is the solution to the heat equation.**
- **Repeated convolutions of any function concentrated in the origin result in a Gaussian (central limit theorem).**
- **In the limit $\sigma \rightarrow 0$ the Gaussian becomes an impulse.** This property is shared by many other functions, but it is a useful thing to know.
:::

## Gaussian Filter
:::{.callout-tip icon=false}
## Limitations

However, many of these properties are only true for the continuous version of the Gaussian and do not work for its discrete approximation $g\left[n,m;\sigma \right]$ obtained by directly sampling the values of the Gaussian at discrete locations. To see this, let's look at one example in 1D. Let's consider a Gaussian with variance $\sigma^2=1/2$. It can be approximated by five samples. We will call this approximation $g_5$ and it takes the values:

$$
g_5\left[ n \right] = \left[0.0183, \,    0.3679, \,    1.0000, \,    0.3679, \,    0.0183 \right] 
$$

You can check that if you compute the approximation for $\sigma^2=1$ by discretizing the Gaussian, the result obtained is not equal to doing $g_5 \circ g_5$. Therefore, as you apply successive convolutions of discretized Gaussians the errors will accumulate. That is, the convolution of discretized Gaussians is not a Gaussian anymore. 

Note that the convolution of $g_5\left[ n \right]$ with the wave $\left[1,-1,1,-1,...\right]$ is not zero. This is to be expected from the form of the FT of the Gaussian. This is not a strong limitation, and in many applications, it does not matter. However, in some cases, it is important to completely cancel the highest frequencies (like when applying antialiasing filters as we will see later).

The next low-pass filter addresses the limitations of the box and Gaussian filters.
:::


## Binomial Filters
:::{.callout-tip icon=false}
## Definition

In practice, there are very efficient discrete approximations to the Gaussian filter that, for certain $\sigma$ values, have nicer properties than when working with discretized Gaussians. One common approximation of the Gaussian filter is to use **binomial coefficients** [@Chehikian91]. Binomial filters are obtained by successive convolutions of the box filter $\left[1,1\right]$.

The binomial coefficients use the central limit theorem to approximate a Gaussian as successive convolutions of a very simple function. 
The binomial coefficients form the **Pascal's triangle** as shown in figure.

$$
\begin{array}{*{20}{>{\centering\arraybackslash}p{.2cm}}{>{\arraybackslash}p{1.5cm}}}
b_0 &   ~ &  ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & 1 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~  & ~ & \sigma_0^2=0\\
b_1 &    ~ &  ~ & ~ & ~ & ~ & ~ & ~ & ~ & 1 & ~ & 1 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & \sigma_1^2=1/4\\
b_2 &   ~ &  ~ & ~ & ~ & ~ & ~ & ~ & 1 & ~ & 2 & ~ & 1 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & \sigma_2^2=1/2\\
b_3 &  ~ &  ~ & ~ & ~ & ~ & ~ & 1 & ~ & 3 & ~ & 3 & ~ & 1 & ~ & ~ & ~ & ~ & ~ & ~ & \sigma_3^2=3/4\\
b_4 &  ~ &  ~ & ~ & ~ & ~ & 1 & ~ & 4 & ~ &  6 & ~  & 4 & ~ & 1 & ~ & ~ & ~ & ~ & ~ & \sigma_4^2=1\\
b_5 &  ~ &  ~ & ~  & ~ & 1 & ~   & 5   & ~   &10  &  ~  & 10 & ~   & 5   & ~   & 1 & ~ & ~ & ~ & ~ & \sigma_5^2=5/4\\
b_6 &  ~ &  ~ & ~  & 1 & ~ & 6   & ~   & 15 & ~   & 20 & ~   & 15 & ~   & 6   & ~ & 1 & ~ & ~ & ~ & \sigma_6^2=3/2\\
b_7 &  ~ &  ~ &  1 & ~ & 7 & ~   & 21 & ~   & 35 & ~   & 35 & ~   & 21 & ~   & 7 & ~ & 1 & ~ & ~ & \sigma_7^2=7/4\\
b_8 &  ~ &  1 & ~  & 8 & ~ & 28 & ~   & 56 & ~   & 70 & ~   & 56 & ~   & 28 & ~ & 8 & ~ & 1 & ~ & \sigma_8^2=2
\end{array}
$$

Binomial coefficients. To build the Pascal's triangle, each number is the sum of the number above to the left and the one above to the right.
:::

<!-- Each row of @fig-pascaltriangle shows one 1D-binomial kernel. The first binomial kernel, $b_0$, is the impulse. -->

## Binomial Filters
:::{.callout-tip icon=false}
## Properties

- **Binomial coefficients provide a compact approximation of the Gaussian coefficients using only integers.** Note that the values of $b_2$ are different from $g_5$ despite that both will be used as approximations to a Gaussian with the same variance $\sigma^2 = 1/2$. The thing to note is that the variance of $g_5$ is not really $\sigma^2 = 1/2$ despite being obtained by discretizing a Gaussian with that variance.

- **The sum of all the coefficients (DC gain) for each binomial filter $b_n$ is $2^n$, and their spatial variance is $\sigma^2 = n/4$.**

- **One remarkable property of the binomial filters is that $b_n \circ b_m = b_{n+m}$, and, therefore, $\sigma_n^2 + \sigma_m^2  = \sigma_{n+m}^2$, which is analogous to the Gaussian property in the continuous domain.** That is, the convolution of two binomial filters is another binomial filter. 
:::

## Binomial Filters
:::{.callout-tip icon=false}
## Properties
- **The simplest approximation to the Gaussian filter is the 3-tap binomial kernel:**

$$
b_2 = \left[1, 2, 1\right] 
$$

This filter is interesting because it is even (so it can be applied to an image without producing any translation) and its discrete Fourier transform (DFT) is:

$$
B_2 \left[u\right] = 2+2 \cos (2 \pi u/N)
$$
:::


## Binomial Filters
:::{.callout-tip icon=false}
## Properties
The gain decreases monotonically (there are no ripples) with spatial frequency, $u$, and it becomes zero at the highest frequency, $G_3 \left[ 10 \right]=0$.

:::: {.columns}
::: {.column width="50%"}
![1D Binomial Filter.](img/blur_filters/gauss3filter_1d.png){#fig-gauss3filter-a}
:::


::: {.column width="50%"}
![Fourier Transform of Binomial Filter.](img/blur_filters/gauss3filter_fourier.png){#fig-gauss3filter-b}
:::
::::

Fig (a) A one-dimensional three-tap approximation to the Gaussian filter ($\left[1,2,1\right]$) and, (b) its Fourier transform for $N=20$ samples.
:::

## Binomial Filters
:::{.callout-tip icon=false}
## Properties
- **All the even binomial filters can be written as successive convolutions with the kernel $\left[1,2,1\right]$.** Therefore, their Fourier transform is a power of the Fourier transform of the filter $\left[1,2,1\right]$ and therefore they are also monotonic:

$$
B_{2n} \left[u\right] = (2+2 \cos (2 \pi u/N))^n
$$

The filter transfer function, $B_{2n}$, is real and positive. It is a **zero-phase filter**. \index{Filter!zero-phase}

- **For all the binomial filters $b_n$, when they are convolved with the wave $\left[1,-1,1,-1,...\right]$, the result is the zero signal $\left[0,0,0,0,...\right]$.** This is a very nice property of binomial filters and will become very useful later when talking about downsampling an image.
:::


## Binomial Filters
:::{.callout-tip icon=false}
## 2D Binomial Filters

The Gaussian in 2D can be approximated, using separability, as the convolution of two binomial filters, one vertical and another horizontal. For instance:

$$
b_{2,2} = b_{2,0} \circ b_{0,2} =  \begin{bmatrix}
  1 & 2 & 1 \\
\end{bmatrix}\circ \begin{bmatrix}
  1 \\
  2 \\
  1
\end{bmatrix}=
\begin{bmatrix}
  1 & 2 & 1 \\
  2 & 4 & 2\\
  1 & 2 & 1
\end{bmatrix}
$$

The filter $b_{2,2}$ has a DC gain of 16, so we will divide the convolution output by 16 so that the output image has a similar contrast to the input image. 
:::

## Binomial Filters
:::{.callout-tip icon=false}
## 2D binomial filter:

![](img/blur_filters/bin2.png){width="90%"}
:::


## Binomial Filters

:::{.callout-tip icon=false}
## Example
:::: {.columns}
::: {.column width="33%"}
![Noisy Image.](img/blur_filters/boat_b_noise.jpg){#fig-boat_b_noise-a}
:::

::: {.column width="33%"}
![Box Filtered Image.](img/blur_filters/boat_c_box.jpg){#fig-boat_b_noise-b}
:::

::: {.column width="33%"}
![Binomial Filtered Image.](img/blur_filters/boat_d_binomial.jpg){#fig-boat_b_noise-c}
:::
::::

Image corrupted by a checkerboard-pattern noise (right) and its output to two different blur kernels: (middle) $3 \times 3$ box filter. (right) Binomial filter $b_{2,2}$.
:::

## Concluding Remarks

:::{.callout-note}

- the Gaussian and the binomial filters are widely used in computer vision.
- the binomial filter $[1,2,1]/4$ (here normalized so that its DC gain is 1), and its 2D extension, are very useful kernels that you can use in many situations like when you need to remove high-frequency noise, or downsample an image by a factor of 2.
- Blur kernels are useful when building image pyramids, or in neural networks when performing different pooling operations or when resizing feature maps.
:::

::: aside
Keep the 2D binomial filter close to you:

$$
\begin{bmatrix}
  1 & 2 & 1 \\
  2 & 4 & 2\\
  1 & 2 & 1
\end{bmatrix}/16
$$
:::


# Image Derivatives {#sec-image_derivatives}

## Introduction

:::{.callout-tip icon=false}
## Derivatives
Computing image derivatives is an essential operator for extracting useful information from images:

- boundaries computation
- figuring out where changes are happening in the image.

The derivative operator is linear and translation invariant. Therefore, it can be written as a convolution. 
:::

## Image Derivatives
:::{.callout-note icon=false}
## Linearity
The operator that computes the image derivatives along the spatial dimensions ($x$ and $y$) is a linear system:

![Computing image derivatives along $x$- and $y$-dimensions.](img/derivatives/genericfilterH.png){#fig-genericfilterH width="60%"}
:::

## Image Derivatives

:::{.callout-tip icon=false}
## Examples
The following images show an input image and the resulting derivatives along the horizontal and vertical dimensions using one of the discrete approximations that we will discuss in this chapter.

:::: {.columns}
::: {.column width="33%"}
![](img/derivatives/mit_der_a.jpg){#fig-derivativesmit-a}
:::

::: {.column width="33%"}
![](img/derivatives/mit_der_b.jpg){#fig-derivativesmit-b}
:::

::: {.column width="33%"}
![](img/derivatives/mit_der_c.jpg){#fig-derivativesmit-c}
:::
::::

Image derivatives. (left) Input image, (middle) its $x$-derivative, and (right) its $y$-derivative.
:::


## Image Derivatives
:::{.callout-tip icon=false}
## Discretizing

If we had access to the continuous image, then image derivatives could be computed as: $\partial \ell (x,y) / \partial x$, which is defined as:

$$
\frac{\partial \ell(x,y)} {\partial x} = \lim_{\epsilon \to 0} \frac{ \ell(x+\epsilon,y) -\ell(x,y)} {\epsilon}
$$

However, there are several reasons why we might not be able to apply this definition:

- We only have access to a sampled version of the input image, $\ell \left[n,m\right]$, and we cannot compute the limit when $\epsilon$ goes to zero.
- The image could contain many non-derivable points, and the gradient would not be defined. We will see how to address this issue later when we study Gaussian derivatives.
- In the presence of noise, the image derivative might not be meaningful as it might just be dominated by the noise and not by the image content.
:::

## Image Derivatives
:::{.callout-tip icon=false}
## Approximation

For now, let's focus on the problem of approximating the continuous derivative with discrete operators. As the derivative is a linear operator, it can be approximated by a discrete linear filter. There are several ways in which image derivatives can be approximated.

Let's start with a simple approximation to the derivative operator that we have already played with $d_0  = \left[1, -1 \right]$. In one dimension (1D), convolving a signal $\ell \left[n \right]$ with this filter results in:
$$
\ell \circ d_0 = \ell \left[n \right] - \ell \left[n-1 \right]
$$
This approximates the derivative by the difference between consecutive values, which is obtained when $\epsilon=1$. @fig-discretederivative (c) shows the result of filtering a 1D signal (@fig-discretederivative [a]) convolved with $d_0 \left[n\right]$ (@fig-discretederivative [b]). The output is zero wherever the input signal is constant, and it is large in the places where there are variations in the input values. However, note that the output is not perfectly aligned with the input. In fact, there is half a sample displacement to the right. This is due to the fact that $d_0 \left[n\right]$ is not centered around the origin.
:::


## Image Derivatives
:::{.callout-tip icon=false}
## Approximation
This can be addressed with a different approximation to the spatial derivative $d_1  = \left[1, 0, -1 \right]/2$. In one dimension, convolving a signal $\ell \left[n \right]$ with $d_1 \left[n\right]$ results in:
$$
\ell \circ d_1 = \frac{\ell \left[n+1 \right] - \ell \left[n-1 \right]}{2}
$$

@fig-discretederivative (e) shows the result of filtering the 1D signal (@fig-discretederivative [a]) convolved with $d_1 \left[n\right]$ (@fig-discretederivative [d]). Now the output shows the highest magnitude output in the midpoint where there is variation in the input signal.

![(a) Input signal, $\ell [n]$. (b) Convolutional kernel $d_0 [n]$, defined as $d_0 [0]=1$ and $d_0 [1]=-1$ and zero everywhere else. (c) Output of the convolution between $\ell [n]$ and $d_0 [n]$. (d) Kernel $d_1 [n]$, defined as $d_1 [-1]=1$ and $d_1 [1]=-1$ and zero everywhere else. (e) Output of the convolution between $\ell [n]$ and $d_1 [n]$.](img/derivatives/discretederivative.png){width=70% #fig-discretederivative}
:::

## Image Derivatives
:::{.callout-tip icon=false}
## Approximation
It is also interesting to see the behavior of the derivative and its discrete approximations in the Fourier domain. In the continuous domain, the relationship between the Fourier transform on a function and the Fourier transform of its derivative is:
$$
\frac{\partial \ell (x)}{\partial x}  
\xrightarrow{\mathscr{F}} 
j w \mathscr{L} (w)
$$
In the continuous Fourier domain, derivation is equivalent to multiplying by $jw$. In the discrete domain, the DFT of a signal derivative will depend on the approximation used.
:::


## Image Derivatives
:::{.callout-tip icon=false}
## DFT of the approximation

Let's study now the DFT of the two approximations that we have discussed here, $d_0$ and $d_1$.

The DFT of $d_0 \left[n\right]$ is:
$$
\begin{split}
D_0 \left[u \right] & = 1 - \exp \left( -2 \pi j \frac{u}{N} \right)  \\
& = \exp \left( - \pi j \frac{u}{N} \right) \left(  \exp \left( \pi j \frac{u}{N} \right) - \exp \left( -\pi j \frac{u}{N} \right)  \right) \\
& = \exp \left( - \pi j \frac{u}{N} \right) 2 j \sin (\pi u /N)
\end{split}
$$
The first term is a pure phase shift, and it is responsible for the half a sample delay in the output. The second term is the amplitude gain, and it can be approximated by a linear dependency on $u$ for small $u$ values.

The DFT of $d_1 \left[n\right]$ is:
$$
\begin{split}
D_1 \left[u \right] & =  1/2\exp \left( 2 \pi j \frac{u}{N} \right) - 1/2 \exp \left( -2 \pi j \frac{u}{N} \right)  \\
& =  j \sin (2 \pi u /N)
\end{split}
$$
:::

## Image Derivatives
:::{.callout-tip icon=false}
## DFT of the approximation
@fig-d0andd1_dft shows the magnitude of $D_0\left[u \right]$ and $D_1\left[u \right]$ and compares it with $\left| 2 \pi u/N \right|$, which will be the ideal approximation to the derivative. The amplitude of $D_0\left[u \right]$ provides a better approximation to the ideal derivative, but the phase of $D_0\left[u \right]$ introduces a small shift in the output. Conversely, $D_1\left[u \right]$ has no shift, but it approximates the derivative over a smaller range of frequencies. The output to $D_1\left[u \right]$ is smoother than the output to $D_0\left[u \right]$, and, in particular, $D_1\left[u \right]$ gives a zero output when the input is the signal $\left[ 1, -1, 1, -1, ... \right]$. In fact, we can see that $\left[1,0,-1\right] = \left[1,-1\right] \circ \left[1,1\right]$, and, therefore $D_1\left[u \right] = D_0\left[u \right] B_1\left[u \right]$, where $B_1\left[u \right]$ is the DFT of the binomial filter $b_1 \left[n \right]$.

![Magnitude of (a) $D_0\left[u \right]$ and (b) $D_1\left[u \right]$ and comparison with $\left| 2 \pi u/N \right|$, shown as a thin black line. Both DFTs are computed over 20 samples.](img/derivatives/d0andd1_dft.png){width=90% #fig-d0andd1_dft}
:::


## Image Derivatives
:::{.callout-tip icon=false}
## DFT of the approximation
When working with two-dimensional (2D) images, there are several ways in which partial image derivatives can be approximated. For instance, we can compute derivatives along

 the $n$ and $m$ components:

$$
\begin{bmatrix}
  1 \\
  -1
\end{bmatrix}

\begin{bmatrix}
  1 & -1
\end{bmatrix}
$$
:::


## Image Derivatives

:::{.callout-tip icon=false}
## DFT of the approximation
The problem with this discretization of the image derivatives is that the outputs are spatially misaligned because a filter of length 2 shifts the output image by half a pixel as we discussed earlier. The spatial misalignment can be removed by using a slightly different version of the same operator: we can use a rotated reference frame as it is done in the **Roberts cross** operator, introduced in 1963 @Roberts63 in a time when reading an image of $256 \times 256$ pixels into memory took several minutes:

$$
\begin{bmatrix}
  1 & ~0\\
  0 & -1
\end{bmatrix}

\begin{bmatrix}
  ~0 & 1 \\
  -1 & 0
\end{bmatrix}
$$
Now, both outputs are spatially aligned because they are shifted in the same way. Another discretization is the Sobel operator, based on the $[-1,0,1]$ kernel, and will be discussed in @sec-derivatives_binomial_filters.

While newer algorithms for edge extraction have been developed since the creation of these operators, they still hold value in cases where efficiency is important. These simple operators continue to be used as fundamental components in modern computer vision descriptors, including the Scale-invariant feature transform (SIFT) @Lowe04 and the histogram of oriented gradients (HOG) @Dalal2005.
:::

## Gradient-Based Image Representation
:::{.callout-tip icon=false}
## Recovering original image

Derivatives have become an important tool to represent images, and they can be used to extract a great deal of information from the image as it was shown in the previous chapter. One thing about derivatives is that it might seem as though we are losing information from the input image. An important question is if we have the derivative of a signal, can we recover the original image? What information is being lost? Intuitively, we should be able to recover the input image by integrating its derivative, but it is an interesting exercise to look in detail at how this integration can be performed. We will start with a 1D signal, and then we will discuss the 2D case.

A simple way of seeing that we can recover the input from its derivatives is to write the derivative in matrix form. This is the matrix that corresponds to the convolution with the kernel $\left[1, -1 \right]$ that we will call $\mathbf{D_0}$. The next two matrices show the matrix $\mathbf{D_0}$ and its inverse $\mathbf{D_0}^{-1}$ for a 1D image of length five pixels using zero boundary conditions:

$$
\mathbf{D_0} = 
\begin{bmatrix}
  1 ~& 0 ~& 0 ~& 0~& 0 \\
  -1 ~& 1 ~& 0 ~& 0~& 0 \\
  0 ~& -1 ~& 1 ~& 0 ~& 0\\
  0~& 0 ~& -1 ~& 1 ~& 0\\
  0~& 0 ~& 0 ~& -1 ~& 1
\end{bmatrix}

\mathbf{D_0}^{-1} = 
\begin{bmatrix}
  1 ~&~ 0 ~&~ 0 ~&~ 0~&~ 0 \\
  1 ~&~ 1 ~&~ 0 ~&~ 0~&~ 0 \\
  1 ~&~ 1 ~&~ 1 ~&~ 0 ~&~ 0\\
  1~&~ 1 ~&~ 1 ~&~ 1 ~&~ 0\\
  1~&~ 1 ~&~ 1 ~&~ 1 ~&~ 1
\end{bmatrix}
$$
We can see that the inverse $\mathbf{D_0}^{-1}$ is reconstructing each pixel as a sum of all the derivative values from the left-most pixel to the right. And the inverse perfectly reconstructs the input. But, this is cheating because the first sample of the derivative gets to see the actual value of the input signal, and then we can integrate back the entire signal. That matrix is assuming zero boundary conditions for the signal, and the boundary gives us the needed constraint to be able to integrate back the input signal.
:::

## Gradient-Based Image Representation
:::{.callout-tip icon=false}
## Recovering original image

But what happens if you only get to see valid differences and you remove any pixel that was affected by the boundary? In this case, the derivative operator in matrix form is:

$$
\mathbf{D_0} = 
\begin{bmatrix}
  -1 ~& 1 ~& 0 ~& 0~& 0 \\
  0 ~& -1 ~& 1 ~& 0 ~& 0\\
  0~& 0 ~& -1 ~& 1 ~& 0\\
  0~& 0 ~& 0 ~& -1 ~& 1
\end{bmatrix}
$$
:::


## Gradient-Based Image Representation
:::{.callout-tip icon=false}
## Recovering original image
Let's consider the next 1D input signal:

$$
\boldsymbol\ell = \left[1, 1, 2, 2, 0\right]
$$

Then, the output of the derivative operator is:

$$
\mathbf{r}=\mathbf{D_0} \boldsymbol\ell=\left[0, -1, 0, 2\right]
$$
Note that this vector has one sample less than the input. To recover the input $\boldsymbol\ell$ we cannot invert $\mathbf{D_0}$ as it is not a square matrix, but we can compute the pseudoinverse, which turns out to be:

$$
\mathbf{D_0}^{+} = \frac{1}{5}
\begin{bmatrix}
  -4 ~& -3 ~& -2~& -1 \\
  1 ~& -3 ~& -2 ~&-1 \\
  1~& 2 ~& -2 ~& -1\\
  1~& 2 ~& 3 ~& -1\\
  1~& 2 ~& 3 ~& 4
\end{bmatrix}
$$
:::


## Gradient-Based Image Representation
:::{.callout-tip icon=false}
## Recovering original image
The pseudoinverse has an interesting structure, and it is easy to see how it can be written in the general form for signals of length $N$. Also, note that $\mathbf{D_0}^{+}$ cannot be written as a convolution. Another important thing is that the inversion process is trying to recover more samples than there are observations. The trade-off is that the signal that it will recover will have zero mean (so it loses one degree of freedom that cannot be estimated). In this example, the reconstructed input is:

$$
\hat{\boldsymbol\ell} = \mathbf{D_0}^{+} \mathbf{r} =
\left[-0.2, -0.2, 0.8, 0.8, -1.2 \right]
$$
Note that $\hat{\boldsymbol\ell}$ is a zero mean vector. In fact, the recovered input is a shifted version of the original input, $\hat{\boldsymbol\ell} = \boldsymbol\ell - 1.2$, where 1.2 is the mean value of samples on $\boldsymbol\ell$. Then, you still can recover the input signal up to the DC component (mean signal value).
:::


## Image Editing in the Gradient Domain {#sec-editinggradientdomain}

:::{.callout-tip icon=false}
## Example
One of the interests of transforming an image into a different representation than pixels is that it allows us to manipulate aspects of the image that would be hard to control using the pixels directly. @fig-edit_with_derivatives shows an example of image editing using derivatives.

![Image inpainting: Using image derivatives, we delete the word “stop” by setting to zero the gradients indicated by the mask. The resulting decoded image propagates the red color inside the region that contained the word.](img/derivatives/edit_with_derivatives.png){#fig-edit_with_derivatives}
:::


## Image Editing in the Gradient Domain
:::{.callout-tip icon=false}
## Encoding/Decoding
First, the image is **encoded** using derivatives along the $x$ and $y$ dimensions:
$$
\mathbf{r} = 
\left[ 
\begin{array}{c}
\mathbf{D_x}  \\
\mathbf{D_y} 
\end{array}
\right] 
\boldsymbol\ell 
$$
The resulting representation, $\mathbf{r}$, contains the concatenation of the output of both derivative operators. $\mathbf{r}$ will have high values in the image regions that contain changes in pixel intensities and colors and will be near zero in regions with small variations. This representation can be **decoded** back into the input image by using the pseudoinverse as we did in the 1D case. The pseudoinverse can be efficiently computed in the Fourier domain @Weiss01derivingintrinsic.
:::

## Image Editing in the Gradient Domain
:::{.callout-tip icon=false}
## Manipulation
We can now manipulate this representation. In the example shown in @fig-edit_with_derivatives, if we set to zero the image derivatives that correspond to the word “stop,” when we decode the representation, we obtain a new image where the word “stop” has been removed. The red color fills the region with the word present in the input image. 


We did not have to specify what color that region had to be; that information comes from the integration process. Therefore, deleting an object using image derivatives can be achieved by setting to zero the gradients regardless of the content that will fill that region. Similarly, we could remove any object in an image or create new textures by modifying its gradient representation.
:::

::: aside
**Image inpainting** consists in filling a missing region in an image.
:::



## Gaussian Derivatives
:::{.callout-tip icon=false}
## Computational difficulties

In the previous sections, we studied how to discretize derivatives and how to use them to represent images. However, computing derivatives in practice presents several difficulties. First, derivatives are sensitive to noise. In the presence of noise, as images tend to vary slowly, the difference between two continuous pixel values will be dominated by noise.

:::: {.columns}

::: {.column width="33%"}
![](img/derivatives/stop_noise.jpg){#fig-derivativesnoisystop-a}
:::

::: {.column width="33%"}
![](img/derivatives/stop_noise_gradient.jpg){#fig-derivativesnoisystop-b}
:::

::: {.column width="33%"}
![](img/derivatives/stop_noise_gaussian_gradient.jpg){#fig-derivativesnoisystop-c}
:::
::::

Derivatives of a noisy image. (a) Input image with noise, (b) its $x$-derivative obtained by convolving with a kernel $[1, -1]$, and (c) its $x$-derivative obtained using a Gaussian derivative kernel.
:::

## Gaussian Derivatives
:::{.callout-tip icon=false}
## Computational difficulties
As shown in above, the input is an image corrupted with Gaussian noise. The result of convolving the input noisy image with the kernel $[1, -1]$ results in an output with increased noise. This is because the noise at each pixel is independent of the other, so when computing the difference between two adjacent pixels we get increased noise (the difference between two Gaussian random variables with variance $\sigma^2$ is a new Gaussian with variance $2\sigma^2$). On the other hand, the values of the pixels of the original image are very similar and the difference is a small number (except at the object boundaries). As a consequence, the output is dominated by noise as shown in @fig-derivativesnoisystop-b.

There are also situations in which the derivative of an image is not defined. For instance, consider an image in the continuous domain with the form $\ell (x,y) = 0$ if $x<0$ and 1 otherwise. If we try to compute $\partial \ell (x,y) / \partial x$, we will get 0 everywhere, but around $x=0$ the value of the derivative is not defined. We avoided this issue in the previous section because for discrete images the approximation of the derivative is always defined. 
:::

::: aside
Gaussian derivatives address these two issues. They were popularized by Koenderink and Van Doorm @Koenderink87 as a model of neurons in the visual system.
:::

## Gaussian Derivatives
:::{.callout-tip icon=false}
## Observation: commutativity
Let's start with the following observation. For two functions defined in the continuous domain $\ell(x,y)$ and $g(x,y)$, the convolution and the derivative are commutative:
$$
\frac {\partial \ell(x,y)}{\partial x} \circ g(x,y) = \ell(x,y) \circ \frac {\partial g(x,y)}{\partial x} 
$$

This equality is easy to prove in the Fourier domain. If our goal is to compute image derivatives and then blur the output using a differentiable low-pass filter, $g(x,y)$, then instead of computing the derivative of the image we can compute the derivatives of the filter kernel and convolve it with the image. Even if the derivative of $\ell(x,y)$ is not defined in some locations (e.g., step boundaries), we can always compute the result of this convolution.
:::


## Gaussian Derivatives
:::{.callout-tip icon=false}
## Smoothing
If $g(x,y)$ is a blurring kernel, it will smooth the derivatives, reducing the output noise at the expense of a loss in spatial resolution ([Figure 1](#fig-derivativesnoisystop-c)). A common smoothing kernel for computing derivatives is the Gaussian kernel. The Gaussian has nice smoothing properties as we discussed in the previous chapter, and it is infinitely differentiable. 

If $g$ is a Gaussian, then the first-order derivative is
$$
\begin{split}
g_x(x,y; \sigma) & = \frac {\partial g(x,y; \sigma)}{\partial x} \\
& = \frac{-x}{2 \pi \sigma^4} \exp{-\frac{x^2 +
   y^2}{2 \sigma^2}} \\
& = \frac{-x}{\sigma^2} g(x,y; \sigma)
\end{split}
$${#eq-derivate1gauss2dcont}
:::


## Gaussian Derivatives
:::{.callout-tip icon=false}
## Visual examples

The next plots show the 2D Gaussian with $\sigma=1$, and its $x$-derivative:

:::: {.columns}
::: {.column width="50%"}
![](img/derivatives/gaussian_sigma_1.png){#fig-gaussian-derivative-a}  
:::

::: {.column width="50%"}
![](img/derivatives/gaussian_x_derivative_sigma_1.png){#fig-gaussian-derivative-b}  
:::
::::

Fig (a) 2D Gaussian with $\sigma=1$, and (b) its $x$-derivative.
:::

## Gaussian Derivatives
:::{.callout-tip icon=false}
## Visual examples
The figure shows an image filtered with three Gaussian derivatives with different widths, $\sigma$. Derivatives at different scales emphasize different aspects of the image. The fine-scale derivatives @fig-gaussiander_zebra-a highlight the bands in the zebra, while the coarse-scale derivatives @fig-gaussiander_zebra-c emphasize more the object boundaries. This multiscale image analysis will be studied in depth in the following chapter.

:::: {.columns}
::: {.column width="33%"}
![](img/spatial_filters/gausian_der_c_2.jpg){#fig-gaussiander_zebra-a}  
:::

::: {.column width="33%"}
![](img/spatial_filters/gausian_der_c_4.jpg){#fig-gaussiander_zebra-b}  
:::

::: {.column width="33%"}
![](img/spatial_filters/gausian_der_c_8.jpg){#fig-gaussiander_zebra-c}  
:::
::::
:::

## Gaussian Derivatives
:::{.callout-tip icon=false}
## Visual examples
:::: {.columns}
::: {.column width="33%"}
![](img/spatial_filters/gausian_der_c_2plot.jpg){#fig-gaussiander_zebra-d}  
:::
::: {.column width="33%"}
![](img/spatial_filters/gausian_der_c_4plot.jpg){#fig-gaussiander_zebra-e}  
:::

::: {.column width="33%"}
![](img/spatial_filters/gausian_der_c_8plot.jpg){#fig-gaussiander_zebra-f}  
:::
::::

An image filtered with three Gaussian derivatives: Fig (a) $\sigma=2$; (b) $\sigma=4$; and (c) $\sigma=8$. Plots (d-f) show the three Gaussian derivatives over the same spatial support as the image. The discrete functions are approximated by sampling the continuous Gaussian derivatives. The convolutions are performed with mirror boundary extension.
:::

## High-Order Gaussian Derivatives

:::{.callout-tip icon=false}
## Computation
The second-order derivative of a Gaussian is:
$$
g_{x^2}(x,y; \sigma) = \frac{x^2-\sigma^2}{\sigma^4} g(x,y; \sigma)
$$

The $n$-th order derivative of a Gaussian can be written as the product between a polynomial on $x$, with the same order as the derivative, times a Gaussian. The family of polynomials that result in computing Gaussian derivatives is called Hermite polynomials.

The general expression for the $n$ derivative of a Gaussian is:
$$
g_{x^n}(x; \sigma) =  \frac{\partial^{n} g(x)}{\partial x^n} =
\left( \frac{-1}{\sigma \sqrt{2}} \right)^n
H_n\left( \frac{x}{\sigma \sqrt {2}} \right)
g(x; \sigma)
$$

The first Hermite polynomial is $H_0(x)=1$, the second is $H_1(x) = 2x$, the third is $H_2(x)=4x^2-2$, and they can be computed recursively as:
$$
H_n(x) = 2x H_{n-1}(x) - 2(n-1)H_{n-2}(x)
$$
:::


## High-Order Gaussian Derivatives
:::{.callout-tip icon=false}
## Examples
For $n=0$ we have the original Gaussian. @fig-gaussian_gaussiander shows the 1D Gaussian derivatives.

![Fig (a) 1D Gaussian with $\sigma=1$. (b–d) Gaussian derivatives up to order 3.](img/derivatives/gaussian_gaussiander.png){#fig-gaussian_gaussiander}
:::

## High-Order Gaussian Derivatives
:::{.callout-tip icon=false}
## Partial derivatives
In two dimensions, as the Gaussian is separable, the partial derivatives result on the product of two Hermite polynomial, one for each spatial dimension:

$$
\begin{split}
g_{x^n,y^m}(x,y; \sigma) & =  
\frac{\partial^{n+m} g(x,y)}{\partial x^n \partial y^m} \\
& = \left( \frac{-1}{\sigma \sqrt{2}} \right)^{n+m}
H_n\left( \frac{x}{\sigma \sqrt {2}} \right)
H_m\left( \frac{y}{\sigma \sqrt {2}} \right)
g(x,y; \sigma)
\end{split}
$$ {#eq-derivate2gauss2dhermite}
:::

## High-Order Gaussian Derivatives
:::{.callout-tip icon=false}
## Example
The figure shows the 2D Gaussian derivatives, and @fig-gauss_derivatives_triangle_FT shows the corresponding Fourier transforms.  
The figure shows the output of the derivatives when applied to a simple input image containing a square and a circle. Different derivatives detect a diverse set of image features. However, this representation might not be very useful as it is not rotationally invariant.

![](img/spatial_filters/gauss_derivatives_triangle.png){#fig-gauss_derivatives_triangle width=80%}

Fig Gaussian derivatives up to order 6. All the kernels are separable. They seem similar to Fourier basis multiplied with a Gaussian window.  
@fig-gauss_derivatives_triangle_FT shows that they are different from sine and cosine waves; instead, they look more like products of cosine and sine waves.

The Gaussian derivatives share many of the properties of the Gaussian. The convolution of two Gaussian derivatives of order $n$ and $m$ and variances $\sigma_1^2$ and $\sigma_2^2$ result in another Gaussian derivative of order $n+m$ and variance $\sigma_1^2 + \sigma_2^2$. Proving this property on the spatial domain can be tedious. However, it is trivial to prove it in the Fourier domain. 

:::

## High-Order Gaussian Derivatives
:::{.callout-tip icon=false}
## Example
![](img/spatial_filters/gauss_derivatives_triangle_FT.png){#fig-gauss_derivatives_triangle_FT width=80%}

Fourier transform of the Gaussian derivatives shown in @fig-gauss_derivatives_triangle.
:::

## High-Order Gaussian Derivatives
:::{.callout-tip icon=false}
## Example
![](img/derivatives/fig_gauss_derivatives_triangle_mondrian.png){width=80%}

An image containing a square and a circle and its output to the Gaussian derivatives up to order 3.
:::



## Derivatives using Binomial Filters {#sec-derivatives_binomial_filters}
:::{.callout-tip icon=false}
## Discrete approximations
When processing images we have to use discrete approximations for the Gaussian derivatives. After discretization, many of the properties of the continuous Gaussian will not hold exactly. 

There are many discrete approximations. For instance, we can take samples of the continuous functions. In practice it is common to use the discrete approximation given by the binomial filters. Below is the result of convolving the binomial coefficients, $b_n$, with  $\left[1, -1\right]$.

\begin{array}{ccccccccccccccccccccl} 
d_0 &    ~ &  ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~~1 & ~~~ & -1 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ &\\
d_1 &    ~ &  ~ & ~ & ~ & ~ & ~ & ~ & ~~1 & ~ & ~~0 & ~ & -1 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & \\
d_2 &    ~ &  ~ & ~ & ~ & ~ & ~ & ~~~1 & ~ & ~~~1 & ~~~ & -1 & ~ & -1 & ~ & ~ & ~ & ~ & ~ & ~ & \\
d_3 &    ~ &  ~ & ~ & ~ & ~ & ~~~1 & ~ & ~~~2 & ~ &  ~~0 & ~~~  & -2 & ~ & -1 & ~ & ~ & ~ & ~ & ~ &\\
d_4 &    ~ &  ~ & ~  & ~ & ~~~1 & ~   & ~~~3   & ~   & ~~~2  &  ~~~  & -2 & ~   & -3   & ~   & -1 & ~ & ~ & ~ & ~ &\\
d_5 &    ~ &  ~ & ~  & ~~~1 & ~ & ~~~4   & ~   & ~~~5 & ~   & ~~0 & ~   & -5 & ~   & -4   & ~ & -1 & ~ & ~ & ~ &
\end{array}

Derivative of binomial coefficients resulting from the convolution $b_n \circ \left[1, -1\right]$. The filters, $d_0$ and $d_1$, are the ones we have studied in the previous section.
:::

## Derivatives using Binomial Filters
:::{.callout-tip icon=false}
## Sobel operator

In two dimensions, we can use separable filters and build a partial derivative as:

$$
Sobel_x =  \begin{bmatrix}
  1 & 0 & -1 \\
\end{bmatrix} \circ \begin{bmatrix}
  1 \\
  2 \\
  1
\end{bmatrix} =
\begin{bmatrix}
  1 & 0 & -1 \\
  2 & 0 & -2 \\
  1 & 0 & -1
\end{bmatrix}
$$

$$
Sobel_y =  \begin{bmatrix}
  -1 & -2 & -1 \\
  0 & 0 & 0 \\
  1 & 2 & 1
\end{bmatrix}
$$ {#eq-sobel_kernels}

This particular filter is called the **Sobel-Feldman operator**.  
The goal of this operator was to be compact and as isotropic as possible. The Sobel-Feldman operator can be implemented very efficiently as it can be written as the convolution with four small kernels, $$Sobel_x = b_1 \circ d_0 \circ b_1^T \circ b_1^T$$.
:::

::: aside
Remember that \(b_1=[1, 1]\), and \(b_2=b_1 \circ b_1 = [1,2,1]\).
:::

## Derivatives using Binomial Filters
:::{.callout-tip icon=false}
## DFT of the Sobel-Feldman operator
The DFT of the Sobel-Feldman operator is:

$$
Sobel_x \left[u,v \right] = D_1\left[u\right] B_2 \left[v \right] = j \sin \left( 2 \pi u /N \right) \left( 2+2 \cos \left(2 \pi v/N \right) \right)
$$

and it is shown in @fig-DFTderivativeoperators (d). \(N \times N\) is the extension of the domain (the operator is zero-padded). As \(d_0\) and \(d_1\) are 1D, their 2D DFT varies only along one dimension. The Roberts cross operator is similar to a rotated version of \(d_1\). The Sobel-Feldman operator has the profile of \(D_1\) along the axis \(u=0\) and it is proportional to the profile of \(B_2\) along any section \(v=\text{constant}\).

![](img/spatial_filters/DFTderivativeoperators.png){#fig-DFTderivativeoperators width=100%}

Magnitude of the DFT of four different discretizations of Gaussian derivatives: (a) \(d_0\); (b) \(d_1\); (c) Robert cross operator; and (d) Sobel-Feldman operator.

@fig-DFTderivativeoperators compares the DFT of the four types of approximations of the derivatives that we have discussed. These operators are still very popular. \(Sobel\) has the best tolerance to noise due to its band-pass nature. The kernel \(d_0\) is the one that provides the highest resolution in the output. @fig-circle shows the output of different derivative approximations to a simple input image containing a circle. In the next section, we will discuss how to use these derivatives to extract other interesting quantities.

:::

## Derivatives using Binomial Filters
:::{.callout-tip icon=false}
## Derivatives of a circle
![](img/spatial_filters/circle.png){#fig-circle width=100%}

Derivatives of a circle along the directions $n$, $m$, and 45 degrees.  
The angle is shown only where the magnitude is $>0$. The derivative output along 45 degrees is obtained as a linear combination of the derivatives outputs along $n$ and $m$. Check the differences among the different kernels. The Sobel operator gives the most rotationally invariant gradient magnitude, but it is blurrier.
:::

## Image Gradient and Directional Derivatives
:::{.callout-tip icon=false}
## Image gradient

As we saw previously, an important image representation is given by the image gradient. From the image derivatives, we can also define the image gradient as the vector:
$$
\nabla \ell (x,y) = \left( \frac{\partial \ell(x,y)}{\partial x}, \frac{\partial \ell(x,y)}{\partial y} \right)
$$
For each pixel, the output is a 2D vector. In the case of using Gaussian derivatives, we can write:
$$
\nabla \ell \circ g = \nabla g \circ \ell = \left( g_x(x,y), g_y(x,y) \right) \circ \ell
$$

Although we have mostly computed derivatives along the $x$ and $y$ variables, we can obtain the derivative in any orientation as a linear combination of the two derivatives along the main axes. With ${\bf t}=\left( \cos (\theta), \sin(\theta) \right)$, we can write the directional derivative along the vector ${\bf t}$ as:
$$
\frac{\partial \ell (x,y)}{\partial {\bf t}} =  \nabla \ell \cdot {\bf t} = \cos(\theta) \frac{\partial \ell}{\partial x} + \sin(\theta) \frac{\partial \ell}{\partial y}
$$
:::


## Image Gradient and Directional Derivatives
:::{.callout-tip icon=false}
## Image gradient
In the Gaussian case:
$$
\begin{split}
\frac{\partial \ell}{\partial {\bf t}} \circ g & = \left( \cos(\theta) g_x(x,y) + \sin(\theta) g_y(x,y) \right) \circ \ell \\
& = \left( \nabla g  \cdot {\bf t} \right) \circ \ell \\
& = g_{\theta} (x,y)  \circ \ell(x,y)
\end{split}
$$ {#eq-steerable_derivative_filter}

with $g_{\theta} (x,y) = \cos(\theta) g_x(x,y) + \sin(\theta) g_y(x,y)$. However, to compute the derivative along any arbitrary angle $\theta$ does not require doing new convolutions. Instead, we can compute any derivative as a linear combination of the output of convolving the image with $g_x(x,y)$ and $g_y(x,y)$:
$$
\frac{\partial \ell}{\partial {\bf t}} \circ g =  \cos(\theta) g_x(x,y) \circ \ell + \sin(\theta) g_y(x,y) \circ \ell(x,y)
$$
:::

## Image Gradient and Directional Derivatives
:::{.callout-tip icon=false}
## Image gradient
When using discrete convolutional kernels $d_n\left[n,m\right]$ and $d_m\left[n,m\right]$ to approximate the derivatives along $n$ and $m$, it can be written as:
$$
\nabla \ell = \left( d_n\left[n,m\right], d_m\left[n,m\right] \right) \circ \ell \left[n,m\right]
$$
and
$$
\frac{\partial \ell}{\partial {\bf t}} \circ g = d_{\theta} \left[n,m\right] \circ \ell \left[n,m\right]
$$

with $d_{\theta} \left[n,m\right]  = \cos(\theta) d_n\left[n,m\right] + \sin(\theta) d_m\left[n,m\right]$. We expect that the linear combination of these two kernels should approximate the derivative along the direction $\theta$. The quality of this approximation will vary for the different kernels we have seen in the previous sections.
:::


## Image Laplacian
:::{.callout-tip icon=false}
## Definition

The **Laplacian filter** was made popular by Marr and Hildreth in 1980 [@Marr80] in the search for operators that locate the boundaries between objects. The Laplacian is a common operator from differential geometry to measure the divergence of the gradient and it appears frequently in modeling fields in physics. The Laplacian also has applications in graph theory and in spectral methods for image segmentation [@ng2002].


The Laplacian operator is defined as the sum of the second order partial derivatives of a function:

$$
\nabla^2 \ell = \frac{\partial^2 \ell}{\partial x^2} + \frac{\partial^2 \ell}{\partial y^2}
$$

The Laplacian is more sensitive to noise than the first order derivative. Therefore, in the presence of noise, when computing the Laplacian operator on an image $\ell(x,y)$, it is useful to smooth the output with a Gaussian kernel, $g(x,y)$. We can write,

$$
\nabla^2 \ell \circ g = \nabla^2 g \circ \ell
$$
:::

::: aside
One example of application of the Laplacian is in the paper *Can One Hear the Shape of a Drum* [@Kac_1966], where the Laplacian is used for modeling vibrations in a drum and the sounds it produces as a function of its shape.
:::

## Image Laplacian
:::{.callout-tip icon=false}
## Gaussian
Therefore, the same result can be obtained if we first compute the Laplacian of the Gaussian kernel, $g(x,y)$ and then convolve it with the input image. The Laplacian of the Gaussian is

$$
\nabla^2 g = \frac{x^2 + y^2 -2\sigma^2}{\sigma^4} g(x,y)
$$

The next plot (@fig-mexican_hat_wavelet) shows the Gaussian Laplacian ($\sigma = 1$). Due to its shape, the Laplacian is also called the inverted **mexican hat wavelet**.


![The Gaussian Laplacian ($\sigma = 1$) is also called the inverted **mexican hat wavelet**. (a) 2D plot. (b) 1D section at $y=0$.
](img/derivatives/hat.png){#fig-mexican_hat_wavelet width=45%} 
:::


## Image Laplacian
:::{.callout-tip icon=false}
## Discrete approximation

In the discrete domain, there are several approximations to the Laplacian filter. The simplest one consists in sampling the continuous Gaussian Laplacian in discrete locations. Figures @fig-DFTlaplacians(a-c) show the DFT of the Gaussian Laplacian for three different values of $\sigma$. For values $\sigma > 1$, the resulting filter is band-pass, which is the expected behavior for the Gaussian Laplacian.

![](img/spatial_filters/DFTlaplacians.png){#fig-DFTlaplacians width=100%}

Magnitude of the DFT of the Gaussian Laplacian with (a) $\sigma=1/2$; (b) $\sigma=1$; (c) $\sigma=2$; and (d) DFT of the five-point discrete approximation, @eq-five_point_laplacian.
:::

## Image Laplacian
:::{.callout-tip icon=false}
## Discrete approximation
In one dimension, the Laplacian can be approximated by $[1,-2,1]$, which is the result of the convolution of two 2-tap discrete approximations of the derivative $[1,-1] \circ [1,-1]$. In two dimensions, the most popular approximation is the five-point formula, which consists of convolving the image with the kernel:

$$
\nabla_5^2 = 
\begin{bmatrix}
  0 & 1 & 0 \\
  1 & -4 & 1\\
  0 & 1 & 0
\end{bmatrix}
$$ {#eq-five_point_laplacian}

It involves the central pixel and its four nearest neighbors. This is a sum-separable kernel: it corresponds to approximating the second-order derivative for each coordinate and summing the result (i.e., convolve the image with $[1,-2,1]$ and also with its transpose and summing the two outputs).

The Laplacian of the image, using the five-point formula, is:

$$
\begin{split}
\nabla_5^2 \ell[n,m] = & - 4 \ell[n,m] \\
                     &  + \ell[n+1,m] \\
                     &  + \ell[n-1,m] \\
                     &  + \ell[n,m+1] \\
                     &  + \ell[n,m-1]
\end{split}
$$
:::

## Image Laplacian
:::{.callout-tip icon=false}
## Discrete approximation
The figure compares the output of image derivatives and the Laplacian. Note that when summing two first-order derivatives along $x$ and $y$, what we obtain is a first-order derivative along the 45-degree angle. However, when summing up the two second-order derivatives, we obtain a rotationally invariant kernel, as shown in (d).

:::: {.columns}
::: {.column width="25%"}
![](img/spatial_filters/wheel256.jpg){#fig-wheellaplacian-a }  
:::

::: {.column width="25%"}
![](img/spatial_filters/wheelLaplacianx.jpg){#fig-wheellaplacian-b }  
:::

::: {.column width="25%"}
![](img/spatial_filters/wheelLaplaciany.jpg){#fig-wheellaplacian-c }  
:::

::: {.column width="25%"}
![](img/spatial_filters/wheelLaplacian.jpg){#fig-wheellaplacian-d }  
:::
::::

Fig (a) Input image. (b) Second order derivative along $x$. (c) Second-order derivative along $y$. (d) The sum of (b) + (c), which gives the Laplacian.
:::

## Image Laplacian

:::{.callout-tip icon=false}
## Advantages
The discrete approximation also provides a better intuition of how it works than its continuous counterpart. The Laplacian filter has a number of advantages with respect to the gradient:

- It is rotationally invariant. It is a linear operator that responds equally to edges in any orientation (this is only approximate in the discrete case).
- It measures curvature. If the image contains a linear trend the derivative will be non-zero despite having no boundaries, while the Laplacian will be zero.
- Edges can be located as the zero-crossings in the Laplacian output. However, this way of detecting edges is not very reliable.
- Zero crossings of an image form closed contours.


Marr and Hildreth [@Marr80] used zero-crossings of the Laplacian output to compute edges, but this method is not used nowadays for edge detection. Instead, the Laplacian filter is widely used in a number of other image representations. It is used to build image pyramids, and to detect points of interest in images (it is the basic operator used to detect keypoints to compute SIFT descriptors [@Lowe04]).
:::

## Image Laplacian

:::{.callout-tip icon=false}
## Comparison
@fig-discretelaplacian1d compares the output of the first-order derivative @fig-discretelaplacian1d (b) and the Laplacian @fig-discretelaplacian1d (d) on a simple 1D signal @fig-discretelaplacian1d (a). The local maximum of the derivative output @fig-discretelaplacian1d (c) and the zero crossings of the Laplacian output @fig-discretelaplacian1d (e) are aligned with the transitions (boundaries) of the input signal @fig-discretelaplacian1d (a).


![Comparison between the output of a first-order derivative and the Laplacian of 1D signal. (a) Input signal. (b) Kernel $d_1$. (c) Output of the derivative, that is, convolution of (a) and (b). (d) Discrete approximation of the Laplacian. (e) Output of convolving the signal (a) with the Laplacian kernel.](img/derivatives/discretelaplacian1d.png){width=70% #fig-discretelaplacian1d}
:::


## A Simple Model of the Early Visual System
:::{.callout-tip icon=false}
## Approximation of the visual system

The Laplacian filter can also be used as a coarse approximation of the behavior of the **early visual system**. When looking at the magnitude of the DFT of the Laplacian with $\sigma=1$ (@fig-DFTlaplacians), the shape seems reminiscent of our subjective evaluation of our own visual sensitivity to spatial frequencies when we look at the Campbell and Robson chart (@fig-csfchart). As the visual filter does not seem to cancel exactly the very low spatial frequencies as the Laplacian does, a better approximation is

$$
h = -\nabla^2 g  + \lambda g
$$ {#eq-humanmodel}

where the negative sign in front of the Laplacian helps to fit our perception better. The kernel $h$ is the approximate impulse response of the human visual system, $\lambda$ is a small constant that is equal to the DC gain of the visual filter (here we have set $\lambda = 2$ and $\sigma=5$). This results in the profile shown in @fig-EVS_sigma5_lambda2.

![](img/derivatives/EVS_sigma5_lambda2.png){#fig-EVS_sigma5_lambda2 width=40%}

Kernel corresponding to @eq-humanmodel with $\lambda = 2$ and $\sigma=5$.
:::

## A Simple Model of the Early Visual System
:::{.callout-tip icon=false}
The first thing worth pointing out is that the Fourier transform of the impulse response shown in @fig-EVS_sigma5_lambda2 has a shape that is qualitatively similar to the shape of the human contrast sensitivity function as discussed in Fourier analysis lecture. Here we show again the Campbell and Robson chart, together with a radial section of the Fourier transform of $h$ from @eq-humanmodel:

:::: {.columns}
::: {.column width="50%"}
![](img/spatial_filters/csf.jpg){#fig-csfchart width=100%}  
:::

::: {.column width="50%"}
![](img/derivatives/dft_radial_EVS.png){width=80%}  
:::
::::

Campbell and Robson chart, and a radial section of the Fourier transform of $h$ from @eq-humanmodel.
:::

## A Simple Model of the Early Visual System
:::{.callout-tip icon=false}
## Visual illusion
This particular form of impulse response explains some visual illusions as shown in @fig-vasarely-a. This visual illusion is called the **Vasarely visual illusion**.


:::: {.columns}
::: {.column width="33%"}
![](img/spatial_filters/vasarely_c.jpg){#fig-vasarely-a}  
:::

::: {.column width="33%"}
![](img/spatial_filters/vasarely_d.jpg){#fig-vasarely-b}  
:::

::: {.column width="33%"}
![](img/spatial_filters/vasarely_section.png){#fig-vasarely-c}  
:::
::::


:::: {.columns}
::: {.column width="33%"}
![](img/spatial_filters/vasarely_a.jpg){#fig-vasarely-d}  
:::

::: {.column width="33%"}
![](img/spatial_filters/vasarely_b.jpg){#fig-vasarely-e}  
:::

::: {.column width="33%"}
![](img/spatial_filters/vasarely_section_b.png){#fig-vasarely-f}  
:::
::::



Vasarely visual illusion. Images (a) and (d), formed by nested-squares, appear as having bright diagonals in (a) and dark in (d). Images (b) and (e) show the output of the human model given by the filter from @eq-humanmodel. Plot (c) displays the intensity profiles of images (a) and (b) as horizontal sections, with image (a) represented in red and image (b) in blue. Similarly, Plot (f) represents the intensity profiles of images (d) and (e).
:::

## Sharpening Filter
:::{.callout-tip icon=false}
## Definition

One example of a simple but very useful filter is a **sharpening filter**. The goal of a sharpening filter is to transform an image so that it appears sharper (i.e., it contains more fine details). This can be achieved by amplifying the amplitude of the high-spatial frequency content of the image. We can achieve this with a combination of filters that we have already discussed in this section.

A simple way to design a sharpening filter is to de-emphasize the blurry components of an image. By the linearity of the convolution operator, we're allowed to add and subtract kernels to make a new kernel that would give us the same filtered image as if we had added and subtracted the filtered outputs of each of the component kernels. For this example, we start with twice the original image (sharp plus blurred parts), then subtract away the blurred components of the image:

$$
\text{sharpening filter} = 
\begin{bmatrix}
  0 & 0 & 0 \\
  0 & 2 & 0\\
  0 & 0 & 0
\end{bmatrix}
-
\frac{1}{16}
\begin{bmatrix}
  1 & 2 & 1 \\
  2 & 4 & 2\\
  1 & 2 & 1
\end{bmatrix}
$$ {#eq-sharpening}

Note that the DC gain of this sharpening filter is 1. That would leave one original image in there, plus an additional component of the sharp details. The perceptual result is that of a sharpened image. We can apply this filter successively in order to further enhance the image details. If too much sharpening is applied, we might end up enhancing noise and introducing image artifacts.
:::

## Sharpening Filter
:::{.callout-tip icon=false}
## Example

:::: {.columns}
::: {.column width="33%"}
![](img/spatial_filters/boat_sharp.jpg){#fig-convExamps3-a}  
:::

::: {.column width="33%"}
![](img/spatial_filters/boat_sharp0.jpg){#fig-convExamps3-b}  
:::

::: {.column width="33%"}
![](img/spatial_filters/boat_sharp1.jpg){#fig-convExamps3-c}  
:::
::::

:::: {.columns}
::: {.column width="33%"}
![](img/spatial_filters/boat_sharp2.jpg){#fig-convExamps3-d}  
:::

::: {.column width="33%"}
![](img/spatial_filters/boat_sharp3.jpg){#fig-convExamps3-e}  
:::

::: {.column width="33%"}
![](img/spatial_filters/boat_sharp4.jpg){#fig-convExamps3-f}  
:::
::::

Sharpening achieved by subtraction of blurred components. (a) Original image. (b) Sharpened once by filtering with kernel from @eq-sharpening. Each color channel is filtered independently. (c-f) The same filter is applied successively to the previous output. In the last image, the sharpening filter has been applied five times to the input image. The last image looks substantially sharper than the original image, but close inspection will reveal some artifacts.
:::

Note that the sharpening filter described here does not introduce new fine image details, it only enhances the ones already present in the image.

It is also interesting to note that this sharpening filter is very similar to the model of the early visual system that we described before.

## Retinex

:::{.callout-tip icon=false}
## Retinex
How do you tell gray from white? This might seem like a simple question, but one remarkable aspect of vision is that perception of the simplest quantity might be far from trivial. As a final example to illustrate the power of using image derivatives as an image representation, we will discuss here a partial answer to this question.

To understand that answering this question is not trivial, let's consider the structure of light that reaches the eye, which is the input upon which our visual system will try to differentiate black from white. The amount of light that reaches the eye from a painted piece of paper is the result of two quantities: the amount of light reaching the piece of paper, and the reflectance of the surface (what fraction of the light is reflected back into space and what fraction is absorbed by the material).

If two patches receive the same amount of light, then we will perceive as being darker the patch that reflects less light. But what happens if we change the amount of light that reaches the scene? If we increase the amount of light projected on top of the dark patch, what will be seen now? What will happen if we have two patches on the same scene, each of them receiving different amounts of light? How does the visual system decide what white is and how does it deal with changes in the amount of light illuminating the scene? The visual system is constantly trying to estimate what the incident illumination is and what are the actual reflectances of the surfaces present in the scene.

If our goal is to estimate the shade of gray of a piece of painted paper, then the quantity that we care about is the reflectance of the surface. But, what happens if we see two patches of unknown reflectance, and each is illuminated with two different light sources of unknown identity? How does the visual system manage to infer what the illumination and reflectances are?


![](img/statistical_image_models/retinex.jpg){#fig-simultaneous width=100%}

Simultaneous contrast illusion. What happens if we see two patches of unknown reflectance, and each is illuminated with two different light sources of unknown identity?
:::

## Retinex

:::{.callout-tip icon=false}
## Image formation
Visual illusions are a way of getting to feel how our own visual system processes images. To experience how this estimation process works, let's start by considering a very simple image as shown in @fig-simultaneous. This image shows a very simple scene formed by a surface with a set of squares of different gray levels illuminated by a light spot oriented toward the left. Our visual system tries to estimate the two quantities: the reflectance of each square and the illumination intensity reaching each pixel.

Let's think of the image formation process. The surface is made of patches of different reflectances $r(x,y) \in (0,1)$. Each location receives an illumination $l(x,y)$. The observed brightness is the product:

$$
\ell(x,y) = r(x,y) \times l(x,y)
$$

Despite what reaches the eye is the signal $\ell(x,y)$, our perception is not the value of $\ell(x,y)$. In fact, the squares 1 and 2 in @fig-simultaneous have the exact same values of intensity, but we see them differently, which is generally explained by saying that we discount (at least partially) the effects of the illumination, $l(x,y)$.
:::


::: aside

The Retinex algorithm, by Land and McCann [@Land1971], is based on modeling images as if they were part of a Mondrian world (images that look like the paintings of Piet Mondrian). One example of a color Mondrian is

:::

## Retinex
![](img/spatial_filters/mondrian.png){width=100%}

## Retinex
:::{.callout-tip icon=false}
## Algorithm
But how can we estimate $r(x,y)$ and $l(x,y)$ by only observing $\ell(x,y)$? One of the first solutions to this problem was the **Retinex algorithm** proposed by Land and McCann [@Land1971]. Land and McCann observed that it is possible to extract $r$ and $l$ from $\ell$ if one takes into account some constraints about the expected nature of the images $r$ and $l$. Land and McCann noted that if one considers two adjacent pixels inside one of the patches of uniform reflectance, the difference between the two pixel values will be very small as the illumination, even if it is nonuniform, will only produce a small change in the brightness of the two pixels. However, if the two adjacent pixels are on the boundary between two patches of different reflectances, then the intensities of these two pixels will be very different.

The Retinex algorithm works by first extracting $x$ and $y$ spatial derivatives of the image $\ell(x,y)$ and then thresholding the gradients. First, we transform the product into a sum using the $\log$:

$$
\log \ell(x,y) = \log r(x,y) + \log l(x,y)
$$

Taking derivatives along $x$ and $y$ is now simple:

$$
\frac{\partial \log \ell(x,y)}{\partial x} = \frac{\partial \log r(x,y)}{\partial x} + \frac{\partial \log l(x,y)}{\partial x}
$$
And the same thing is done for the derivative along $y$.
:::

## Retinex
:::{.callout-tip icon=false}
## Algorithm
Any derivative larger than the threshold is assigned to the derivative of the reflectance image $r(x,y)$, and the ones smaller than a threshold are assigned to the illumination image $l(x,y)$:

$$
\frac{\partial \log r(x,y)}{\partial x} =  \begin{cases}
\frac{\partial \log \ell(x,y)}{\partial x} & \text{if} ~  \left| \frac{\partial \log \ell(x,y)}{\partial x} \right|>T\\
0 & \text{otherwise}
\end{cases}
$$

@fig-simultaneous2 shows how the image derivatives are decomposed into reflectance and illumination changes.

![](img/statistical_image_models/retinex_solution_b.png){#fig-simultaneous2 width=100%}

Derivatives classified into reflectance or luminance components.
:::


## Retinex
:::{.callout-tip icon=false}
## Algorithm
Then, the image $\log r(x,y)$ is obtained by integrating the gradients, as shown in @sec-editinggradientdomain, and exponentiating the result. Finally, the illumination can be obtained as $l(x,y) = \ell(x,y)/r(x,y)$. The results are shown in @fig-simultaneous3.

![](img/statistical_image_models/retinex_solution_a.png){#fig-simultaneous3 width=100%}

Recovered components. The estimated reflectance, $r(x,y)$, is close to what we perceive. It seems that what we perceive contains part of $l(x,y)$.
:::

## Retinex
:::{.callout-tip icon=false}
## Algorithm
Note that the illumination derivatives have much lower magnitude than the reflectance derivatives; however, they extend over much larger spatial regions, which means that once they are integrated, changes in brightness due to nonuniform illumination might be larger than changes due to variations in reflectance values.

Despite the simplicity of this approach, it works remarkably well with images like the ones shown in @fig-simultaneous and also with a number of real images. However, the algorithm is limited and it is easy to make it fail in situations where the visual system works, as we will see later in more depth.

The Retinex algorithm works by making assumptions about the structure of the images it tries to separate. The underlying assumption is that the illumination image, $l(x,y)$, varies smoothly and the reflectance image, $r(x,y)$, is composed of uniform regions separated by sharp boundaries. These assumptions are very restrictive and will limit the domain of applicability of such an approach. Understanding the structure of images in order to build models such as Retinex has been the focus of a large amount of research. This chapter will introduce some of the most important image models.

The recovered brightness is stronger than what we perceive. There are a number of reasons that weaken the perceived brightness. The effect is bigger if the display occupies the entire visual field. Right now the picture appears in the context of the page, which also affects our perception of the illumination. Also, the two larger squares do not seem to group well with the rest of the display, as if they were floating in a different plane. And finally, for such a simple display, the visual system does not fully separate the perception of both components.

Decomposing an image into different physical causes is known as **intrinsic images decomposition**. The intrinsic image decomposition, proposed by Barrow and Tenenbaum [@Barrow1978], aims to recover intrinsic scene characteristics from images, such as occlusions, depth, surface normals, shading, reflectance, reflections, and so on.
:::

## Concluding Remarks

In this chapter we have covered a very powerful image representation: image derivatives (first order, second order, and the Laplacian) and their discrete approximations. Despite the simplicity of this representation, we have seen that it can be used in a number of applications such as image inpaining, separation of illumination and reflectance, and it can be used to explain simple visual illusions. It is not surprising that similar filters like the ones we have seen in this chapter emerge in convolutional neural networks when trained to solve visual tasks. 
